{"cells":[{"cell_type":"markdown","metadata":{"id":"YPHN7PJgKOzb"},"source":["# Interacting with CLIP\n","\n","This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."]},{"cell_type":"markdown","metadata":{"id":"53N4k0pj_9qL"},"source":["# Preparation for Colab\n","\n","Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the `clip` package and its dependencies, and check if PyTorch 1.7.1 or later is installed."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"0BpdJkdBssk9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692939045253,"user_tz":-540,"elapsed":12334,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"0ef45bee-bc20-42aa-b95b-dec516884819"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.1\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-vugtggus\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-vugtggus\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=c947878e719c68b36b504f4442f9fe593c437030d880727a371ac27650f21031\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-likvserf/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}],"source":["! pip install ftfy regex tqdm\n","! pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"C1hkDT38hSaP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692939283799,"user_tz":-540,"elapsed":288,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"1a508957-3e57-4588-c790-272e40663ac4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 2.0.1+cu118\n"]}],"source":["import numpy as np\n","import torch\n","from pkg_resources import packaging\n","\n","print(\"Torch version:\", torch.__version__)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{"id":"eFxgLV5HAEEw"},"source":["# Loading the model\n","\n","`clip.available_models()` will list the names of available CLIP models."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"uLFS29hnhlY4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692939290593,"user_tz":-540,"elapsed":748,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"e834ec9e-548d-47c4-8def-d29aec52829e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['RN50',\n"," 'RN101',\n"," 'RN50x4',\n"," 'RN50x16',\n"," 'RN50x64',\n"," 'ViT-B/32',\n"," 'ViT-B/16',\n"," 'ViT-L/14',\n"," 'ViT-L/14@336px']"]},"metadata":{},"execution_count":4}],"source":["import clip\n","\n","clip.available_models()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IBRVTY9lbGm8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692939311114,"user_tz":-540,"elapsed":17131,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"ab356ec2-d0d8-4a99-b771-c5437f449e48"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:07<00:00, 46.3MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model parameters: 151,277,313\n","Input resolution: 224\n","Context length: 77\n","Vocab size: 49408\n"]}],"source":["model, preprocess = clip.load(\"ViT-B/32\")\n","model.cuda().eval()\n","input_resolution = model.visual.input_resolution\n","context_length = model.context_length\n","vocab_size = model.vocab_size\n","\n","print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n","print(\"Input resolution:\", input_resolution)\n","print(\"Context length:\", context_length)\n","print(\"Vocab size:\", vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"21slhZGCqANb"},"source":["# Image Preprocessing\n","\n","We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n","\n","The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"d6cpiIFHp9N6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692939314621,"user_tz":-540,"elapsed":280,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"86c56a8f-2fbd-48cd-bc2a-5544086249b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Compose(\n","    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n","    CenterCrop(size=(224, 224))\n","    <function _convert_image_to_rgb at 0x78fb3306a4d0>\n","    ToTensor()\n","    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",")"]},"metadata":{},"execution_count":6}],"source":["preprocess"]},{"cell_type":"markdown","metadata":{"id":"xwSB5jZki3Cj"},"source":["# Text Preprocessing\n","\n","We use a case-insensitive tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qGom156-i2kL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692939319153,"user_tz":-540,"elapsed":312,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"60683562-c8f7-4013-86a9-ba605db87f9b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[49406,  3306,  1002,   256, 49407,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"]},"metadata":{},"execution_count":7}],"source":["clip.tokenize(\"Hello World!\")"]},{"cell_type":"markdown","metadata":{"id":"ZB7DroL3NE83"},"source":["# CLIP Encoders"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"DfDluIAyNCEx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692939323553,"user_tz":-540,"elapsed":296,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"450ebaa2-15da-4a84-e74c-2ffde497c787"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CLIP(\n","  (visual): VisionTransformer(\n","    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n","    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (transformer): Transformer(\n","      (resblocks): Sequential(\n","        (0): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (6): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (7): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (8): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (9): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (10): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (11): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (transformer): Transformer(\n","    (resblocks): Sequential(\n","      (0): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (6): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (7): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (8): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (9): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (10): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (11): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (token_embedding): Embedding(49408, 512)\n","  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")"]},"metadata":{},"execution_count":8}],"source":["model"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jY1UefZgOCgn","executionInfo":{"status":"ok","timestamp":1692939330648,"user_tz":-540,"elapsed":313,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["from torch import nn\n","import copy\n","from collections import OrderedDict\n","\n","class LayerNorm(nn.LayerNorm):\n","    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n","\n","    def forward(self, x: torch.Tensor):\n","        orig_type = x.dtype\n","        ret = super().forward(x.type(torch.float32))\n","        return ret.type(orig_type)\n","\n","class QuickGELU(nn.Module):\n","    def forward(self, x: torch.Tensor):\n","        return x * torch.sigmoid(1.702 * x)"]},{"cell_type":"code","source":["class MyResidualAttentionBlock(nn.Module):\n","    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n","        super().__init__()\n","\n","        ### Q1 TODO ###\n","\n","        self.attn = None\n","        self.ln_1 = None\n","        self.mlp = nn.Sequential(OrderedDict([\n","            (\"c_fc\", None),\n","            (\"gelu\", None),\n","            (\"c_proj\", None)\n","        ]))\n","        self.ln_2 = None\n","        self.attn_mask = attn_mask\n","\n","        ###############\n","\n","    def attention(self, x: torch.Tensor):\n","        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n","        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n","\n","    def forward(self, x: torch.Tensor):\n","\n","        ### Q1 TODO ###\n","\n","        x = None\n","        x = None\n","\n","        ###############\n","\n","        return x\n","\n","\n"],"metadata":{"id":"vr2pcztQX3k2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attn_block = copy.deepcopy(model.visual.transformer.resblocks[0]).float().cpu()\n","my_block = MyResidualAttentionBlock(768, 12)\n","my_block.load_state_dict(attn_block.state_dict())\n","\n","N, L, D = 4, 49, 768\n","x = torch.rand(L, N, D)\n","a = attn_block(x)\n","b = my_block(x)\n","attn_block(x).equal(my_block(x))"],"metadata":{"id":"8Pny5OvoZChE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4W8ARJVqBJXs"},"source":["# Setting up input images and texts\n","\n","We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n","\n","The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMc1AXzBlhzm"},"outputs":[],"source":["import os\n","import skimage\n","import IPython.display\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import numpy as np\n","\n","from collections import OrderedDict\n","import torch\n","\n","%matplotlib inline\n","\n","# images in skimage to use and their textual descriptions\n","descriptions = {\n","    \"page\": \"a page of text about segmentation\",\n","    \"chelsea\": \"a facial photo of a tabby cat\",\n","    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n","    \"rocket\": \"a rocket standing on a launchpad\",\n","    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n","    \"camera\": \"a person looking at a camera on a tripod\",\n","    \"horse\": \"a black-and-white silhouette of a horse\",\n","    \"coffee\": \"a cup of coffee on a saucer\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSSrLY185jSf"},"outputs":[],"source":["original_images = []\n","images = []\n","texts = []\n","plt.figure(figsize=(16, 5))\n","\n","for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n","    name = os.path.splitext(filename)[0]\n","    if name not in descriptions:\n","        continue\n","\n","    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n","\n","    plt.subplot(2, 4, len(images) + 1)\n","    plt.imshow(image)\n","    plt.title(f\"{filename}\\n{descriptions[name]}\")\n","    plt.xticks([])\n","    plt.yticks([])\n","\n","    original_images.append(image)\n","    images.append(preprocess(image))\n","    texts.append(descriptions[name])\n","\n","plt.tight_layout()\n"]},{"cell_type":"markdown","metadata":{"id":"WEVKsji6WOIX"},"source":["## Building features\n","\n","We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBgCanxi8JKw"},"outputs":[],"source":["image_input = torch.tensor(np.stack(images)).cuda()\n","text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZN9I0nIBZ_vW"},"outputs":[],"source":["with torch.no_grad():\n","    image_features = model.encode_image(image_input).float()\n","    text_features = model.encode_text(text_tokens).float()"]},{"cell_type":"markdown","metadata":{"id":"cuxm2Gt4Wvzt"},"source":["## Calculating cosine similarity\n","\n","We normalize the features and calculate the dot product of each pair."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKAxkQR7bf3A"},"outputs":[],"source":["def get_similarity(image_features, text_features):\n","\n","    ### Q2 TODO ###\n","\n","\n","\n","\n","    similarity = None\n","\n","    ###############\n","\n","    return similarity\n","\n","similarity = get_similarity(image_features, text_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5zvMxh8cU6m"},"outputs":[],"source":["count = len(descriptions)\n","\n","plt.figure(figsize=(20, 14))\n","plt.imshow(similarity, vmin=0.1, vmax=0.3)\n","# plt.colorbar()\n","plt.yticks(range(count), texts, fontsize=18)\n","plt.xticks([])\n","for i, image in enumerate(original_images):\n","    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n","for x in range(similarity.shape[1]):\n","    for y in range(similarity.shape[0]):\n","        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n","\n","for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n","  plt.gca().spines[side].set_visible(False)\n","\n","plt.xlim([-0.5, count - 0.5])\n","plt.ylim([count + 0.5, -2])\n","\n","plt.title(\"Cosine similarity between text and image features\", size=20)"]},{"cell_type":"markdown","source":["## Calculating contrastive loss\n","\n","We normalize the features and calculate the InfoNCE loss within the 8 image-text pairs."],"metadata":{"id":"JiQdqhNFt836"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","def InfoNCE(similarity):\n","    similarity = torch.from_numpy(similarity)\n","    n = similarity.shape[0]\n","\n","    ### Q3 TODO ###\n","\n","\n","\n","    loss = None\n","\n","    ###############\n","\n","    return loss\n","\n","print(InfoNCE(similarity))\n","print(InfoNCE(similarity[:4, :4]))"],"metadata":{"id":"gIjwlludumJ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"alePijoXy6AH"},"source":["# Zero-Shot Image Classification\n","\n","You can classify images using the cosine similarity (times 100) as the logits to the softmax operation."]},{"cell_type":"code","source":["from torchvision.datasets import CIFAR100\n","\n","cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)\n","cifar100_test = CIFAR100(os.path.expanduser(\"~/.cache\"), train=False, transform=preprocess, download=True)\n","\n","prompt = \"this is a photo of a {}\"\n","text_descriptions = [prompt.format(label) for label in cifar100.classes]\n","text_tokens = clip.tokenize(text_descriptions).cuda()"],"metadata":{"id":"2zaEoR0Fmkzn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4z1fm9vCpSR"},"outputs":[],"source":["with torch.no_grad():\n","    text_features = model.encode_text(text_tokens).float()\n","    text_features /= text_features.norm(dim=-1, keepdim=True)\n","\n","text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n","top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6Ju_6IBE2Iz"},"outputs":[],"source":["plt.figure(figsize=(16, 16))\n","\n","for i, image in enumerate(original_images):\n","    plt.subplot(4, 4, 2 * i + 1)\n","    plt.imshow(image)\n","    plt.axis(\"off\")\n","\n","    plt.subplot(4, 4, 2 * i + 2)\n","    y = np.arange(top_probs.shape[-1])\n","    plt.grid()\n","    plt.barh(y, top_probs[i])\n","    plt.gca().invert_yaxis()\n","    plt.gca().set_axisbelow(True)\n","    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n","    plt.xlabel(\"probability\")\n","\n","plt.subplots_adjust(wspace=0.5)\n","plt.show()"]},{"cell_type":"markdown","source":["## Evaluate zero-shot CLIP on CIFAR 100"],"metadata":{"id":"2NgyhFOTP_RZ"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","import tqdm\n","\n","target = cifar100.targets\n","target_test = cifar100_test.targets\n","\n","train_loader = DataLoader(cifar100, batch_size=16, shuffle=True, num_workers=0)\n","test_loader = DataLoader(cifar100_test, batch_size=16, shuffle=False, num_workers=0)"],"metadata":{"id":"3l3mkQ6nNJyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a photo of a {}\"\n","text_descriptions = [prompt.format(label) for label in cifar100.classes]\n","text_tokens = clip.tokenize(text_descriptions).cuda()\n","\n","text_features = model.encode_text(text_tokens).float()\n","text_features /= text_features.norm(dim=-1, keepdim=True)"],"metadata":{"id":"cCpF1hqoNOsa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def evaluate():\n","    acc = 0.0\n","    num_samples = len(target_test)\n","\n","    for batch in tqdm.tqdm(test_loader):\n","        input_image = batch[0].cuda()\n","        input_label = batch[1].cuda()\n","\n","        ### Q4 TODO ###\n","\n","        image_features = None\n","        image_features /= image_features.norm(dim=-1, keepdim=True)\n","\n","        similarity = None\n","\n","        ###############\n","\n","        probs = (100.0 * similarity).softmax(dim=-1)\n","\n","        top_label = torch.argmax(probs, dim=1)\n","        prediction = torch.eq(input_label, top_label)\n","        acc += prediction.sum()\n","\n","    accuracy = acc / num_samples\n","\n","    return accuracy.item()\n","\n","print(\"Acc: , \", evaluate())"],"metadata":{"id":"mL2Kghm-NJ0T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Linear Probing"],"metadata":{"id":"DfoZuLIsPTml"}},{"cell_type":"code","source":["class LinearProbing(nn.Module):\n","    def __init__(self, num_classes=100):\n","        super(LinearProbing, self).__init__()\n","        self.fully_connected = nn.Linear(512, num_classes, bias=True)\n","\n","    def forward(self, x):\n","        x = self.fully_connected(x)\n","        return x\n","\n","lp_model = LinearProbing().cuda()"],"metadata":{"id":"orrMDcVqwFW3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_one_epoch(train_loader):\n","    running_loss = 0.0\n","    last_loss = 0.0\n","    num_samples = len(cifar100)\n","    optimizer = torch.optim.SGD(lp_model.parameters(), lr=2e-2, momentum=0.9)\n","\n","    for i, batch in tqdm.tqdm(enumerate(train_loader)):\n","\n","        input_image = batch[0].cuda()\n","        input_label = batch[1]\n","\n","        labels = torch.eye(100)[input_label].cuda()\n","\n","        optimizer.zero_grad()\n","\n","        ### Q5 TODO ###\n","\n","        image_features = None\n","\n","        outputs = None\n","\n","        ###############\n","\n","        loss = F.cross_entropy(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        if i % 100 == 99:\n","            last_loss = running_loss / 100 # loss per batch\n","            print('  batch {} loss: {}'.format(i + 1, last_loss))\n","            running_loss = 0.0\n","\n","    return last_loss"],"metadata":{"id":"_2chzQUNwId2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def evaluate_lp():\n","    acc = 0.0\n","    num_samples = len(target_test)\n","\n","    for batch in tqdm.tqdm(test_loader):\n","        input_image = batch[0].cuda()\n","        input_label = batch[1].cuda()\n","\n","        image_features = model.encode_image(input_image).float()\n","        #image_features /= image_features.norm(dim=-1, keepdim=True)\n","\n","        outputs = lp_model(image_features)\n","        probs = outputs.softmax(dim=-1)\n","\n","        top_label = torch.argmax(probs, dim=1)\n","        prediction = torch.eq(input_label, top_label)\n","        acc += prediction.sum()\n","\n","    accuracy = acc / num_samples\n","\n","    return accuracy.item()\n","\n","print(\"Acc: , \", evaluate_lp())"],"metadata":{"id":"usJTvMNBnB48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch_number = 0\n","EPOCHS = 1\n","\n","for epoch in range(EPOCHS):\n","    print('EPOCH {}:'.format(epoch + 1))\n","    model.train(False)\n","    lp_model.train(True)\n","    avg_loss = train_one_epoch(train_loader)\n","    print(evaluate_lp())"],"metadata":{"id":"KrNLDuJ-wPQS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Applications: ZegFormer"],"metadata":{"id":"TQ4kZeO2eV6s"}},{"cell_type":"code","source":["# Install detectron2\n","!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n","!git clone https://github.com/dingjiansw101/ZegFormer.git\n","%cd ZegFormer\n","\n","import sys\n","sys.path.insert(0,'/content/ZegFormer')"],"metadata":{"id":"SXxc2_P0eVQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install timm\n","!pip install -r requirements.txt"],"metadata":{"id":"7Hbx8EhK72Ee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import glob\n","import multiprocessing as mp\n","import os\n","\n","# fmt: off\n","import sys\n","sys.path.insert(1, os.path.join(sys.path[0], '..'))\n","# fmt: on\n","\n","import tempfile\n","import time\n","import warnings\n","\n","import cv2\n","import numpy as np\n","import tqdm\n","\n","from detectron2.config import get_cfg\n","from detectron2.data.detection_utils import read_image\n","from detectron2.projects.deeplab import add_deeplab_config\n","from detectron2.utils.logger import setup_logger\n","\n","from mask_former import add_mask_former_config\n","from demo.predictor import VisualizationDemo"],"metadata":{"id":"H0Q763T26aeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","lst = [\"sky\", \"airplane\", \"road\"]\n","json.dump(lst, open(\"cls.json\", \"w\"))"],"metadata":{"id":"lwkzwTbcH9n6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget http://images.cocodataset.org/val2017/000000005477.jpg -q -O input.jpg\n","!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1bA6DXr9VOMsRkU0vyY2EpGRkyQnhnze3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1bA6DXr9VOMsRkU0vyY2EpGRkyQnhnze3\" -O weights.pth && rm -rf ~/cookies.txt"],"metadata":{"id":"s4zol97tJf5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import atexit\n","import bisect\n","import multiprocessing as mp\n","from collections import deque\n","\n","import cv2\n","import torch\n","\n","from detectron2.data import MetadataCatalog\n","from detectron2.engine.defaults import DefaultPredictor\n","from detectron2.utils.video_visualizer import VideoVisualizer\n","from detectron2.utils.visualizer import ColorMode, Visualizer\n","from types import SimpleNamespace\n","\n","\n","class VisualizationDemo(object):\n","    def __init__(self, cfg, instance_mode=ColorMode.IMAGE, parallel=False):\n","        \"\"\"\n","        Args:\n","            cfg (CfgNode):\n","            instance_mode (ColorMode):\n","            parallel (bool): whether to run the model in different processes from visualization.\n","                Useful since the visualization logic can be slow.\n","        \"\"\"\n","        self.metadata = MetadataCatalog.get(\n","            cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else \"__unused\"\n","        )\n","        ns = SimpleNamespace()\n","        ns.stuff_classes = demo.predictor.model.sem_seg_head.predictor.test_class_texts\n","        self.metadata = None\n","        self.cpu_device = torch.device(\"cpu\")\n","        self.instance_mode = instance_mode\n","\n","        self.predictor = DefaultPredictor(cfg)\n","\n","        ns.stuff_classes = self.predictor.model.sem_seg_head.predictor.test_class_texts\n","        self.metadata = ns\n","\n","    def run_on_image(self, image):\n","        \"\"\"\n","        Args:\n","            image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n","                This is the format used by OpenCV.\n","        Returns:\n","            predictions (dict): the output of the model.\n","            vis_output (VisImage): the visualized image output.\n","        \"\"\"\n","        vis_output = None\n","        predictions = self.predictor(image)\n","        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n","        image = image[:, :, ::-1]\n","        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n","        vis_output = visualizer.draw_sem_seg(\n","                predictions[\"sem_seg\"].argmax(dim=0).to(self.cpu_device)\n","            )\n","\n","\n","        return predictions, vis_output"],"metadata":{"id":"NkXyQDY03Mt4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import easydict\n","def setup_cfg(args):\n","    # load config from file and command-line arguments\n","    cfg = get_cfg()\n","    add_deeplab_config(cfg)\n","    add_mask_former_config(cfg)\n","    cfg.merge_from_file(args.config_file)\n","    cfg.merge_from_list(args.opts)\n","\n","    cfg.MODEL.SEM_SEG_HEAD.TRAIN_CLASS_JSON = \"cls.json\"\n","    cfg.MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON = \"cls.json\"\n","\n","    cfg.freeze()\n","    return cfg\n","\n","def get_parser():\n","    parser = argparse.ArgumentParser(description=\"Detectron2 demo for builtin configs\")\n","    parser.add_argument(\n","        \"--config-file\",\n","        default=\"configs/ade20k-150/maskformer_R50_bs16_160k.yaml\",\n","        metavar=\"FILE\",\n","        help=\"path to config file\",\n","    )\n","    parser.add_argument(\n","        \"--input\",\n","        nargs=\"+\",\n","        default=\"input.png\",\n","        help=\"A list of space separated input images; \"\n","        \"or a single glob pattern such as 'directory/*.jpg'\",\n","    )\n","    parser.add_argument(\n","        \"--output\",\n","        default=\"./\",\n","        help=\"A file or directory to save output visualizations. \"\n","        \"If not given, will show output in an OpenCV window.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--confidence-threshold\",\n","        type=float,\n","        default=0.5,\n","        help=\"Minimum score for instance predictions to be shown\",\n","    )\n","    parser.add_argument(\n","        \"--opts\",\n","        help=\"Modify config options using the command-line 'KEY VALUE' pairs\",\n","        default=[],\n","        nargs=argparse.REMAINDER,\n","    )\n","    return parser\n","\n","args = easydict.EasyDict({\"config_file\": \"configs/coco-stuff/zegformer_R101_bs32_60k_vit16_coco-stuff_gzss_eval.yaml\",\n","                          \"input\": \"input.png\",\n","                          \"output\": \"./\",\n","                          \"opts\": [\"MODEL.WEIGHTS\", \"weights.pth\", \"MODEL.MASK_FORMER.ENSEMBLING_ALL_CLS\", \"False\", \"MODEL.MASK_FORMER.ENSEMBLING\", \"False\"]})\n","cfg = setup_cfg(args)\n","demo = VisualizationDemo(cfg)\n","demo.predictor.model.gzero_calibrate = -1.0"],"metadata":{"id":"GxECZsyg89VU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img = read_image(\"input.jpg\", format=\"BGR\")\n","predictions, visualized_output = demo.run_on_image(img)\n","visualized_output.fig"],"metadata":{"id":"dqNWcjj7Kq6Q"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["53N4k0pj_9qL","xwSB5jZki3Cj","ZB7DroL3NE83","4W8ARJVqBJXs"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}