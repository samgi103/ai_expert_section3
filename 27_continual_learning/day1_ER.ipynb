{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import data_handler\n",
    "from sklearn.utils import shuffle\n",
    "import trainer\n",
    "import networks\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "# general experimental setting \n",
    "args['date'] = '230828'\n",
    "args['dataset'] = 'MNIST'\n",
    "args['trainer'] = 'er' # ER or EWC\n",
    "args['seed'] = 0\n",
    "args['output_path'] = '' # this is corrected at the next cell\n",
    "\n",
    "# setting for continual learning \n",
    "args['tasknum'] = 5\n",
    "\n",
    "#hyperparameter for optimization\n",
    "args['batch_size'] = 256\n",
    "args['lr'] = 0.001\n",
    "args['epochs'] = 10\n",
    "args['decay'] = 0 # weight decay (L2 penaly for general regularization)\n",
    "\n",
    "args['schedule_milestone'] = [7] #Decrease learning rate at these epochs\n",
    "args['gamma'] = 0.2\n",
    "\n",
    "\n",
    "#hyperparameter for ER\n",
    "args['memory_size'] = 500\n",
    "args['ratio'] = 0.5\n",
    "\n",
    "# for GPU, if you cannot use GPU, set device as None\n",
    "device = 0\n",
    "if device is not None:\n",
    "    torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for saving datasets, output files and trained models.\n",
    "if not os.path.isdir('dat'):\n",
    "    print('Make directory for dataset')\n",
    "    os.makedirs('dat')\n",
    "\n",
    "if not os.path.isdir('result_data'):\n",
    "    print('Make directory for saving results')\n",
    "    os.makedirs('result_data')\n",
    "\n",
    "if not os.path.isdir('trained_model'):\n",
    "    print('Make directory for saving trained models')\n",
    "    os.makedirs('trained_model')\n",
    "\n",
    "# Make filename of a file for logging a result\n",
    "log_name = '{}_{}_{}_{}_memsize_{}_lr_{}_batch_{}_epoch_{}'.format(args['date'], args['dataset'], args['trainer'],args['seed'], \n",
    "                                                                       args['memory_size'], args['lr'], args['batch_size'], args['epochs'])\n",
    "\n",
    "if args['output_path'] == '':\n",
    "    args['output_path'] = './result_data/' + log_name + '.txt'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed for deterministic results\n",
    "np.random.seed(args['seed'])\n",
    "random.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# device = torch.device(\"gpu\")\n",
    "# torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a dataset and a dataloader that outputs a task sequentially\n",
    "print('Load data...')\n",
    "data_dict = None\n",
    "dataset = data_handler.DatasetFactory.get_dataset(args['dataset'], args['tasknum'])\n",
    "task_info = dataset.task_info\n",
    "print('\\nTask info =', task_info)\n",
    "\n",
    "# Loader used for training data\n",
    "shuffle_idx = shuffle(np.arange(dataset.classes), random_state=args['seed'])\n",
    "\n",
    "# list of dataloaders: it consists of dataloaders for each task\n",
    "train_dataset_loaders = data_handler.make_ContinualLoaders(dataset.train_data,\n",
    "                                                        dataset.train_labels,\n",
    "                                                        task_info,\n",
    "                                                        transform=dataset.train_transform,\n",
    "                                                        shuffle_idx = shuffle_idx,\n",
    "                                                        data_dict = data_dict,\n",
    "                                                       )\n",
    "\n",
    "test_dataset_loaders = data_handler.make_ContinualLoaders(dataset.test_data,\n",
    "                                                       dataset.test_labels,\n",
    "                                                       task_info,\n",
    "                                                       transform=dataset.test_transform,\n",
    "                                                       shuffle_idx = shuffle_idx,\n",
    "                                                       data_dict = data_dict,\n",
    "                                                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required model\n",
    "# myModel = networks.ModelFactory.get_model(args['dataset'], args['trainer'], task_info).to(device)\n",
    "if device is not None:\n",
    "    myModel = networks.ModelFactory.get_model(args['dataset'], args['trainer'], task_info).to(device)\n",
    "else:\n",
    "    myModel = networks.ModelFactory.get_model(args['dataset'], args['trainer'], task_info)\n",
    "\n",
    "# Define the optimizer used in the experiment\n",
    "optimizer = torch.optim.Adam(myModel.parameters(), lr=args['lr'], weight_decay=args['decay'])\n",
    "\n",
    "# Initilize the evaluators used to measure the performance of the system.\n",
    "t_classifier = trainer.EvaluatorFactory.get_evaluator(\"trainedClassifier\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(trainer.GenericTrainer):\n",
    "    def __init__(self, model, args, optimizer, evaluator, task_info):\n",
    "        super().__init__(model, args, optimizer, evaluator, task_info)\n",
    "        \n",
    "        self.memory_size = args['memory_size']\n",
    "        self.replay_memory = {}\n",
    "        self.memory_ratio = args['ratio']\n",
    "        self.N_total_data = 0\n",
    "\n",
    "    def train(self, train_loader, test_loader, t, device = None):\n",
    "        \n",
    "        self.device = device\n",
    "        self.setup_training(self.lr)\n",
    "        # Do not update self.t\n",
    "        \n",
    "        #update the total number of data we've seen\n",
    "        self.N_total_data += len(train_loader)\n",
    "        \n",
    "        # Now, you can update self.t\n",
    "        self.t = t\n",
    "        if self.t != 0:\n",
    "            batch_size = round(self.batch_size * (1-self.memory_ratio))\n",
    "            self.train_iterator = torch.utils.data.DataLoader(train_loader, batch_size=batch_size, shuffle=True)\n",
    "        else:\n",
    "            self.train_iterator = torch.utils.data.DataLoader(train_loader, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_iterator = torch.utils.data.DataLoader(test_loader, 100, shuffle=False)\n",
    "        self.fisher_iterator = torch.utils.data.DataLoader(train_loader, batch_size=20, shuffle=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            for samples in self.train_iterator:\n",
    "                data, target = samples\n",
    "                if device is not None:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                if self.t != 0:\n",
    "                    data, target, taskid = self.sample_with_memory(data, target)\n",
    "                    output = self.model(data)\n",
    "                    tmp = torch.zeros_like(output[0]).to(device)\n",
    "                    for _t in range(self.t+1):\n",
    "                        tmp[taskid==_t] = output[_t][taskid==_t]\n",
    "                    output = tmp\n",
    "                else:\n",
    "                    output = self.model(data)[t]\n",
    "                loss_CE = self.criterion(output,target)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                (loss_CE).backward()\n",
    "                self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "            train_loss,train_acc = self.evaluator.evaluate(self.model, self.train_iterator, t, self.device)\n",
    "            num_batch = len(self.train_iterator)\n",
    "            print('| Epoch {:3d} | Train: loss={:.3f}, acc={:5.1f}% |'.format(epoch+1,train_loss,100*train_acc),end='')\n",
    "            test_loss,test_acc=self.evaluator.evaluate(self.model, self.test_iterator, t, self.device)\n",
    "            print(' Test: loss={:.3f}, acc={:5.1f}% |'.format(test_loss,100*test_acc),end='')\n",
    "            print()\n",
    "        self.update_memory(self.replay_memory, self.train_iterator, self.memory_size, device)\n",
    "        \n",
    "    def criterion(self,output,targets):\n",
    "        \"\"\"\n",
    "        Arguments: output (The output logit of self.model), targets (Ground truth label)\n",
    "        Return: loss function for the regularization-based continual learning\n",
    "        \n",
    "        For the hyperparameter on regularization, please use self.lamb\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.ce(output,targets)\n",
    "\n",
    "\n",
    "    def sample_with_memory(self, data, target):\n",
    "        \"\"\"\n",
    "        Arguments : data, target that are sampled from a dataset of current task\n",
    "        Return : Combined data and target with memory_data and memo\n",
    "        \"\"\"\n",
    "        #######################################################################################\n",
    "        #write your code\n",
    "        \n",
    "        #######################################################################################\n",
    "        \n",
    "        return data, target, taskid\n",
    "        \n",
    "    def update_memory(self, memory, loader, memory_size, device = None):\n",
    "        \n",
    "        #######################################################################################\n",
    "        #write your code\n",
    "        \n",
    "        #######################################################################################\n",
    "\n",
    "        \n",
    "        print('Memory update ended')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer object used for training\n",
    "myTrainer = Trainer(myModel, args, optimizer, t_classifier, task_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "\n",
    "utils.print_model_report(myModel)\n",
    "utils.print_optimizer_config(optimizer)\n",
    "print('-' * 100)\n",
    "\n",
    "# Loop tasks\n",
    "acc = np.zeros((len(task_info), len(task_info)), dtype=np.float32)\n",
    "lss = np.zeros((len(task_info), len(task_info)), dtype=np.float32)\n",
    "for t, ncla in task_info:\n",
    "    print(\"tasknum:\", t)\n",
    "    # Add new classes to the train, and test iterator\n",
    "\n",
    "    train_loader = train_dataset_loaders[t]\n",
    "    test_loader = test_dataset_loaders[t]\n",
    "    myTrainer.train(train_loader, test_loader, t, device)\n",
    "\n",
    "    for u in range(t+1):\n",
    "        test_loader = test_dataset_loaders[u]\n",
    "        test_iterator = torch.utils.data.DataLoader(test_loader, 100, shuffle=False)\n",
    "        test_loss, test_acc = t_classifier.evaluate(myTrainer.model, test_iterator, u, device)\n",
    "        print('>>> Test on task {:2d}: loss={:.3f}, acc={:5.1f}% <<<'.format(u, test_loss, 100 * test_acc))\n",
    "        acc[t, u] = test_acc\n",
    "        lss[t, u] = test_loss\n",
    "\n",
    "    print('Average accuracy={:5.1f}%'.format(100 * np.mean(acc[t,:t+1])))\n",
    "\n",
    "    print('Save at ' + args['output_path'])\n",
    "    np.savetxt(args['output_path'], acc, '%.4f')\n",
    "    torch.save(myModel.state_dict(), './trained_model/' + log_name + '_task_{}.pt'.format(t))\n",
    "\n",
    "\n",
    "print('*' * 100)\n",
    "print('Accuracies =')\n",
    "for i in range(acc.shape[0]):\n",
    "    print('\\t', end='')\n",
    "    for j in range(acc.shape[1]):\n",
    "        print('{:5.1f}% '.format(100 * acc[i, j]), end='')\n",
    "    print()\n",
    "print('*' * 100)\n",
    "print('Done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_acc(file_name):\n",
    "    acc_arr = np.loadtxt(file_name)\n",
    "    avg_acc_arr = np.zeros(acc_arr.shape[1])\n",
    "    for i in range(acc_arr.shape[1]):\n",
    "        avg_acc_arr[i] = np.mean(acc_arr[i][:i+1])\n",
    "    \n",
    "    return avg_acc_arr\n",
    "filename_ewc = ''\n",
    "filename_er = ''\n",
    "results_er = avg_acc(filename_er)\n",
    "results_ewc = avg_acc(filename_ewc)\n",
    "print(results_er)\n",
    "print(results_ewc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "task_num = args['tasknum']\n",
    "task = np.arange(task_num) +1\n",
    "ax = plt.subplot(111)\n",
    "    \n",
    "# for key in results.keys():\n",
    "# ax.plot(task, results[key], label = key, linestyle = '-', marker = '.')\n",
    "ax.plot(task, results_ewc, label = 'EWC', linestyle = '-', marker = '.')\n",
    "ax.plot(task, results_er, label = 'ER', linestyle = '-', marker = '.')\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "# if len(task)>20:\n",
    "#     plt.xticks([i for i in task if i/10 == 0])\n",
    "# else:\n",
    "plt.xticks(task)\n",
    "\n",
    "plt.xlabel('Task',fontsize = 20)\n",
    "plt.ylabel('Accuracy',fontsize = 20)\n",
    "\n",
    "ax.legend(loc = 'center right', bbox_to_anchor=(1.3, 0.5))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
