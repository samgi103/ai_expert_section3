{"cells":[{"cell_type":"markdown","metadata":{"id":"dFETUmNgQkEQ"},"source":["# Domain Specific Batch Normalization (DSBN)\n","\n"," > Domain-Specific Batch Normalization for Unsupervised Domain Adaptation (DSBN) (https://arxiv.org/abs/1906.03950)\n","\n","## Overview\n","\n","DSBN은 domain마다 고유의 batch normalization (BN) layer를 사용해, 각 BN layer가 맡은 domain의 정보를 BN parameter를 통해 학습합니다.\n","\n","그 후, normalization을 하면서 domain-specific한 정보를 제거하고, 모델로 하여금 domain-invariant feature를 학습할 수 있게 한 method입니다.\n","\n","DSBN은 2 stage로 학습이 되는데, 이번 실습은 그 중 첫 번째 stage에 대한 학습을 구현하는 실습입니다.\n","\n","<img src = \"https://github.com/wgchang/DSBN/raw/master/captions/dsbn.jpg\">"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dkoZK7JWQkEV","executionInfo":{"status":"ok","timestamp":1692342692555,"user_tz":-540,"elapsed":304,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["import argparse\n","import logging\n","import pprint\n","import datetime\n","import sys\n","import random\n","from collections import defaultdict\n","import math\n","\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","from torch.utils import data\n","from torchvision.datasets import MNIST, SVHN\n","from torchvision import transforms\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"63i2ltPuQkEY"},"source":["일관적인 학습 결과를 위해 random seed를 고정하겠습니다."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Z0rinIsBQkEZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692342693206,"user_tz":-540,"elapsed":5,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"b539f141-3f32-4ec5-d231-fba10918c0b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ransom Seed: 2023\n"]}],"source":["# basic random seed\n","import os\n","import random\n","import numpy as np\n","\n","random_seed= 2023\n","\n","def seedBasic(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","\n","# torch random seed\n","import torch\n","def seedTorch(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# basic + torch\n","def seedEverything(seed):\n","    seedBasic(seed)\n","    seedTorch(seed)\n","    print(f\"Ransom Seed: {seed}\")\n","\n","\n","seedEverything(2023)"]},{"cell_type":"markdown","metadata":{"id":"8foX3V75QkEZ"},"source":["# 모델 구현하기\n","\n","DSBN을 구현하고, 이를 이용해 DSBN을 포함한 LeNet을 구현해봅니다.\n","\n","<br>\n","<br>\n","\n","\n","## DSBN(Domain Specific Batch Normalization) 구현\n","먼저, 이번 실습에서 가장 핵심이 되는 DSBN을 정의해줍니다.\n","torch에 이미 정의된 BatchNorm2d를 이용하여 Domain Specific한 Batch Normalization을 구현할 수 있습니다.\n","\n","- nn.BatchNorm2d\n","\n","<img src=\"https://ifh.cc/g/gdkh0f.png\">\n","\n","  - num_features : input size (N,C,H,W)에서 C를 의미합니다.\n","  - eps : Numerical stability를 위해 분모에 추가된 값으로서 default는 0.1입니다. (위의 식에서 ϵ에 해당되며 분모가 0이 되어 NaN이 되는것을 방지)\n","  - momentum : running_mean과 running_var의 계산을 위해 사용되는 값으로서 default는 0.1입니다. (Trainining을 수행시에는 batch단위의 평균과 분산으로 batch-norm을 수행하고, Test를 수행시에는 축적된 running_mean/variance를 사용합니다)\n","  - affine : Boolean value이며, True로 설정시에, 이 모듈에 학습가능한 affine parameter가 있습니다. Default는 True입니다. (False로 설정시에 위의식에서 γ=1, β=0)\n","  - track_running_stats : Boolean value이며, True로 설정시에 이 모듈은 running mean과 variance를 track하고 False로 설정시에는 이 모듈이 track하지 않고, running_mean과 running_var를 None으로 초기화하며, 이 값이 None인 경우에는 이 모듈은 항상 batch statistics를 사용합니다. Default는 True입니다.\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"yP1MifRxQkEa","executionInfo":{"status":"ok","timestamp":1692342693206,"user_tz":-540,"elapsed":3,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["class DomainSpecificBatchNorm2d(nn.Module):\n","    _version = 2\n","\n","    def __init__(self, num_features, num_classes, eps=1e-5, momentum=0.1, affine=True,\n","                 track_running_stats=True):\n","        super(DomainSpecificBatchNorm2d, self).__init__()\n","        ############ TODO #############\n","        '''\n","        https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n","        위 링크에서 BatchNorm2d를 참고하여 다음의 self.bns를 채우세요.\n","        '''\n","        # self.bns = []\n","        self.bns = nn.ModuleList(\n","            [nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats) for _ in range(num_classes)])\n","\n","        ############ TODO #############\n","\n","    def reset_running_stats(self):\n","        for bn in self.bns:\n","            bn.reset_running_stats()\n","\n","    def reset_parameters(self):\n","        for bn in self.bns:\n","            bn.reset_parameters()\n","\n","    def _check_input_dim(self, input):\n","        if input.dim() != 4:\n","            raise ValueError('expected 4D input (got {}D input)'\n","                             .format(input.dim()))\n","\n","    def forward(self, x, domain_label):\n","        self._check_input_dim(x)\n","        bn = self.bns[domain_label[0]]\n","        return bn(x), domain_label\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Y0pyKhaLQkEb","executionInfo":{"status":"ok","timestamp":1692342693206,"user_tz":-540,"elapsed":3,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["def init_weights(obj):\n","    for m in obj.modules():\n","        if isinstance(m, nn.Conv2d):\n","            # init.xavier_normal_(m.weight)\n","            m.weight.data.normal_(0, 0.01).clamp_(min=-0.02, max=0.02)\n","            try:\n","                m.bias.data.zero_()\n","            except AttributeError:\n","                # no bias\n","                pass\n","        if isinstance(m, nn.Linear):\n","            m.weight.data.normal_(0, 0.01).clamp_(min=-0.02, max=0.02)\n","            try:\n","                m.bias.data.zero_()\n","            except AttributeError:\n","                # no bias\n","                pass\n","        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n","            m.reset_parameters()\n","        elif isinstance(m, nn.Embedding):\n","            init.normal_(m.weight, 0, 0.01)"]},{"cell_type":"markdown","source":["## LeNet 구현하기\n","다음으로 LeNet을 구현해봅니다. LeNet의 기본 구조는 다음과 같습니다.\n","\n","<img src='https://miro.medium.com/v2/resize:fit:1204/format:webp/1*9MRcNBz9uHXplXzd3ii6VQ.png'>\n","\n","이번 실습에서는 LeNet의 기본 구조를 따라가되, 몇 가지 파라미터를 수정하고 Batch Normalization 모듈을 추가하여 실험을 진행할 것입니다."],"metadata":{"id":"Eim0B1ic5TEz"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"xfnEiH0XQkEc","executionInfo":{"status":"ok","timestamp":1692342693207,"user_tz":-540,"elapsed":4,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["class LeNet(nn.Module):\n","    \"\"\"\"Network used for MNIST or USPS experiments.\"\"\"\n","\n","    def __init__(self, num_classes=10, weights_init_path=None):\n","        super(LeNet, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_channels = 3\n","        self.image_size = 28\n","        self.name = 'LeNet'\n","        self.setup_net()\n","\n","        if weights_init_path is not None:\n","            init_weights(self)\n","            self.load(weights_init_path)\n","        else:\n","            init_weights(self)\n","\n","    def setup_net(self):\n","        self.conv1 = nn.Conv2d(self.num_channels, 20, kernel_size=5)\n","        self.bn1 = nn.BatchNorm2d(20)\n","        self.pool1 = nn.MaxPool2d(2)\n","        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n","        self.bn2 = nn.BatchNorm2d(50)\n","        self.pool2 = nn.MaxPool2d(2)\n","\n","        self.fc1 = nn.Linear(50 * 4 * 4, 500)\n","        self.fc2 = nn.Linear(500, self.num_classes)\n","\n","    def forward(self, x, with_ft=False):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.pool1(F.relu(x))\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.pool2(F.relu(x))\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        x = self.fc2(F.relu(x))\n","        feat = x\n","\n","        if with_ft:\n","            return x, feat\n","        else:\n","            return x\n","\n","    def load(self, init_path):\n","        net_init_dict = torch.load(init_path)\n","        init_weights(self)\n","        updated_state_dict = self.state_dict()\n","        print('load {} params.'.format(init_path))\n","        for k, v in updated_state_dict.items():\n","            if k in net_init_dict:\n","                if v.shape == net_init_dict[k].shape:\n","                    updated_state_dict[k] = net_init_dict[k]\n","                else:\n","                    print(\n","                        \"{0} params' shape not the same as pretrained params. Initialize with default settings.\".format(\n","                            k))\n","            else:\n","                print(\"{0} params does not exist. Initialize with default settings.\".format(k))\n","        self.load_state_dict(updated_state_dict)"]},{"cell_type":"markdown","source":["## DSBN을 포함한 LeNet 구현하기\n","이제 DSBN을 LeNet을 구현해봅니다."],"metadata":{"id":"dqsIpK3L8Phr"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"RO_0gEmVQkEd","executionInfo":{"status":"ok","timestamp":1692342693807,"user_tz":-540,"elapsed":604,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["class DSBNLeNet(nn.Module):\n","    \"\"\"\"Network used for MNIST or USPS experiments. Conditional Batch Normalization is added.\"\"\"\n","\n","    def __init__(self, num_classes=10, weights_init_path=None, num_domains=2):\n","        super(DSBNLeNet, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_channels = 3\n","        self.image_size = 28\n","        self.num_domains = num_domains\n","        self.name = 'DSBNLeNet'\n","        self.setup_net()\n","\n","        if weights_init_path is not None:\n","            init_weights(self)\n","            self.load(weights_init_path)\n","        else:\n","            init_weights(self)\n","\n","    def setup_net(self):\n","        ############ TODO #############\n","        '''\n","        위 LeNet을 참고하여 DSBN을 포함시킨 DSBNLeNet을 완성하세요.\n","        '''\n","        self.conv1 = nn.Conv2d(self.num_channels, 20, kernel_size = 5)\n","        self.bn1 = DomainSpecificBatchNorm2d(20, self.num_domains)\n","        self.pool1 = nn.MaxPool2d(2)\n","        self.conv2 = nn.Conv2d(20, 50, kernel_size = 5)\n","        self.bn2 = nn.DomainSpecificBatchNorm2d(50, self.num_domains)\n","        self.pool2 = nn.MaxPool2d(2)\n","\n","        self.fc1 = nn.Linear(50 * 4 * 4, 500)\n","        self.fc2 = nn.Linear(500, self.num_classes)\n","        ############ TODO #############\n","\n","    def forward(self, x, y, with_ft=False):\n","        x = self.conv1(x)\n","        x, _ = self.bn1(x, y)\n","        x = self.pool1(F.relu(x))\n","        x = self.conv2(x)\n","        x, _ = self.bn2(x, y)\n","        x = self.pool2(F.relu(x))\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        x = self.fc2(F.relu(x))\n","        feat = x\n","\n","        if with_ft:\n","            return x, feat\n","        else:\n","            return x\n","\n","    def load(self, init_path):\n","        net_init_dict = torch.load(init_path)\n","        init_weights(self)\n","        updated_state_dict = self.state_dict()\n","        print('load {} params.'.format(init_path))\n","        for k, v in updated_state_dict.items():\n","            if k in net_init_dict:\n","                if v.shape == net_init_dict[k].shape:\n","                    updated_state_dict[k] = net_init_dict[k]\n","                else:\n","                    print(\n","                        \"{0} params' shape not the same as pretrained params. Initialize with default settings.\".format(\n","                            k))\n","            else:\n","                print(\"{0} params does not exist. Initialize with default settings.\".format(k))\n","        self.load_state_dict(updated_state_dict)"]},{"cell_type":"markdown","metadata":{"id":"GJ6bhTTiQkEe"},"source":["학습에 필요한 추가적인 model들도 정의해줍니다."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"2mVGpqBdQkEe","executionInfo":{"status":"ok","timestamp":1692342693808,"user_tz":-540,"elapsed":9,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, in_features):\n","        super(Discriminator, self).__init__()\n","        self.in_features = in_features\n","\n","        self.discriminator = nn.Sequential(\n","            nn.Linear(self.in_features, 500),\n","            nn.ReLU(),\n","            nn.Linear(500, 500),\n","            nn.ReLU(),\n","            nn.Linear(500, 1)\n","        )\n","\n","        init_weights(self)\n","\n","    def forward(self, x):\n","        return self.discriminator(x)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Yv6CauO7QkEf","executionInfo":{"status":"ok","timestamp":1692342693808,"user_tz":-540,"elapsed":8,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["class Centroids(nn.Module):\n","  '''\n","  Learning Semantic Representations for Unsupervised Domain Adaptation\n","  http://proceedings.mlr.press/v80/xie18c/xie18c.pdf\n","  '''\n","  def __init__(self, feature_dim, num_classes, decay_const=0.3):\n","      super(Centroids, self).__init__()\n","      self.decay_const = decay_const\n","      self.num_classes = num_classes\n","      self.centroids = nn.Parameter(torch.randn(num_classes, feature_dim))\n","      self.centroids.requires_grad = False\n","      self.reset_parameters()\n","\n","  def reset_parameters(self):\n","      self.centroids.data.zero_()\n","\n","  def forward(self, x, y, y_mask=None):\n","      classes = torch.unique(y)\n","      current_centroids = []\n","      for c in range(self.num_classes):\n","          if c in classes:\n","              if y_mask is not None:\n","                  avg_c = torch.sum(x[(y == c) & y_mask, :], dim=0) / torch.sum((y == c) & y_mask).float()\n","              else:\n","                  avg_c = torch.sum(x[(y == c), :], dim=0) / torch.sum((y == c)).float()\n","              current_centroids.append(avg_c * self.decay_const + (1 - self.decay_const) * self.centroids[c:c + 1, :])\n","          else:\n","              current_centroids.append(self.centroids[c:c + 1, :])\n","      current_centroids = torch.cat(current_centroids, 0)\n","      return current_centroids\n"]},{"cell_type":"markdown","metadata":{"id":"9x5-QVKrQkEf"},"source":["# DataLoader 구현하기\n","\n","이번 실습을 위한 domain adaptation setting은 svhn --> mnist 입니다.\n","<br>\n","<br>\n","\n","### MNIST\n","MNIST 데이터셋은 손으로 쓴 숫자 이미지로 이루어진 대형 데이터셋이며,\n","\n","60,000개의 Training dataset과 10,000개의 Test dataset으로 이루어져 있습니다.\n","\n","<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/440px-MnistExamples.png'>\n","\n","<br>\n","<br>\n","\n","### SVHN\n","SVHN은 Google Street View에서 수집된 숫자 데이터셋으로,\n","\n","MNIST와 마찬가지로 0부터 9까지 숫자 10개의 이미지로 이루어진 데이터셋 입니다.\n","\n","<img src='https://production-media.paperswithcode.com/datasets/SVHN-0000000424-c12734ed_mMXUnWD.jpg' width=\"500\" height=\"300\">\n","\n","\n","데이터에 대해서 더 자세하게 알고 싶다면, 다음의 링크를 참조해주세요.\n","\n","https://kjhov195.github.io/2020-02-09-image_dataset_1/"]},{"cell_type":"markdown","metadata":{"id":"ATVjyy9TQkEf"},"source":["### source dataset으로 svhn, target dataset으로 mnist를 구현해주시길 바랍니다."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"bKPa_miaQkEg","executionInfo":{"status":"ok","timestamp":1692342693808,"user_tz":-540,"elapsed":8,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["MNIST_DIR = './data/mnist'\n","SVHN_DIR  = './data/svhn'\n","batch_size = 40\n","num_workers = 2\n","\n","def get_dataloaders(src_data_path = './data/svhn', trg_data_path = './data/mnist'):\n","    ############ TODO #############\n","    '''\n","    source_transform:\n","        1. resize to 28 by 28\n","        2. convert to tensor\n","        3. normalize by mean (0.5, 0.5, 0.5) and std (0.5, 0.5, 0.5)\n","\n","    target_transform:\n","        1. convert to RGB from Grayscale (Hint: Use transforms.Lambda)\n","        2. convert to tensor\n","        3. normalize by mean (0.5, 0.5, 0.5) and std (0.5, 0.5, 0.5)\n","\n","    source_train_dataset:\n","        SVHN을 이용해 구현\n","        (참고: https://pytorch.org/vision/main/generated/torchvision.datasets.SVHN.html#torchvision.datasets.SVHN)\n","\n","    target_train_dataset & target_val_dataset:\n","        MNIST로 구현\n","        (참고: https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html)\n","    '''\n","\n","\n","    # source_transform =\n","    # target_transform =\n","\n","    # source_train_dataset =\n","    # target_train_dataset =\n","    # target_val_dataset =\n","\n","    source_transform = transforms.Compose([\n","        transforms.Resize((28, 28)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n","    ])\n","\n","    target_transform = transforms.Compose([\n","        transforms.Lambda(lambda x: x.convert(\"RGB\")),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","    ])\n","\n","    source_train_dataset = SVHN(root = src_data_path,\n","                                split = 'train',\n","                                download = True,\n","                                transform = source_transform)\n","\n","    target_train_dataset = MNIST(root = trg_data_path,\n","                                 train = True,\n","                                 download = True,\n","                                 transform = target_transform)\n","\n","    target_val_dataset = MNIST(root = trg_data_path,\n","                               train = False,\n","                               download = True,\n","                               transform = target_transform)\n","\n","    ############ TODO #############\n","\n","    source_train_dataloader = data.DataLoader(source_train_dataset,\n","                                              batch_size = batch_size,\n","                                              shuffle = True,\n","                                              num_workers=num_workers,\n","                                              drop_last=True,\n","                                              pin_memory=True)\n","\n","    target_train_dataloader = data.DataLoader(target_train_dataset,\n","                                              batch_size = batch_size,\n","                                              shuffle = True,\n","                                              num_workers=num_workers,\n","                                              drop_last=True,\n","                                              pin_memory=True)\n","\n","    target_val_dataloader = data.DataLoader(target_val_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers=num_workers,\n","                                            drop_last=True,\n","                                            pin_memory=True)\n","\n","    return source_train_dataloader, target_train_dataloader, target_val_dataloader"]},{"cell_type":"markdown","metadata":{"id":"y5dnPC_1QkEg"},"source":["# Utils for training\n","\n","학습에 필요한 다양한 기능들을 구현한 부분입니다."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"VMH2wHnGQkEg","executionInfo":{"status":"ok","timestamp":1692342693808,"user_tz":-540,"elapsed":8,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["def get_optimizer_params(modules, lr, weight_decay=0.0005, base_weight_factor=0.1):\n","    weights = []\n","    biases = []\n","    base_weights = []\n","    base_biases = []\n","\n","    module = modules\n","    for key, value in dict(module.named_parameters()).items():\n","        if value.requires_grad:\n","            if 'fc' in key or 'score' in key:\n","                if 'bias' in key:\n","                    biases += [value]\n","                else:\n","                    weights += [value]\n","            else:\n","                if 'bias' in key:\n","                    base_biases += [value]\n","                else:\n","                    base_weights += [value]\n","    if base_weight_factor:\n","        params = [\n","            {'params': weights, 'lr': lr, 'weight_decay': weight_decay},\n","            {'params': biases, 'lr': lr },\n","            {'params': base_weights, 'lr': lr * base_weight_factor, 'weight_decay': weight_decay},\n","            {'params': base_biases, 'lr': lr * base_weight_factor},\n","        ]\n","    else:\n","        params = [\n","            {'params': base_weights + weights, 'lr': lr, 'weight_decay': weight_decay},\n","            {'params': base_biases + biases, 'lr': lr},\n","        ]\n","    return params"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"zJhl9KArQkEh","executionInfo":{"status":"ok","timestamp":1692342693809,"user_tz":-540,"elapsed":8,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["class Monitor:\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self._cummulated_losses = defaultdict(lambda: 0.0)\n","        self._total_counts = defaultdict(lambda: 0)\n","\n","    def update(self, losses_dict):\n","        for key in losses_dict:\n","            self._cummulated_losses[key] += losses_dict[key]\n","            self._total_counts[key] += 1\n","\n","    @property\n","    def cummulated_losses(self):\n","        return self._cummulated_losses\n","\n","    @property\n","    def total_counts(self):\n","        return self._total_counts\n","\n","    @property\n","    def losses(self):\n","        losses = {}\n","        for k, v in self._cummulated_losses.items():\n","            if self._total_counts[k] > 0:\n","                losses[k] = v / float(self._total_counts[k])\n","            else:\n","                losses[k] = 0.0\n","        return losses\n","\n","    def __repr__(self):\n","        sorted_loss_keys = sorted([k for k in self._cummulated_losses.keys()])\n","        losses = self.losses\n","        repr_str = ''\n","        for key in sorted_loss_keys:\n","            repr_str += ', {0}={1:.4f}'.format(key, losses[key])\n","        return repr_str[2:]\n","\n","\n","def one_hot_encoding(y, n_classes):\n","    tensor_size = [y.size(i) for i in range(len(y.size()))]\n","    if tensor_size[-1] != 1:\n","        tensor_size += [1]\n","    tensor_size = tuple(tensor_size)\n","    y_one_hot = torch.zeros(tensor_size[:-1] + (n_classes,)).to(y.device).scatter_(len(tensor_size) - 1,\n","                                                                                   y.view(tensor_size), 1)\n","    return y_one_hot"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"0w6qtkkkQkEh","executionInfo":{"status":"ok","timestamp":1692342693809,"user_tz":-540,"elapsed":8,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["def lr_poly(base_lr, i_iter, alpha=10, beta=0.75, num_steps=250000):\n","    if i_iter < 0:\n","        return base_lr\n","    return base_lr / ((1 + alpha * float(i_iter) / num_steps) ** (beta))\n","\n","class LRScheduler:\n","    def __init__(self, learning_rate, num_steps=200000, alpha=10,\n","                 beta=0.75, base_weight_factor=False):\n","        self.learning_rate = learning_rate\n","        self.num_steps = num_steps\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.base_weight_factor = base_weight_factor\n","\n","    def __call__(self, optimizer, i_iter):\n","        lr_i_iter = i_iter\n","        lr = self.learning_rate\n","\n","        if len(optimizer.param_groups) == 1:\n","            optimizer.param_groups[0]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","        elif len(optimizer.param_groups) == 2:\n","            optimizer.param_groups[0]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","            optimizer.param_groups[1]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","        elif len(optimizer.param_groups) == 4:\n","            optimizer.param_groups[0]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","            optimizer.param_groups[1]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","            optimizer.param_groups[2]['lr'] = self.base_weight_factor * lr_poly(lr, lr_i_iter, alpha=self.alpha,\n","                                                                                beta=self.beta,\n","                                                                                num_steps=self.num_steps)\n","            optimizer.param_groups[3]['lr'] = self.base_weight_factor * lr_poly(lr, lr_i_iter, alpha=self.alpha,\n","                                                                                beta=self.beta,\n","                                                                                num_steps=self.num_steps)\n","        else:\n","            raise RuntimeError('Wrong optimizer param groups')\n","\n","    def current_lr(self, i_iter):\n","        return lr_poly(self.learning_rate, i_iter, alpha=self.alpha, beta=self.beta, num_steps=self.num_steps)\n"]},{"cell_type":"markdown","metadata":{"id":"CFs0KmUkQkEi"},"source":["# Train"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2Qpwqjn4QkEi","executionInfo":{"status":"ok","timestamp":1692342693809,"user_tz":-540,"elapsed":8,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["save_dir = './logs'\n","num_classes = 10\n","num_domains = 2 # source + target\n","num_source_domains = 1\n","num_target_domains = 1\n","\n","start_iter = 1\n","end_iter = 30000\n","adaptation_gamma = 10\n","\n","learning_rate = 0.001\n","weight_decay = 0.0\n","base_weight_factor = 0.1\n","\n","best_accuracy = 0.0\n","best_accuracy_each_c = 0.0\n","best_mean_val_accuracy = 0.0\n","best_total_val_accuracy = 0.0\n","\n","disp_interval = 10\n","save_interval = 500\n","domain_loss_adjust_factor = 0.1\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Df1HemmrQkEi","executionInfo":{"status":"ok","timestamp":1692342693810,"user_tz":-540,"elapsed":9,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["def adaptation_factor(p, gamma=10):\n","    p = max(min(p, 1.0), 0.0)\n","    den = 1.0 + math.exp(-gamma * p)\n","    lamb = 2.0 / den - 1.0\n","    return min(lamb, 1.0)\n","\n","def semantic_loss_calc(x, y, mean=True):\n","    loss = (x - y) ** 2\n","    if mean:\n","        return torch.mean(loss)\n","    else:\n","        return loss\n","\n","def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n","    maxk = max(topk)\n","    num_samples = target.size(0)\n","\n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].view(-1).float().sum(0)\n","        res.append(correct_k.div_(num_samples))\n","    return res\n","\n","\n","def accuracy_of_c(output, target, class_idx, topk=(1,)):\n","    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n","    maxk = max(topk)\n","    # num_samples = target.size(0)\n","    selection = target == class_idx\n","    target_selected = target[selection]\n","    output_selected = output[selection]\n","    num_samples = torch.sum(selection).float()\n","\n","    _, pred = output_selected.topk(maxk, 1, True, True)\n","    pred = pred.t().float()\n","    correct = pred.eq((target_selected.view(1, -1).expand_as(pred)).float())\n","\n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].view(-1).float().sum(0)\n","        res.append(correct_k.div_(num_samples))\n","    return res"]},{"cell_type":"markdown","metadata":{"id":"HzWISwUMQkEi"},"source":["# Train with no adaptation\n","\n","### Domain Adaptation 없이 학습 시켰을 때 결과를 보도록 하겠습니다."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"DrdiI5ESQkEj","executionInfo":{"status":"ok","timestamp":1692342693810,"user_tz":-540,"elapsed":8,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["def train_no_adapt(model, dataloaders, optimizer, lr_scheduler, ce_loss, start_time):\n","    source_train_loader, target_train_loader, target_val_loader = dataloaders\n","    source_train_loader_iter, target_train_loader_iter, target_val_loader_iter = map(iter, dataloaders)\n","\n","    best_accuracy = 0.0\n","    monitor = Monitor()\n","    for i_iter in range(start_iter, 10001):\n","        try:\n","            x_s, y_s = next(source_train_loader_iter)\n","        except StopIteration:\n","            source_train_loader_iter = iter(source_train_loader)\n","            x_s, y_s = next(source_train_loader_iter)\n","\n","        x_s, y_s = x_s.cuda(), y_s.cuda()\n","        current_lr = lr_scheduler.current_lr(i_iter)\n","\n","        # init optimizer\n","        optimizer.zero_grad()\n","        lr_scheduler(optimizer, i_iter)\n","\n","        ########################################################################################################\n","        #                                               Train                                                  #\n","        ########################################################################################################\n","\n","        pred_s, f_s = model(x_s, with_ft=True)\n","\n","        Closs_src = ce_loss(pred_s, y_s)\n","        monitor.update({\"Loss/Closs_src\": float(Closs_src)})\n","\n","        Floss = Closs_src\n","\n","        # Floss backward\n","        Floss.backward()\n","        optimizer.step()\n","\n","\n","        if i_iter % disp_interval == 0  and i_iter != 0:\n","            disp_msg = 'iter[{:8d}/{:8d}], '.format(i_iter, 10000)\n","            disp_msg += str(monitor)\n","            disp_msg += ', lr={:.6f}'.format(current_lr)\n","            print(disp_msg)\n","\n","            monitor.reset()\n","\n","        if i_iter % save_interval == 0 and i_iter != 0:\n","            print(\"Elapsed Time: {}\".format(datetime.datetime.now() - start_time))\n","            print(\"Start Evaluation at {:d}\".format(i_iter))\n","\n","            model.eval()\n","\n","            pred_vals = []\n","            y_vals = []\n","            x_val = None\n","            y_val = None\n","            pred_val = None\n","\n","            with torch.no_grad():\n","                for i, (x_val, y_val) in enumerate(target_val_loader):\n","                    y_vals.append(y_val.cpu())\n","                    x_val = x_val.cuda()\n","                    y_val = y_val.cuda()\n","\n","                    pred_val = model(x_val, with_ft=False)\n","                    pred_vals.append(pred_val.cpu())\n","\n","            pred_vals = torch.cat(pred_vals, 0)\n","            y_vals = torch.cat(y_vals, 0)\n","            total_val_accuracy = float(accuracy(pred_vals, y_vals, topk=(1,))[0])\n","\n","            val_accuracy_each_c = [(c_name, float(accuracy_of_c(pred_vals, y_vals,\n","                                                                class_idx=c, topk=(1,))[0]))\n","                                   for c, c_name in enumerate(range(num_classes))]\n","            print('\\nMNIST Accuracy of Each class')\n","            print(''.join([\"{:<25}: {:.2f}%\\n\".format(c_name, 100 * c_val_acc)\n","                            for c_name, c_val_acc in val_accuracy_each_c]))\n","\n","            mean_val_accuracy = float(\n","                torch.mean(torch.FloatTensor([c_val_acc for _, c_val_acc in val_accuracy_each_c])))\n","\n","            print('MNIST mean Accuracy: {:.2f}%'.format(100 * mean_val_accuracy))\n","            print(\"MNIST Accuracy: {:.2f}%\".format(total_val_accuracy * 100))\n","\n","            model.train()\n","\n","            val_accuracy = total_val_accuracy\n","\n","            del x_val, y_val, pred_val, pred_vals, y_vals\n","\n","            if val_accuracy > best_accuracy:\n","                #save best model\n","                best_accuracy = val_accuracy\n","                best_accuracy_each_c = val_accuracy_each_c\n","                best_mean_val_accuracy = mean_val_accuracy\n","                best_total_val_accuracy = total_val_accuracy\n","\n","                model = model.cuda()\n","\n","            print('\\nBest {MNIST} Accuracy of Each class')\n","            print(''.join([\"{:<25}: {:.2f}%\\n\".format(c_name, 100 * c_val_acc)\n","                           for c_name, c_val_acc in best_accuracy_each_c]))\n","            print('Best Accs: ' + ''.join([\"{:.2f}% \".format(100 * c_val_acc)\n","                                           for _, c_val_acc in best_accuracy_each_c]))\n","            print('Best mean Accuracy: {:.2f}%'.format(100 * best_mean_val_accuracy))\n","            print('Best Accuracy: {:.2f}%'.format(100 * best_total_val_accuracy))"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"uyGt8fY-QkEj","executionInfo":{"status":"ok","timestamp":1692342693810,"user_tz":-540,"elapsed":8,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["def no_adapt():\n","    start_time = datetime.datetime.now()\n","\n","    # make save_dir\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","\n","    dataloaders = get_dataloaders()\n","\n","    ###################################################################################################################\n","    #                                               Model Loading                                                     #\n","    ###################################################################################################################\n","    model = LeNet(num_classes = num_classes)\n","\n","    model.train(True)\n","    model = model.cuda()\n","    params = get_optimizer_params(model,\n","                                  lr = learning_rate,\n","                                  weight_decay=weight_decay,\n","                                  base_weight_factor=base_weight_factor)\n","\n","    ###################################################################################################################\n","    #                                               Train Configurations                                              #\n","    ###################################################################################################################\n","    ce_loss = nn.CrossEntropyLoss()\n","\n","    lr_scheduler = LRScheduler(learning_rate, end_iter, base_weight_factor=base_weight_factor)\n","\n","    optimizer = optim.Adam(params, betas=(0.9, 0.999))\n","\n","    train_no_adapt(model, dataloaders, optimizer, lr_scheduler, ce_loss, start_time)\n","    print('Total Time:  {}'.format((datetime.datetime.now() - start_time)))\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"1wvCaI1-QkEj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692342916840,"user_tz":-540,"elapsed":223038,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"9e711f6c-2790-480e-d08d-3fc12581ceda"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/svhn/train_32x32.mat\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 182040794/182040794 [00:03<00:00, 46132970.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 111333491.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 111440380.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 29521280.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 15191809.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n","\n","iter[      10/   10000], Loss/Closs_src=2.3011, lr=0.000998\n","iter[      20/   10000], Loss/Closs_src=2.2565, lr=0.000995\n","iter[      30/   10000], Loss/Closs_src=2.2474, lr=0.000993\n","iter[      40/   10000], Loss/Closs_src=2.1834, lr=0.000990\n","iter[      50/   10000], Loss/Closs_src=2.0746, lr=0.000988\n","iter[      60/   10000], Loss/Closs_src=2.0082, lr=0.000985\n","iter[      70/   10000], Loss/Closs_src=1.8850, lr=0.000983\n","iter[      80/   10000], Loss/Closs_src=1.6392, lr=0.000980\n","iter[      90/   10000], Loss/Closs_src=1.5203, lr=0.000978\n","iter[     100/   10000], Loss/Closs_src=1.3351, lr=0.000976\n","iter[     110/   10000], Loss/Closs_src=1.2181, lr=0.000973\n","iter[     120/   10000], Loss/Closs_src=1.1553, lr=0.000971\n","iter[     130/   10000], Loss/Closs_src=0.9906, lr=0.000969\n","iter[     140/   10000], Loss/Closs_src=0.9666, lr=0.000966\n","iter[     150/   10000], Loss/Closs_src=0.9805, lr=0.000964\n","iter[     160/   10000], Loss/Closs_src=0.9582, lr=0.000962\n","iter[     170/   10000], Loss/Closs_src=0.8775, lr=0.000960\n","iter[     180/   10000], Loss/Closs_src=0.8632, lr=0.000957\n","iter[     190/   10000], Loss/Closs_src=0.7459, lr=0.000955\n","iter[     200/   10000], Loss/Closs_src=0.8537, lr=0.000953\n","iter[     210/   10000], Loss/Closs_src=0.7751, lr=0.000951\n","iter[     220/   10000], Loss/Closs_src=0.7930, lr=0.000948\n","iter[     230/   10000], Loss/Closs_src=0.7570, lr=0.000946\n","iter[     240/   10000], Loss/Closs_src=0.7964, lr=0.000944\n","iter[     250/   10000], Loss/Closs_src=0.7478, lr=0.000942\n","iter[     260/   10000], Loss/Closs_src=0.6634, lr=0.000940\n","iter[     270/   10000], Loss/Closs_src=0.7343, lr=0.000937\n","iter[     280/   10000], Loss/Closs_src=0.7359, lr=0.000935\n","iter[     290/   10000], Loss/Closs_src=0.7288, lr=0.000933\n","iter[     300/   10000], Loss/Closs_src=0.7253, lr=0.000931\n","iter[     310/   10000], Loss/Closs_src=0.7147, lr=0.000929\n","iter[     320/   10000], Loss/Closs_src=0.7078, lr=0.000927\n","iter[     330/   10000], Loss/Closs_src=0.6551, lr=0.000925\n","iter[     340/   10000], Loss/Closs_src=0.6862, lr=0.000923\n","iter[     350/   10000], Loss/Closs_src=0.7326, lr=0.000921\n","iter[     360/   10000], Loss/Closs_src=0.5777, lr=0.000919\n","iter[     370/   10000], Loss/Closs_src=0.6943, lr=0.000916\n","iter[     380/   10000], Loss/Closs_src=0.7205, lr=0.000914\n","iter[     390/   10000], Loss/Closs_src=0.7266, lr=0.000912\n","iter[     400/   10000], Loss/Closs_src=0.5885, lr=0.000910\n","iter[     410/   10000], Loss/Closs_src=0.6530, lr=0.000908\n","iter[     420/   10000], Loss/Closs_src=0.6580, lr=0.000906\n","iter[     430/   10000], Loss/Closs_src=0.5970, lr=0.000904\n","iter[     440/   10000], Loss/Closs_src=0.5048, lr=0.000902\n","iter[     450/   10000], Loss/Closs_src=0.6180, lr=0.000900\n","iter[     460/   10000], Loss/Closs_src=0.5327, lr=0.000899\n","iter[     470/   10000], Loss/Closs_src=0.6284, lr=0.000897\n","iter[     480/   10000], Loss/Closs_src=0.6131, lr=0.000895\n","iter[     490/   10000], Loss/Closs_src=0.6331, lr=0.000893\n","iter[     500/   10000], Loss/Closs_src=0.5508, lr=0.000891\n","Elapsed Time: 0:00:18.248271\n","Start Evaluation at 500\n","\n","MNIST Accuracy of Each class\n","0                        : 35.41%\n","1                        : 50.75%\n","2                        : 75.48%\n","3                        : 86.44%\n","4                        : 49.59%\n","5                        : 82.74%\n","6                        : 21.92%\n","7                        : 90.37%\n","8                        : 27.62%\n","9                        : 28.74%\n","\n","MNIST mean Accuracy: 54.91%\n","MNIST Accuracy: 54.98%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 35.41%\n","1                        : 50.75%\n","2                        : 75.48%\n","3                        : 86.44%\n","4                        : 49.59%\n","5                        : 82.74%\n","6                        : 21.92%\n","7                        : 90.37%\n","8                        : 27.62%\n","9                        : 28.74%\n","\n","Best Accs: 35.41% 50.75% 75.48% 86.44% 49.59% 82.74% 21.92% 90.37% 27.62% 28.74% \n","Best mean Accuracy: 54.91%\n","Best Accuracy: 54.98%\n","iter[     510/   10000], Loss/Closs_src=0.5660, lr=0.000889\n","iter[     520/   10000], Loss/Closs_src=0.6178, lr=0.000887\n","iter[     530/   10000], Loss/Closs_src=0.7335, lr=0.000885\n","iter[     540/   10000], Loss/Closs_src=0.6077, lr=0.000883\n","iter[     550/   10000], Loss/Closs_src=0.6428, lr=0.000881\n","iter[     560/   10000], Loss/Closs_src=0.5198, lr=0.000880\n","iter[     570/   10000], Loss/Closs_src=0.5824, lr=0.000878\n","iter[     580/   10000], Loss/Closs_src=0.5934, lr=0.000876\n","iter[     590/   10000], Loss/Closs_src=0.5653, lr=0.000874\n","iter[     600/   10000], Loss/Closs_src=0.5485, lr=0.000872\n","iter[     610/   10000], Loss/Closs_src=0.5771, lr=0.000870\n","iter[     620/   10000], Loss/Closs_src=0.6411, lr=0.000869\n","iter[     630/   10000], Loss/Closs_src=0.5354, lr=0.000867\n","iter[     640/   10000], Loss/Closs_src=0.5543, lr=0.000865\n","iter[     650/   10000], Loss/Closs_src=0.6206, lr=0.000863\n","iter[     660/   10000], Loss/Closs_src=0.5863, lr=0.000861\n","iter[     670/   10000], Loss/Closs_src=0.5401, lr=0.000860\n","iter[     680/   10000], Loss/Closs_src=0.4528, lr=0.000858\n","iter[     690/   10000], Loss/Closs_src=0.6651, lr=0.000856\n","iter[     700/   10000], Loss/Closs_src=0.5044, lr=0.000854\n","iter[     710/   10000], Loss/Closs_src=0.4954, lr=0.000853\n","iter[     720/   10000], Loss/Closs_src=0.6108, lr=0.000851\n","iter[     730/   10000], Loss/Closs_src=0.5539, lr=0.000849\n","iter[     740/   10000], Loss/Closs_src=0.5313, lr=0.000848\n","iter[     750/   10000], Loss/Closs_src=0.6205, lr=0.000846\n","iter[     760/   10000], Loss/Closs_src=0.5163, lr=0.000844\n","iter[     770/   10000], Loss/Closs_src=0.5475, lr=0.000843\n","iter[     780/   10000], Loss/Closs_src=0.5448, lr=0.000841\n","iter[     790/   10000], Loss/Closs_src=0.4532, lr=0.000839\n","iter[     800/   10000], Loss/Closs_src=0.7087, lr=0.000838\n","iter[     810/   10000], Loss/Closs_src=0.4791, lr=0.000836\n","iter[     820/   10000], Loss/Closs_src=0.6373, lr=0.000834\n","iter[     830/   10000], Loss/Closs_src=0.4439, lr=0.000833\n","iter[     840/   10000], Loss/Closs_src=0.5143, lr=0.000831\n","iter[     850/   10000], Loss/Closs_src=0.5720, lr=0.000829\n","iter[     860/   10000], Loss/Closs_src=0.5204, lr=0.000828\n","iter[     870/   10000], Loss/Closs_src=0.5073, lr=0.000826\n","iter[     880/   10000], Loss/Closs_src=0.5131, lr=0.000825\n","iter[     890/   10000], Loss/Closs_src=0.4755, lr=0.000823\n","iter[     900/   10000], Loss/Closs_src=0.5904, lr=0.000821\n","iter[     910/   10000], Loss/Closs_src=0.5031, lr=0.000820\n","iter[     920/   10000], Loss/Closs_src=0.4929, lr=0.000818\n","iter[     930/   10000], Loss/Closs_src=0.5803, lr=0.000817\n","iter[     940/   10000], Loss/Closs_src=0.4448, lr=0.000815\n","iter[     950/   10000], Loss/Closs_src=0.5941, lr=0.000814\n","iter[     960/   10000], Loss/Closs_src=0.6139, lr=0.000812\n","iter[     970/   10000], Loss/Closs_src=0.5968, lr=0.000810\n","iter[     980/   10000], Loss/Closs_src=0.5984, lr=0.000809\n","iter[     990/   10000], Loss/Closs_src=0.5247, lr=0.000807\n","iter[    1000/   10000], Loss/Closs_src=0.4869, lr=0.000806\n","Elapsed Time: 0:00:28.712050\n","Start Evaluation at 1000\n","\n","MNIST Accuracy of Each class\n","0                        : 34.08%\n","1                        : 43.08%\n","2                        : 67.15%\n","3                        : 80.59%\n","4                        : 41.96%\n","5                        : 94.28%\n","6                        : 11.80%\n","7                        : 72.96%\n","8                        : 24.33%\n","9                        : 69.57%\n","\n","MNIST mean Accuracy: 53.98%\n","MNIST Accuracy: 53.85%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 35.41%\n","1                        : 50.75%\n","2                        : 75.48%\n","3                        : 86.44%\n","4                        : 49.59%\n","5                        : 82.74%\n","6                        : 21.92%\n","7                        : 90.37%\n","8                        : 27.62%\n","9                        : 28.74%\n","\n","Best Accs: 35.41% 50.75% 75.48% 86.44% 49.59% 82.74% 21.92% 90.37% 27.62% 28.74% \n","Best mean Accuracy: 54.91%\n","Best Accuracy: 54.98%\n","iter[    1010/   10000], Loss/Closs_src=0.5705, lr=0.000804\n","iter[    1020/   10000], Loss/Closs_src=0.4971, lr=0.000803\n","iter[    1030/   10000], Loss/Closs_src=0.4761, lr=0.000801\n","iter[    1040/   10000], Loss/Closs_src=0.5740, lr=0.000800\n","iter[    1050/   10000], Loss/Closs_src=0.4592, lr=0.000798\n","iter[    1060/   10000], Loss/Closs_src=0.4757, lr=0.000797\n","iter[    1070/   10000], Loss/Closs_src=0.5396, lr=0.000796\n","iter[    1080/   10000], Loss/Closs_src=0.4963, lr=0.000794\n","iter[    1090/   10000], Loss/Closs_src=0.5539, lr=0.000793\n","iter[    1100/   10000], Loss/Closs_src=0.4983, lr=0.000791\n","iter[    1110/   10000], Loss/Closs_src=0.4427, lr=0.000790\n","iter[    1120/   10000], Loss/Closs_src=0.4955, lr=0.000788\n","iter[    1130/   10000], Loss/Closs_src=0.5393, lr=0.000787\n","iter[    1140/   10000], Loss/Closs_src=0.4803, lr=0.000785\n","iter[    1150/   10000], Loss/Closs_src=0.4620, lr=0.000784\n","iter[    1160/   10000], Loss/Closs_src=0.4925, lr=0.000783\n","iter[    1170/   10000], Loss/Closs_src=0.5214, lr=0.000781\n","iter[    1180/   10000], Loss/Closs_src=0.5357, lr=0.000780\n","iter[    1190/   10000], Loss/Closs_src=0.4729, lr=0.000778\n","iter[    1200/   10000], Loss/Closs_src=0.4623, lr=0.000777\n","iter[    1210/   10000], Loss/Closs_src=0.3752, lr=0.000776\n","iter[    1220/   10000], Loss/Closs_src=0.4235, lr=0.000774\n","iter[    1230/   10000], Loss/Closs_src=0.3810, lr=0.000773\n","iter[    1240/   10000], Loss/Closs_src=0.3968, lr=0.000771\n","iter[    1250/   10000], Loss/Closs_src=0.4614, lr=0.000770\n","iter[    1260/   10000], Loss/Closs_src=0.4230, lr=0.000769\n","iter[    1270/   10000], Loss/Closs_src=0.5060, lr=0.000767\n","iter[    1280/   10000], Loss/Closs_src=0.4465, lr=0.000766\n","iter[    1290/   10000], Loss/Closs_src=0.4175, lr=0.000765\n","iter[    1300/   10000], Loss/Closs_src=0.5072, lr=0.000763\n","iter[    1310/   10000], Loss/Closs_src=0.3379, lr=0.000762\n","iter[    1320/   10000], Loss/Closs_src=0.4159, lr=0.000761\n","iter[    1330/   10000], Loss/Closs_src=0.4443, lr=0.000759\n","iter[    1340/   10000], Loss/Closs_src=0.4422, lr=0.000758\n","iter[    1350/   10000], Loss/Closs_src=0.5019, lr=0.000757\n","iter[    1360/   10000], Loss/Closs_src=0.5049, lr=0.000755\n","iter[    1370/   10000], Loss/Closs_src=0.5120, lr=0.000754\n","iter[    1380/   10000], Loss/Closs_src=0.5700, lr=0.000753\n","iter[    1390/   10000], Loss/Closs_src=0.4419, lr=0.000752\n","iter[    1400/   10000], Loss/Closs_src=0.4948, lr=0.000750\n","iter[    1410/   10000], Loss/Closs_src=0.5004, lr=0.000749\n","iter[    1420/   10000], Loss/Closs_src=0.4583, lr=0.000748\n","iter[    1430/   10000], Loss/Closs_src=0.4239, lr=0.000747\n","iter[    1440/   10000], Loss/Closs_src=0.4414, lr=0.000745\n","iter[    1450/   10000], Loss/Closs_src=0.4801, lr=0.000744\n","iter[    1460/   10000], Loss/Closs_src=0.3454, lr=0.000743\n","iter[    1470/   10000], Loss/Closs_src=0.4974, lr=0.000741\n","iter[    1480/   10000], Loss/Closs_src=0.3338, lr=0.000740\n","iter[    1490/   10000], Loss/Closs_src=0.4020, lr=0.000739\n","iter[    1500/   10000], Loss/Closs_src=0.5524, lr=0.000738\n","Elapsed Time: 0:00:39.114064\n","Start Evaluation at 1500\n","\n","MNIST Accuracy of Each class\n","0                        : 24.80%\n","1                        : 52.78%\n","2                        : 76.55%\n","3                        : 75.94%\n","4                        : 55.91%\n","5                        : 92.15%\n","6                        : 29.12%\n","7                        : 82.78%\n","8                        : 76.49%\n","9                        : 36.37%\n","\n","MNIST mean Accuracy: 60.29%\n","MNIST Accuracy: 60.12%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 24.80%\n","1                        : 52.78%\n","2                        : 76.55%\n","3                        : 75.94%\n","4                        : 55.91%\n","5                        : 92.15%\n","6                        : 29.12%\n","7                        : 82.78%\n","8                        : 76.49%\n","9                        : 36.37%\n","\n","Best Accs: 24.80% 52.78% 76.55% 75.94% 55.91% 92.15% 29.12% 82.78% 76.49% 36.37% \n","Best mean Accuracy: 60.29%\n","Best Accuracy: 60.12%\n","iter[    1510/   10000], Loss/Closs_src=0.4519, lr=0.000737\n","iter[    1520/   10000], Loss/Closs_src=0.4215, lr=0.000735\n","iter[    1530/   10000], Loss/Closs_src=0.4956, lr=0.000734\n","iter[    1540/   10000], Loss/Closs_src=0.4324, lr=0.000733\n","iter[    1550/   10000], Loss/Closs_src=0.4311, lr=0.000732\n","iter[    1560/   10000], Loss/Closs_src=0.4465, lr=0.000730\n","iter[    1570/   10000], Loss/Closs_src=0.3728, lr=0.000729\n","iter[    1580/   10000], Loss/Closs_src=0.4891, lr=0.000728\n","iter[    1590/   10000], Loss/Closs_src=0.4354, lr=0.000727\n","iter[    1600/   10000], Loss/Closs_src=0.4284, lr=0.000726\n","iter[    1610/   10000], Loss/Closs_src=0.4356, lr=0.000725\n","iter[    1620/   10000], Loss/Closs_src=0.4150, lr=0.000723\n","iter[    1630/   10000], Loss/Closs_src=0.3992, lr=0.000722\n","iter[    1640/   10000], Loss/Closs_src=0.4790, lr=0.000721\n","iter[    1650/   10000], Loss/Closs_src=0.4164, lr=0.000720\n","iter[    1660/   10000], Loss/Closs_src=0.4783, lr=0.000719\n","iter[    1670/   10000], Loss/Closs_src=0.3917, lr=0.000718\n","iter[    1680/   10000], Loss/Closs_src=0.5069, lr=0.000716\n","iter[    1690/   10000], Loss/Closs_src=0.4444, lr=0.000715\n","iter[    1700/   10000], Loss/Closs_src=0.4419, lr=0.000714\n","iter[    1710/   10000], Loss/Closs_src=0.3622, lr=0.000713\n","iter[    1720/   10000], Loss/Closs_src=0.4343, lr=0.000712\n","iter[    1730/   10000], Loss/Closs_src=0.5467, lr=0.000711\n","iter[    1740/   10000], Loss/Closs_src=0.4899, lr=0.000710\n","iter[    1750/   10000], Loss/Closs_src=0.4714, lr=0.000708\n","iter[    1760/   10000], Loss/Closs_src=0.4545, lr=0.000707\n","iter[    1770/   10000], Loss/Closs_src=0.4207, lr=0.000706\n","iter[    1780/   10000], Loss/Closs_src=0.4208, lr=0.000705\n","iter[    1790/   10000], Loss/Closs_src=0.4216, lr=0.000704\n","iter[    1800/   10000], Loss/Closs_src=0.3577, lr=0.000703\n","iter[    1810/   10000], Loss/Closs_src=0.4431, lr=0.000702\n","iter[    1820/   10000], Loss/Closs_src=0.3597, lr=0.000701\n","iter[    1830/   10000], Loss/Closs_src=0.4440, lr=0.000700\n","iter[    1840/   10000], Loss/Closs_src=0.3684, lr=0.000699\n","iter[    1850/   10000], Loss/Closs_src=0.3544, lr=0.000697\n","iter[    1860/   10000], Loss/Closs_src=0.4256, lr=0.000696\n","iter[    1870/   10000], Loss/Closs_src=0.3381, lr=0.000695\n","iter[    1880/   10000], Loss/Closs_src=0.3794, lr=0.000694\n","iter[    1890/   10000], Loss/Closs_src=0.3680, lr=0.000693\n","iter[    1900/   10000], Loss/Closs_src=0.4780, lr=0.000692\n","iter[    1910/   10000], Loss/Closs_src=0.3551, lr=0.000691\n","iter[    1920/   10000], Loss/Closs_src=0.3245, lr=0.000690\n","iter[    1930/   10000], Loss/Closs_src=0.2896, lr=0.000689\n","iter[    1940/   10000], Loss/Closs_src=0.3468, lr=0.000688\n","iter[    1950/   10000], Loss/Closs_src=0.4231, lr=0.000687\n","iter[    1960/   10000], Loss/Closs_src=0.3068, lr=0.000686\n","iter[    1970/   10000], Loss/Closs_src=0.4690, lr=0.000685\n","iter[    1980/   10000], Loss/Closs_src=0.3931, lr=0.000684\n","iter[    1990/   10000], Loss/Closs_src=0.3276, lr=0.000683\n","iter[    2000/   10000], Loss/Closs_src=0.3005, lr=0.000682\n","Elapsed Time: 0:00:48.989831\n","Start Evaluation at 2000\n","\n","MNIST Accuracy of Each class\n","0                        : 18.37%\n","1                        : 52.60%\n","2                        : 63.08%\n","3                        : 81.29%\n","4                        : 80.14%\n","5                        : 92.26%\n","6                        : 16.70%\n","7                        : 81.81%\n","8                        : 42.09%\n","9                        : 45.69%\n","\n","MNIST mean Accuracy: 57.40%\n","MNIST Accuracy: 57.31%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 24.80%\n","1                        : 52.78%\n","2                        : 76.55%\n","3                        : 75.94%\n","4                        : 55.91%\n","5                        : 92.15%\n","6                        : 29.12%\n","7                        : 82.78%\n","8                        : 76.49%\n","9                        : 36.37%\n","\n","Best Accs: 24.80% 52.78% 76.55% 75.94% 55.91% 92.15% 29.12% 82.78% 76.49% 36.37% \n","Best mean Accuracy: 60.29%\n","Best Accuracy: 60.12%\n","iter[    2010/   10000], Loss/Closs_src=0.3432, lr=0.000681\n","iter[    2020/   10000], Loss/Closs_src=0.3894, lr=0.000680\n","iter[    2030/   10000], Loss/Closs_src=0.3766, lr=0.000679\n","iter[    2040/   10000], Loss/Closs_src=0.3486, lr=0.000678\n","iter[    2050/   10000], Loss/Closs_src=0.3866, lr=0.000677\n","iter[    2060/   10000], Loss/Closs_src=0.3879, lr=0.000676\n","iter[    2070/   10000], Loss/Closs_src=0.4178, lr=0.000675\n","iter[    2080/   10000], Loss/Closs_src=0.4961, lr=0.000674\n","iter[    2090/   10000], Loss/Closs_src=0.4280, lr=0.000673\n","iter[    2100/   10000], Loss/Closs_src=0.3693, lr=0.000672\n","iter[    2110/   10000], Loss/Closs_src=0.4265, lr=0.000671\n","iter[    2120/   10000], Loss/Closs_src=0.3749, lr=0.000670\n","iter[    2130/   10000], Loss/Closs_src=0.3932, lr=0.000669\n","iter[    2140/   10000], Loss/Closs_src=0.3600, lr=0.000668\n","iter[    2150/   10000], Loss/Closs_src=0.3322, lr=0.000667\n","iter[    2160/   10000], Loss/Closs_src=0.3612, lr=0.000666\n","iter[    2170/   10000], Loss/Closs_src=0.4728, lr=0.000665\n","iter[    2180/   10000], Loss/Closs_src=0.4048, lr=0.000664\n","iter[    2190/   10000], Loss/Closs_src=0.4804, lr=0.000663\n","iter[    2200/   10000], Loss/Closs_src=0.4348, lr=0.000662\n","iter[    2210/   10000], Loss/Closs_src=0.2942, lr=0.000661\n","iter[    2220/   10000], Loss/Closs_src=0.3503, lr=0.000660\n","iter[    2230/   10000], Loss/Closs_src=0.3525, lr=0.000659\n","iter[    2240/   10000], Loss/Closs_src=0.4308, lr=0.000658\n","iter[    2250/   10000], Loss/Closs_src=0.3157, lr=0.000657\n","iter[    2260/   10000], Loss/Closs_src=0.4404, lr=0.000656\n","iter[    2270/   10000], Loss/Closs_src=0.3649, lr=0.000655\n","iter[    2280/   10000], Loss/Closs_src=0.3583, lr=0.000654\n","iter[    2290/   10000], Loss/Closs_src=0.3785, lr=0.000654\n","iter[    2300/   10000], Loss/Closs_src=0.4073, lr=0.000653\n","iter[    2310/   10000], Loss/Closs_src=0.4024, lr=0.000652\n","iter[    2320/   10000], Loss/Closs_src=0.3854, lr=0.000651\n","iter[    2330/   10000], Loss/Closs_src=0.2999, lr=0.000650\n","iter[    2340/   10000], Loss/Closs_src=0.4335, lr=0.000649\n","iter[    2350/   10000], Loss/Closs_src=0.2894, lr=0.000648\n","iter[    2360/   10000], Loss/Closs_src=0.2926, lr=0.000647\n","iter[    2370/   10000], Loss/Closs_src=0.4147, lr=0.000646\n","iter[    2380/   10000], Loss/Closs_src=0.3985, lr=0.000645\n","iter[    2390/   10000], Loss/Closs_src=0.3269, lr=0.000644\n","iter[    2400/   10000], Loss/Closs_src=0.4230, lr=0.000643\n","iter[    2410/   10000], Loss/Closs_src=0.3482, lr=0.000643\n","iter[    2420/   10000], Loss/Closs_src=0.3663, lr=0.000642\n","iter[    2430/   10000], Loss/Closs_src=0.3231, lr=0.000641\n","iter[    2440/   10000], Loss/Closs_src=0.3859, lr=0.000640\n","iter[    2450/   10000], Loss/Closs_src=0.3806, lr=0.000639\n","iter[    2460/   10000], Loss/Closs_src=0.4366, lr=0.000638\n","iter[    2470/   10000], Loss/Closs_src=0.3387, lr=0.000637\n","iter[    2480/   10000], Loss/Closs_src=0.4533, lr=0.000636\n","iter[    2490/   10000], Loss/Closs_src=0.4445, lr=0.000636\n","iter[    2500/   10000], Loss/Closs_src=0.3015, lr=0.000635\n","Elapsed Time: 0:01:00.104327\n","Start Evaluation at 2500\n","\n","MNIST Accuracy of Each class\n","0                        : 39.18%\n","1                        : 57.27%\n","2                        : 56.88%\n","3                        : 93.96%\n","4                        : 64.66%\n","5                        : 59.75%\n","6                        : 48.85%\n","7                        : 80.93%\n","8                        : 53.90%\n","9                        : 32.61%\n","\n","MNIST mean Accuracy: 58.80%\n","MNIST Accuracy: 58.92%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 24.80%\n","1                        : 52.78%\n","2                        : 76.55%\n","3                        : 75.94%\n","4                        : 55.91%\n","5                        : 92.15%\n","6                        : 29.12%\n","7                        : 82.78%\n","8                        : 76.49%\n","9                        : 36.37%\n","\n","Best Accs: 24.80% 52.78% 76.55% 75.94% 55.91% 92.15% 29.12% 82.78% 76.49% 36.37% \n","Best mean Accuracy: 60.29%\n","Best Accuracy: 60.12%\n","iter[    2510/   10000], Loss/Closs_src=0.3249, lr=0.000634\n","iter[    2520/   10000], Loss/Closs_src=0.4227, lr=0.000633\n","iter[    2530/   10000], Loss/Closs_src=0.3892, lr=0.000632\n","iter[    2540/   10000], Loss/Closs_src=0.4171, lr=0.000631\n","iter[    2550/   10000], Loss/Closs_src=0.4689, lr=0.000630\n","iter[    2560/   10000], Loss/Closs_src=0.3528, lr=0.000630\n","iter[    2570/   10000], Loss/Closs_src=0.3024, lr=0.000629\n","iter[    2580/   10000], Loss/Closs_src=0.5077, lr=0.000628\n","iter[    2590/   10000], Loss/Closs_src=0.4032, lr=0.000627\n","iter[    2600/   10000], Loss/Closs_src=0.3396, lr=0.000626\n","iter[    2610/   10000], Loss/Closs_src=0.4224, lr=0.000625\n","iter[    2620/   10000], Loss/Closs_src=0.3370, lr=0.000625\n","iter[    2630/   10000], Loss/Closs_src=0.3730, lr=0.000624\n","iter[    2640/   10000], Loss/Closs_src=0.4139, lr=0.000623\n","iter[    2650/   10000], Loss/Closs_src=0.3776, lr=0.000622\n","iter[    2660/   10000], Loss/Closs_src=0.3672, lr=0.000621\n","iter[    2670/   10000], Loss/Closs_src=0.2147, lr=0.000620\n","iter[    2680/   10000], Loss/Closs_src=0.4082, lr=0.000620\n","iter[    2690/   10000], Loss/Closs_src=0.3727, lr=0.000619\n","iter[    2700/   10000], Loss/Closs_src=0.3617, lr=0.000618\n","iter[    2710/   10000], Loss/Closs_src=0.4692, lr=0.000617\n","iter[    2720/   10000], Loss/Closs_src=0.3934, lr=0.000616\n","iter[    2730/   10000], Loss/Closs_src=0.3264, lr=0.000615\n","iter[    2740/   10000], Loss/Closs_src=0.3257, lr=0.000615\n","iter[    2750/   10000], Loss/Closs_src=0.4341, lr=0.000614\n","iter[    2760/   10000], Loss/Closs_src=0.4308, lr=0.000613\n","iter[    2770/   10000], Loss/Closs_src=0.3573, lr=0.000612\n","iter[    2780/   10000], Loss/Closs_src=0.2673, lr=0.000611\n","iter[    2790/   10000], Loss/Closs_src=0.3018, lr=0.000611\n","iter[    2800/   10000], Loss/Closs_src=0.3663, lr=0.000610\n","iter[    2810/   10000], Loss/Closs_src=0.3750, lr=0.000609\n","iter[    2820/   10000], Loss/Closs_src=0.3519, lr=0.000608\n","iter[    2830/   10000], Loss/Closs_src=0.4776, lr=0.000608\n","iter[    2840/   10000], Loss/Closs_src=0.3295, lr=0.000607\n","iter[    2850/   10000], Loss/Closs_src=0.3790, lr=0.000606\n","iter[    2860/   10000], Loss/Closs_src=0.3864, lr=0.000605\n","iter[    2870/   10000], Loss/Closs_src=0.3559, lr=0.000604\n","iter[    2880/   10000], Loss/Closs_src=0.3555, lr=0.000604\n","iter[    2890/   10000], Loss/Closs_src=0.4550, lr=0.000603\n","iter[    2900/   10000], Loss/Closs_src=0.3372, lr=0.000602\n","iter[    2910/   10000], Loss/Closs_src=0.3431, lr=0.000601\n","iter[    2920/   10000], Loss/Closs_src=0.3252, lr=0.000601\n","iter[    2930/   10000], Loss/Closs_src=0.3910, lr=0.000600\n","iter[    2940/   10000], Loss/Closs_src=0.3075, lr=0.000599\n","iter[    2950/   10000], Loss/Closs_src=0.3725, lr=0.000598\n","iter[    2960/   10000], Loss/Closs_src=0.3609, lr=0.000598\n","iter[    2970/   10000], Loss/Closs_src=0.3333, lr=0.000597\n","iter[    2980/   10000], Loss/Closs_src=0.3209, lr=0.000596\n","iter[    2990/   10000], Loss/Closs_src=0.3734, lr=0.000595\n","iter[    3000/   10000], Loss/Closs_src=0.3647, lr=0.000595\n","Elapsed Time: 0:01:11.237321\n","Start Evaluation at 3000\n","\n","MNIST Accuracy of Each class\n","0                        : 38.06%\n","1                        : 50.93%\n","2                        : 69.28%\n","3                        : 72.18%\n","4                        : 90.73%\n","5                        : 81.28%\n","6                        : 21.40%\n","7                        : 72.08%\n","8                        : 59.75%\n","9                        : 45.59%\n","\n","MNIST mean Accuracy: 60.13%\n","MNIST Accuracy: 59.99%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 24.80%\n","1                        : 52.78%\n","2                        : 76.55%\n","3                        : 75.94%\n","4                        : 55.91%\n","5                        : 92.15%\n","6                        : 29.12%\n","7                        : 82.78%\n","8                        : 76.49%\n","9                        : 36.37%\n","\n","Best Accs: 24.80% 52.78% 76.55% 75.94% 55.91% 92.15% 29.12% 82.78% 76.49% 36.37% \n","Best mean Accuracy: 60.29%\n","Best Accuracy: 60.12%\n","iter[    3010/   10000], Loss/Closs_src=0.3126, lr=0.000594\n","iter[    3020/   10000], Loss/Closs_src=0.4061, lr=0.000593\n","iter[    3030/   10000], Loss/Closs_src=0.3704, lr=0.000592\n","iter[    3040/   10000], Loss/Closs_src=0.3925, lr=0.000592\n","iter[    3050/   10000], Loss/Closs_src=0.3596, lr=0.000591\n","iter[    3060/   10000], Loss/Closs_src=0.3676, lr=0.000590\n","iter[    3070/   10000], Loss/Closs_src=0.3834, lr=0.000589\n","iter[    3080/   10000], Loss/Closs_src=0.3420, lr=0.000589\n","iter[    3090/   10000], Loss/Closs_src=0.4041, lr=0.000588\n","iter[    3100/   10000], Loss/Closs_src=0.3809, lr=0.000587\n","iter[    3110/   10000], Loss/Closs_src=0.3860, lr=0.000587\n","iter[    3120/   10000], Loss/Closs_src=0.4465, lr=0.000586\n","iter[    3130/   10000], Loss/Closs_src=0.3241, lr=0.000585\n","iter[    3140/   10000], Loss/Closs_src=0.2971, lr=0.000584\n","iter[    3150/   10000], Loss/Closs_src=0.3906, lr=0.000584\n","iter[    3160/   10000], Loss/Closs_src=0.4255, lr=0.000583\n","iter[    3170/   10000], Loss/Closs_src=0.4427, lr=0.000582\n","iter[    3180/   10000], Loss/Closs_src=0.3056, lr=0.000582\n","iter[    3190/   10000], Loss/Closs_src=0.3446, lr=0.000581\n","iter[    3200/   10000], Loss/Closs_src=0.4323, lr=0.000580\n","iter[    3210/   10000], Loss/Closs_src=0.3046, lr=0.000579\n","iter[    3220/   10000], Loss/Closs_src=0.4893, lr=0.000579\n","iter[    3230/   10000], Loss/Closs_src=0.3608, lr=0.000578\n","iter[    3240/   10000], Loss/Closs_src=0.3315, lr=0.000577\n","iter[    3250/   10000], Loss/Closs_src=0.3027, lr=0.000577\n","iter[    3260/   10000], Loss/Closs_src=0.3519, lr=0.000576\n","iter[    3270/   10000], Loss/Closs_src=0.3037, lr=0.000575\n","iter[    3280/   10000], Loss/Closs_src=0.3408, lr=0.000575\n","iter[    3290/   10000], Loss/Closs_src=0.3904, lr=0.000574\n","iter[    3300/   10000], Loss/Closs_src=0.3414, lr=0.000573\n","iter[    3310/   10000], Loss/Closs_src=0.4176, lr=0.000573\n","iter[    3320/   10000], Loss/Closs_src=0.4042, lr=0.000572\n","iter[    3330/   10000], Loss/Closs_src=0.3915, lr=0.000571\n","iter[    3340/   10000], Loss/Closs_src=0.4007, lr=0.000571\n","iter[    3350/   10000], Loss/Closs_src=0.3204, lr=0.000570\n","iter[    3360/   10000], Loss/Closs_src=0.3577, lr=0.000569\n","iter[    3370/   10000], Loss/Closs_src=0.3345, lr=0.000569\n","iter[    3380/   10000], Loss/Closs_src=0.3407, lr=0.000568\n","iter[    3390/   10000], Loss/Closs_src=0.4053, lr=0.000567\n","iter[    3400/   10000], Loss/Closs_src=0.4307, lr=0.000567\n","iter[    3410/   10000], Loss/Closs_src=0.3476, lr=0.000566\n","iter[    3420/   10000], Loss/Closs_src=0.3683, lr=0.000565\n","iter[    3430/   10000], Loss/Closs_src=0.3352, lr=0.000565\n","iter[    3440/   10000], Loss/Closs_src=0.4504, lr=0.000564\n","iter[    3450/   10000], Loss/Closs_src=0.3496, lr=0.000563\n","iter[    3460/   10000], Loss/Closs_src=0.3050, lr=0.000563\n","iter[    3470/   10000], Loss/Closs_src=0.3918, lr=0.000562\n","iter[    3480/   10000], Loss/Closs_src=0.3126, lr=0.000561\n","iter[    3490/   10000], Loss/Closs_src=0.5001, lr=0.000561\n","iter[    3500/   10000], Loss/Closs_src=0.3990, lr=0.000560\n","Elapsed Time: 0:01:22.414455\n","Start Evaluation at 3500\n","\n","MNIST Accuracy of Each class\n","0                        : 44.39%\n","1                        : 49.25%\n","2                        : 55.81%\n","3                        : 85.54%\n","4                        : 69.45%\n","5                        : 85.54%\n","6                        : 30.17%\n","7                        : 82.39%\n","8                        : 22.69%\n","9                        : 23.89%\n","\n","MNIST mean Accuracy: 54.91%\n","MNIST Accuracy: 54.77%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 24.80%\n","1                        : 52.78%\n","2                        : 76.55%\n","3                        : 75.94%\n","4                        : 55.91%\n","5                        : 92.15%\n","6                        : 29.12%\n","7                        : 82.78%\n","8                        : 76.49%\n","9                        : 36.37%\n","\n","Best Accs: 24.80% 52.78% 76.55% 75.94% 55.91% 92.15% 29.12% 82.78% 76.49% 36.37% \n","Best mean Accuracy: 60.29%\n","Best Accuracy: 60.12%\n","iter[    3510/   10000], Loss/Closs_src=0.3319, lr=0.000559\n","iter[    3520/   10000], Loss/Closs_src=0.3452, lr=0.000559\n","iter[    3530/   10000], Loss/Closs_src=0.3124, lr=0.000558\n","iter[    3540/   10000], Loss/Closs_src=0.3671, lr=0.000557\n","iter[    3550/   10000], Loss/Closs_src=0.3448, lr=0.000557\n","iter[    3560/   10000], Loss/Closs_src=0.3317, lr=0.000556\n","iter[    3570/   10000], Loss/Closs_src=0.3426, lr=0.000555\n","iter[    3580/   10000], Loss/Closs_src=0.3698, lr=0.000555\n","iter[    3590/   10000], Loss/Closs_src=0.3733, lr=0.000554\n","iter[    3600/   10000], Loss/Closs_src=0.3356, lr=0.000554\n","iter[    3610/   10000], Loss/Closs_src=0.3170, lr=0.000553\n","iter[    3620/   10000], Loss/Closs_src=0.2662, lr=0.000552\n","iter[    3630/   10000], Loss/Closs_src=0.3707, lr=0.000552\n","iter[    3640/   10000], Loss/Closs_src=0.3313, lr=0.000551\n","iter[    3650/   10000], Loss/Closs_src=0.3022, lr=0.000550\n","iter[    3660/   10000], Loss/Closs_src=0.3182, lr=0.000550\n","iter[    3670/   10000], Loss/Closs_src=0.2967, lr=0.000549\n","iter[    3680/   10000], Loss/Closs_src=0.3711, lr=0.000549\n","iter[    3690/   10000], Loss/Closs_src=0.2748, lr=0.000548\n","iter[    3700/   10000], Loss/Closs_src=0.3624, lr=0.000547\n","iter[    3710/   10000], Loss/Closs_src=0.2784, lr=0.000547\n","iter[    3720/   10000], Loss/Closs_src=0.2778, lr=0.000546\n","iter[    3730/   10000], Loss/Closs_src=0.3834, lr=0.000546\n","iter[    3740/   10000], Loss/Closs_src=0.3012, lr=0.000545\n","iter[    3750/   10000], Loss/Closs_src=0.3532, lr=0.000544\n","iter[    3760/   10000], Loss/Closs_src=0.3130, lr=0.000544\n","iter[    3770/   10000], Loss/Closs_src=0.2791, lr=0.000543\n","iter[    3780/   10000], Loss/Closs_src=0.2846, lr=0.000543\n","iter[    3790/   10000], Loss/Closs_src=0.3093, lr=0.000542\n","iter[    3800/   10000], Loss/Closs_src=0.3057, lr=0.000541\n","iter[    3810/   10000], Loss/Closs_src=0.3406, lr=0.000541\n","iter[    3820/   10000], Loss/Closs_src=0.2828, lr=0.000540\n","iter[    3830/   10000], Loss/Closs_src=0.3850, lr=0.000540\n","iter[    3840/   10000], Loss/Closs_src=0.3007, lr=0.000539\n","iter[    3850/   10000], Loss/Closs_src=0.2702, lr=0.000538\n","iter[    3860/   10000], Loss/Closs_src=0.3147, lr=0.000538\n","iter[    3870/   10000], Loss/Closs_src=0.2331, lr=0.000537\n","iter[    3880/   10000], Loss/Closs_src=0.3043, lr=0.000537\n","iter[    3890/   10000], Loss/Closs_src=0.3063, lr=0.000536\n","iter[    3900/   10000], Loss/Closs_src=0.2446, lr=0.000535\n","iter[    3910/   10000], Loss/Closs_src=0.3126, lr=0.000535\n","iter[    3920/   10000], Loss/Closs_src=0.3177, lr=0.000534\n","iter[    3930/   10000], Loss/Closs_src=0.2819, lr=0.000534\n","iter[    3940/   10000], Loss/Closs_src=0.2864, lr=0.000533\n","iter[    3950/   10000], Loss/Closs_src=0.3145, lr=0.000533\n","iter[    3960/   10000], Loss/Closs_src=0.2968, lr=0.000532\n","iter[    3970/   10000], Loss/Closs_src=0.4036, lr=0.000531\n","iter[    3980/   10000], Loss/Closs_src=0.3204, lr=0.000531\n","iter[    3990/   10000], Loss/Closs_src=0.3400, lr=0.000530\n","iter[    4000/   10000], Loss/Closs_src=0.3309, lr=0.000530\n","Elapsed Time: 0:01:32.865387\n","Start Evaluation at 4000\n","\n","MNIST Accuracy of Each class\n","0                        : 44.49%\n","1                        : 69.34%\n","2                        : 48.55%\n","3                        : 89.80%\n","4                        : 70.16%\n","5                        : 71.64%\n","6                        : 40.50%\n","7                        : 82.78%\n","8                        : 61.91%\n","9                        : 42.81%\n","\n","MNIST mean Accuracy: 62.20%\n","MNIST Accuracy: 62.33%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 44.49%\n","1                        : 69.34%\n","2                        : 48.55%\n","3                        : 89.80%\n","4                        : 70.16%\n","5                        : 71.64%\n","6                        : 40.50%\n","7                        : 82.78%\n","8                        : 61.91%\n","9                        : 42.81%\n","\n","Best Accs: 44.49% 69.34% 48.55% 89.80% 70.16% 71.64% 40.50% 82.78% 61.91% 42.81% \n","Best mean Accuracy: 62.20%\n","Best Accuracy: 62.33%\n","iter[    4010/   10000], Loss/Closs_src=0.2901, lr=0.000529\n","iter[    4020/   10000], Loss/Closs_src=0.3466, lr=0.000529\n","iter[    4030/   10000], Loss/Closs_src=0.3014, lr=0.000528\n","iter[    4040/   10000], Loss/Closs_src=0.2897, lr=0.000527\n","iter[    4050/   10000], Loss/Closs_src=0.2511, lr=0.000527\n","iter[    4060/   10000], Loss/Closs_src=0.3735, lr=0.000526\n","iter[    4070/   10000], Loss/Closs_src=0.3252, lr=0.000526\n","iter[    4080/   10000], Loss/Closs_src=0.3162, lr=0.000525\n","iter[    4090/   10000], Loss/Closs_src=0.3393, lr=0.000525\n","iter[    4100/   10000], Loss/Closs_src=0.3019, lr=0.000524\n","iter[    4110/   10000], Loss/Closs_src=0.3724, lr=0.000524\n","iter[    4120/   10000], Loss/Closs_src=0.3657, lr=0.000523\n","iter[    4130/   10000], Loss/Closs_src=0.3484, lr=0.000522\n","iter[    4140/   10000], Loss/Closs_src=0.2603, lr=0.000522\n","iter[    4150/   10000], Loss/Closs_src=0.3209, lr=0.000521\n","iter[    4160/   10000], Loss/Closs_src=0.3464, lr=0.000521\n","iter[    4170/   10000], Loss/Closs_src=0.2873, lr=0.000520\n","iter[    4180/   10000], Loss/Closs_src=0.3352, lr=0.000520\n","iter[    4190/   10000], Loss/Closs_src=0.3163, lr=0.000519\n","iter[    4200/   10000], Loss/Closs_src=0.3247, lr=0.000519\n","iter[    4210/   10000], Loss/Closs_src=0.3254, lr=0.000518\n","iter[    4220/   10000], Loss/Closs_src=0.3339, lr=0.000518\n","iter[    4230/   10000], Loss/Closs_src=0.3006, lr=0.000517\n","iter[    4240/   10000], Loss/Closs_src=0.3526, lr=0.000516\n","iter[    4250/   10000], Loss/Closs_src=0.2040, lr=0.000516\n","iter[    4260/   10000], Loss/Closs_src=0.2839, lr=0.000515\n","iter[    4270/   10000], Loss/Closs_src=0.3514, lr=0.000515\n","iter[    4280/   10000], Loss/Closs_src=0.2973, lr=0.000514\n","iter[    4290/   10000], Loss/Closs_src=0.2498, lr=0.000514\n","iter[    4300/   10000], Loss/Closs_src=0.2873, lr=0.000513\n","iter[    4310/   10000], Loss/Closs_src=0.3708, lr=0.000513\n","iter[    4320/   10000], Loss/Closs_src=0.3946, lr=0.000512\n","iter[    4330/   10000], Loss/Closs_src=0.3311, lr=0.000512\n","iter[    4340/   10000], Loss/Closs_src=0.3315, lr=0.000511\n","iter[    4350/   10000], Loss/Closs_src=0.2989, lr=0.000511\n","iter[    4360/   10000], Loss/Closs_src=0.2635, lr=0.000510\n","iter[    4370/   10000], Loss/Closs_src=0.2758, lr=0.000510\n","iter[    4380/   10000], Loss/Closs_src=0.2528, lr=0.000509\n","iter[    4390/   10000], Loss/Closs_src=0.3340, lr=0.000509\n","iter[    4400/   10000], Loss/Closs_src=0.3438, lr=0.000508\n","iter[    4410/   10000], Loss/Closs_src=0.3659, lr=0.000508\n","iter[    4420/   10000], Loss/Closs_src=0.3848, lr=0.000507\n","iter[    4430/   10000], Loss/Closs_src=0.2940, lr=0.000507\n","iter[    4440/   10000], Loss/Closs_src=0.3477, lr=0.000506\n","iter[    4450/   10000], Loss/Closs_src=0.3507, lr=0.000506\n","iter[    4460/   10000], Loss/Closs_src=0.2756, lr=0.000505\n","iter[    4470/   10000], Loss/Closs_src=0.3236, lr=0.000504\n","iter[    4480/   10000], Loss/Closs_src=0.3178, lr=0.000504\n","iter[    4490/   10000], Loss/Closs_src=0.3745, lr=0.000503\n","iter[    4500/   10000], Loss/Closs_src=0.2822, lr=0.000503\n","Elapsed Time: 0:01:43.809813\n","Start Evaluation at 4500\n","\n","MNIST Accuracy of Each class\n","0                        : 24.29%\n","1                        : 48.28%\n","2                        : 69.19%\n","3                        : 88.81%\n","4                        : 89.21%\n","5                        : 67.71%\n","6                        : 17.64%\n","7                        : 77.24%\n","8                        : 49.69%\n","9                        : 47.97%\n","\n","MNIST mean Accuracy: 58.00%\n","MNIST Accuracy: 58.08%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 44.49%\n","1                        : 69.34%\n","2                        : 48.55%\n","3                        : 89.80%\n","4                        : 70.16%\n","5                        : 71.64%\n","6                        : 40.50%\n","7                        : 82.78%\n","8                        : 61.91%\n","9                        : 42.81%\n","\n","Best Accs: 44.49% 69.34% 48.55% 89.80% 70.16% 71.64% 40.50% 82.78% 61.91% 42.81% \n","Best mean Accuracy: 62.20%\n","Best Accuracy: 62.33%\n","iter[    4510/   10000], Loss/Closs_src=0.2891, lr=0.000502\n","iter[    4520/   10000], Loss/Closs_src=0.4032, lr=0.000502\n","iter[    4530/   10000], Loss/Closs_src=0.2781, lr=0.000501\n","iter[    4540/   10000], Loss/Closs_src=0.3148, lr=0.000501\n","iter[    4550/   10000], Loss/Closs_src=0.2575, lr=0.000500\n","iter[    4560/   10000], Loss/Closs_src=0.2441, lr=0.000500\n","iter[    4570/   10000], Loss/Closs_src=0.3479, lr=0.000499\n","iter[    4580/   10000], Loss/Closs_src=0.2909, lr=0.000499\n","iter[    4590/   10000], Loss/Closs_src=0.2556, lr=0.000498\n","iter[    4600/   10000], Loss/Closs_src=0.3617, lr=0.000498\n","iter[    4610/   10000], Loss/Closs_src=0.2743, lr=0.000498\n","iter[    4620/   10000], Loss/Closs_src=0.3062, lr=0.000497\n","iter[    4630/   10000], Loss/Closs_src=0.2676, lr=0.000497\n","iter[    4640/   10000], Loss/Closs_src=0.3446, lr=0.000496\n","iter[    4650/   10000], Loss/Closs_src=0.2158, lr=0.000496\n","iter[    4660/   10000], Loss/Closs_src=0.3636, lr=0.000495\n","iter[    4670/   10000], Loss/Closs_src=0.3367, lr=0.000495\n","iter[    4680/   10000], Loss/Closs_src=0.2599, lr=0.000494\n","iter[    4690/   10000], Loss/Closs_src=0.2894, lr=0.000494\n","iter[    4700/   10000], Loss/Closs_src=0.2795, lr=0.000493\n","iter[    4710/   10000], Loss/Closs_src=0.2858, lr=0.000493\n","iter[    4720/   10000], Loss/Closs_src=0.2618, lr=0.000492\n","iter[    4730/   10000], Loss/Closs_src=0.2527, lr=0.000492\n","iter[    4740/   10000], Loss/Closs_src=0.2547, lr=0.000491\n","iter[    4750/   10000], Loss/Closs_src=0.3066, lr=0.000491\n","iter[    4760/   10000], Loss/Closs_src=0.3395, lr=0.000490\n","iter[    4770/   10000], Loss/Closs_src=0.2620, lr=0.000490\n","iter[    4780/   10000], Loss/Closs_src=0.2753, lr=0.000489\n","iter[    4790/   10000], Loss/Closs_src=0.3940, lr=0.000489\n","iter[    4800/   10000], Loss/Closs_src=0.3535, lr=0.000488\n","iter[    4810/   10000], Loss/Closs_src=0.2886, lr=0.000488\n","iter[    4820/   10000], Loss/Closs_src=0.3121, lr=0.000487\n","iter[    4830/   10000], Loss/Closs_src=0.2791, lr=0.000487\n","iter[    4840/   10000], Loss/Closs_src=0.2830, lr=0.000487\n","iter[    4850/   10000], Loss/Closs_src=0.2415, lr=0.000486\n","iter[    4860/   10000], Loss/Closs_src=0.3413, lr=0.000486\n","iter[    4870/   10000], Loss/Closs_src=0.3513, lr=0.000485\n","iter[    4880/   10000], Loss/Closs_src=0.3646, lr=0.000485\n","iter[    4890/   10000], Loss/Closs_src=0.3539, lr=0.000484\n","iter[    4900/   10000], Loss/Closs_src=0.3540, lr=0.000484\n","iter[    4910/   10000], Loss/Closs_src=0.3997, lr=0.000483\n","iter[    4920/   10000], Loss/Closs_src=0.2704, lr=0.000483\n","iter[    4930/   10000], Loss/Closs_src=0.3199, lr=0.000482\n","iter[    4940/   10000], Loss/Closs_src=0.2917, lr=0.000482\n","iter[    4950/   10000], Loss/Closs_src=0.3731, lr=0.000481\n","iter[    4960/   10000], Loss/Closs_src=0.3436, lr=0.000481\n","iter[    4970/   10000], Loss/Closs_src=0.3589, lr=0.000481\n","iter[    4980/   10000], Loss/Closs_src=0.3506, lr=0.000480\n","iter[    4990/   10000], Loss/Closs_src=0.2727, lr=0.000480\n","iter[    5000/   10000], Loss/Closs_src=0.3080, lr=0.000479\n","Elapsed Time: 0:01:53.165992\n","Start Evaluation at 5000\n","\n","MNIST Accuracy of Each class\n","0                        : 43.27%\n","1                        : 75.15%\n","2                        : 63.37%\n","3                        : 94.55%\n","4                        : 65.78%\n","5                        : 81.05%\n","6                        : 38.94%\n","7                        : 73.44%\n","8                        : 22.18%\n","9                        : 30.53%\n","\n","MNIST mean Accuracy: 58.83%\n","MNIST Accuracy: 59.07%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 44.49%\n","1                        : 69.34%\n","2                        : 48.55%\n","3                        : 89.80%\n","4                        : 70.16%\n","5                        : 71.64%\n","6                        : 40.50%\n","7                        : 82.78%\n","8                        : 61.91%\n","9                        : 42.81%\n","\n","Best Accs: 44.49% 69.34% 48.55% 89.80% 70.16% 71.64% 40.50% 82.78% 61.91% 42.81% \n","Best mean Accuracy: 62.20%\n","Best Accuracy: 62.33%\n","iter[    5010/   10000], Loss/Closs_src=0.2043, lr=0.000479\n","iter[    5020/   10000], Loss/Closs_src=0.2685, lr=0.000478\n","iter[    5030/   10000], Loss/Closs_src=0.3612, lr=0.000478\n","iter[    5040/   10000], Loss/Closs_src=0.3086, lr=0.000477\n","iter[    5050/   10000], Loss/Closs_src=0.2193, lr=0.000477\n","iter[    5060/   10000], Loss/Closs_src=0.3954, lr=0.000477\n","iter[    5070/   10000], Loss/Closs_src=0.2208, lr=0.000476\n","iter[    5080/   10000], Loss/Closs_src=0.2732, lr=0.000476\n","iter[    5090/   10000], Loss/Closs_src=0.3150, lr=0.000475\n","iter[    5100/   10000], Loss/Closs_src=0.2407, lr=0.000475\n","iter[    5110/   10000], Loss/Closs_src=0.3530, lr=0.000474\n","iter[    5120/   10000], Loss/Closs_src=0.3000, lr=0.000474\n","iter[    5130/   10000], Loss/Closs_src=0.2501, lr=0.000473\n","iter[    5140/   10000], Loss/Closs_src=0.2927, lr=0.000473\n","iter[    5150/   10000], Loss/Closs_src=0.2653, lr=0.000473\n","iter[    5160/   10000], Loss/Closs_src=0.3675, lr=0.000472\n","iter[    5170/   10000], Loss/Closs_src=0.2772, lr=0.000472\n","iter[    5180/   10000], Loss/Closs_src=0.3237, lr=0.000471\n","iter[    5190/   10000], Loss/Closs_src=0.2794, lr=0.000471\n","iter[    5200/   10000], Loss/Closs_src=0.3053, lr=0.000470\n","iter[    5210/   10000], Loss/Closs_src=0.4077, lr=0.000470\n","iter[    5220/   10000], Loss/Closs_src=0.2792, lr=0.000470\n","iter[    5230/   10000], Loss/Closs_src=0.3702, lr=0.000469\n","iter[    5240/   10000], Loss/Closs_src=0.3268, lr=0.000469\n","iter[    5250/   10000], Loss/Closs_src=0.3533, lr=0.000468\n","iter[    5260/   10000], Loss/Closs_src=0.3415, lr=0.000468\n","iter[    5270/   10000], Loss/Closs_src=0.2792, lr=0.000467\n","iter[    5280/   10000], Loss/Closs_src=0.3453, lr=0.000467\n","iter[    5290/   10000], Loss/Closs_src=0.3249, lr=0.000467\n","iter[    5300/   10000], Loss/Closs_src=0.3663, lr=0.000466\n","iter[    5310/   10000], Loss/Closs_src=0.2975, lr=0.000466\n","iter[    5320/   10000], Loss/Closs_src=0.2863, lr=0.000465\n","iter[    5330/   10000], Loss/Closs_src=0.2641, lr=0.000465\n","iter[    5340/   10000], Loss/Closs_src=0.3001, lr=0.000464\n","iter[    5350/   10000], Loss/Closs_src=0.2979, lr=0.000464\n","iter[    5360/   10000], Loss/Closs_src=0.2858, lr=0.000464\n","iter[    5370/   10000], Loss/Closs_src=0.3134, lr=0.000463\n","iter[    5380/   10000], Loss/Closs_src=0.2843, lr=0.000463\n","iter[    5390/   10000], Loss/Closs_src=0.3077, lr=0.000462\n","iter[    5400/   10000], Loss/Closs_src=0.2711, lr=0.000462\n","iter[    5410/   10000], Loss/Closs_src=0.2338, lr=0.000462\n","iter[    5420/   10000], Loss/Closs_src=0.2652, lr=0.000461\n","iter[    5430/   10000], Loss/Closs_src=0.3016, lr=0.000461\n","iter[    5440/   10000], Loss/Closs_src=0.3073, lr=0.000460\n","iter[    5450/   10000], Loss/Closs_src=0.3084, lr=0.000460\n","iter[    5460/   10000], Loss/Closs_src=0.2466, lr=0.000460\n","iter[    5470/   10000], Loss/Closs_src=0.3870, lr=0.000459\n","iter[    5480/   10000], Loss/Closs_src=0.2964, lr=0.000459\n","iter[    5490/   10000], Loss/Closs_src=0.2708, lr=0.000458\n","iter[    5500/   10000], Loss/Closs_src=0.2727, lr=0.000458\n","Elapsed Time: 0:02:04.194607\n","Start Evaluation at 5500\n","\n","MNIST Accuracy of Each class\n","0                        : 32.65%\n","1                        : 74.71%\n","2                        : 69.67%\n","3                        : 89.70%\n","4                        : 77.90%\n","5                        : 73.88%\n","6                        : 33.72%\n","7                        : 84.34%\n","8                        : 43.74%\n","9                        : 41.82%\n","\n","MNIST mean Accuracy: 62.21%\n","MNIST Accuracy: 62.55%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 32.65%\n","1                        : 74.71%\n","2                        : 69.67%\n","3                        : 89.70%\n","4                        : 77.90%\n","5                        : 73.88%\n","6                        : 33.72%\n","7                        : 84.34%\n","8                        : 43.74%\n","9                        : 41.82%\n","\n","Best Accs: 32.65% 74.71% 69.67% 89.70% 77.90% 73.88% 33.72% 84.34% 43.74% 41.82% \n","Best mean Accuracy: 62.21%\n","Best Accuracy: 62.55%\n","iter[    5510/   10000], Loss/Closs_src=0.2002, lr=0.000458\n","iter[    5520/   10000], Loss/Closs_src=0.2469, lr=0.000457\n","iter[    5530/   10000], Loss/Closs_src=0.2690, lr=0.000457\n","iter[    5540/   10000], Loss/Closs_src=0.3138, lr=0.000456\n","iter[    5550/   10000], Loss/Closs_src=0.2831, lr=0.000456\n","iter[    5560/   10000], Loss/Closs_src=0.3591, lr=0.000455\n","iter[    5570/   10000], Loss/Closs_src=0.3181, lr=0.000455\n","iter[    5580/   10000], Loss/Closs_src=0.3107, lr=0.000455\n","iter[    5590/   10000], Loss/Closs_src=0.2714, lr=0.000454\n","iter[    5600/   10000], Loss/Closs_src=0.2920, lr=0.000454\n","iter[    5610/   10000], Loss/Closs_src=0.2401, lr=0.000454\n","iter[    5620/   10000], Loss/Closs_src=0.2633, lr=0.000453\n","iter[    5630/   10000], Loss/Closs_src=0.2572, lr=0.000453\n","iter[    5640/   10000], Loss/Closs_src=0.1920, lr=0.000452\n","iter[    5650/   10000], Loss/Closs_src=0.3242, lr=0.000452\n","iter[    5660/   10000], Loss/Closs_src=0.2559, lr=0.000452\n","iter[    5670/   10000], Loss/Closs_src=0.2708, lr=0.000451\n","iter[    5680/   10000], Loss/Closs_src=0.2951, lr=0.000451\n","iter[    5690/   10000], Loss/Closs_src=0.1609, lr=0.000450\n","iter[    5700/   10000], Loss/Closs_src=0.2335, lr=0.000450\n","iter[    5710/   10000], Loss/Closs_src=0.3474, lr=0.000450\n","iter[    5720/   10000], Loss/Closs_src=0.2443, lr=0.000449\n","iter[    5730/   10000], Loss/Closs_src=0.2526, lr=0.000449\n","iter[    5740/   10000], Loss/Closs_src=0.2276, lr=0.000448\n","iter[    5750/   10000], Loss/Closs_src=0.3237, lr=0.000448\n","iter[    5760/   10000], Loss/Closs_src=0.2889, lr=0.000448\n","iter[    5770/   10000], Loss/Closs_src=0.2806, lr=0.000447\n","iter[    5780/   10000], Loss/Closs_src=0.3027, lr=0.000447\n","iter[    5790/   10000], Loss/Closs_src=0.2187, lr=0.000447\n","iter[    5800/   10000], Loss/Closs_src=0.3703, lr=0.000446\n","iter[    5810/   10000], Loss/Closs_src=0.3139, lr=0.000446\n","iter[    5820/   10000], Loss/Closs_src=0.2301, lr=0.000445\n","iter[    5830/   10000], Loss/Closs_src=0.2753, lr=0.000445\n","iter[    5840/   10000], Loss/Closs_src=0.2789, lr=0.000445\n","iter[    5850/   10000], Loss/Closs_src=0.2611, lr=0.000444\n","iter[    5860/   10000], Loss/Closs_src=0.2745, lr=0.000444\n","iter[    5870/   10000], Loss/Closs_src=0.3641, lr=0.000444\n","iter[    5880/   10000], Loss/Closs_src=0.2614, lr=0.000443\n","iter[    5890/   10000], Loss/Closs_src=0.1693, lr=0.000443\n","iter[    5900/   10000], Loss/Closs_src=0.2676, lr=0.000442\n","iter[    5910/   10000], Loss/Closs_src=0.2072, lr=0.000442\n","iter[    5920/   10000], Loss/Closs_src=0.2763, lr=0.000442\n","iter[    5930/   10000], Loss/Closs_src=0.2478, lr=0.000441\n","iter[    5940/   10000], Loss/Closs_src=0.3782, lr=0.000441\n","iter[    5950/   10000], Loss/Closs_src=0.3096, lr=0.000441\n","iter[    5960/   10000], Loss/Closs_src=0.3661, lr=0.000440\n","iter[    5970/   10000], Loss/Closs_src=0.2884, lr=0.000440\n","iter[    5980/   10000], Loss/Closs_src=0.3107, lr=0.000439\n","iter[    5990/   10000], Loss/Closs_src=0.3061, lr=0.000439\n","iter[    6000/   10000], Loss/Closs_src=0.3326, lr=0.000439\n","Elapsed Time: 0:02:15.020801\n","Start Evaluation at 6000\n","\n","MNIST Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","MNIST mean Accuracy: 63.59%\n","MNIST Accuracy: 63.74%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","Best Accs: 31.22% 68.37% 57.27% 84.16% 85.54% 71.86% 31.73% 82.88% 72.07% 50.74% \n","Best mean Accuracy: 63.59%\n","Best Accuracy: 63.74%\n","iter[    6010/   10000], Loss/Closs_src=0.1840, lr=0.000438\n","iter[    6020/   10000], Loss/Closs_src=0.3105, lr=0.000438\n","iter[    6030/   10000], Loss/Closs_src=0.2594, lr=0.000438\n","iter[    6040/   10000], Loss/Closs_src=0.2259, lr=0.000437\n","iter[    6050/   10000], Loss/Closs_src=0.1782, lr=0.000437\n","iter[    6060/   10000], Loss/Closs_src=0.2265, lr=0.000437\n","iter[    6070/   10000], Loss/Closs_src=0.2384, lr=0.000436\n","iter[    6080/   10000], Loss/Closs_src=0.2760, lr=0.000436\n","iter[    6090/   10000], Loss/Closs_src=0.2202, lr=0.000435\n","iter[    6100/   10000], Loss/Closs_src=0.3106, lr=0.000435\n","iter[    6110/   10000], Loss/Closs_src=0.2727, lr=0.000435\n","iter[    6120/   10000], Loss/Closs_src=0.2869, lr=0.000434\n","iter[    6130/   10000], Loss/Closs_src=0.2946, lr=0.000434\n","iter[    6140/   10000], Loss/Closs_src=0.2780, lr=0.000434\n","iter[    6150/   10000], Loss/Closs_src=0.3185, lr=0.000433\n","iter[    6160/   10000], Loss/Closs_src=0.2927, lr=0.000433\n","iter[    6170/   10000], Loss/Closs_src=0.2242, lr=0.000433\n","iter[    6180/   10000], Loss/Closs_src=0.2033, lr=0.000432\n","iter[    6190/   10000], Loss/Closs_src=0.2409, lr=0.000432\n","iter[    6200/   10000], Loss/Closs_src=0.2768, lr=0.000432\n","iter[    6210/   10000], Loss/Closs_src=0.2794, lr=0.000431\n","iter[    6220/   10000], Loss/Closs_src=0.2399, lr=0.000431\n","iter[    6230/   10000], Loss/Closs_src=0.3099, lr=0.000430\n","iter[    6240/   10000], Loss/Closs_src=0.3188, lr=0.000430\n","iter[    6250/   10000], Loss/Closs_src=0.2535, lr=0.000430\n","iter[    6260/   10000], Loss/Closs_src=0.3130, lr=0.000429\n","iter[    6270/   10000], Loss/Closs_src=0.3179, lr=0.000429\n","iter[    6280/   10000], Loss/Closs_src=0.2285, lr=0.000429\n","iter[    6290/   10000], Loss/Closs_src=0.2180, lr=0.000428\n","iter[    6300/   10000], Loss/Closs_src=0.3144, lr=0.000428\n","iter[    6310/   10000], Loss/Closs_src=0.2688, lr=0.000428\n","iter[    6320/   10000], Loss/Closs_src=0.2657, lr=0.000427\n","iter[    6330/   10000], Loss/Closs_src=0.2951, lr=0.000427\n","iter[    6340/   10000], Loss/Closs_src=0.2633, lr=0.000427\n","iter[    6350/   10000], Loss/Closs_src=0.2797, lr=0.000426\n","iter[    6360/   10000], Loss/Closs_src=0.2209, lr=0.000426\n","iter[    6370/   10000], Loss/Closs_src=0.2600, lr=0.000426\n","iter[    6380/   10000], Loss/Closs_src=0.3226, lr=0.000425\n","iter[    6390/   10000], Loss/Closs_src=0.2933, lr=0.000425\n","iter[    6400/   10000], Loss/Closs_src=0.3323, lr=0.000425\n","iter[    6410/   10000], Loss/Closs_src=0.2741, lr=0.000424\n","iter[    6420/   10000], Loss/Closs_src=0.3111, lr=0.000424\n","iter[    6430/   10000], Loss/Closs_src=0.3097, lr=0.000424\n","iter[    6440/   10000], Loss/Closs_src=0.2406, lr=0.000423\n","iter[    6450/   10000], Loss/Closs_src=0.2289, lr=0.000423\n","iter[    6460/   10000], Loss/Closs_src=0.2675, lr=0.000423\n","iter[    6470/   10000], Loss/Closs_src=0.3005, lr=0.000422\n","iter[    6480/   10000], Loss/Closs_src=0.2417, lr=0.000422\n","iter[    6490/   10000], Loss/Closs_src=0.2249, lr=0.000422\n","iter[    6500/   10000], Loss/Closs_src=0.2946, lr=0.000421\n","Elapsed Time: 0:02:25.714795\n","Start Evaluation at 6500\n","\n","MNIST Accuracy of Each class\n","0                        : 31.53%\n","1                        : 74.63%\n","2                        : 58.53%\n","3                        : 91.09%\n","4                        : 77.39%\n","5                        : 76.91%\n","6                        : 34.45%\n","7                        : 84.63%\n","8                        : 58.21%\n","9                        : 37.86%\n","\n","MNIST mean Accuracy: 62.52%\n","MNIST Accuracy: 62.75%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","Best Accs: 31.22% 68.37% 57.27% 84.16% 85.54% 71.86% 31.73% 82.88% 72.07% 50.74% \n","Best mean Accuracy: 63.59%\n","Best Accuracy: 63.74%\n","iter[    6510/   10000], Loss/Closs_src=0.2737, lr=0.000421\n","iter[    6520/   10000], Loss/Closs_src=0.2100, lr=0.000421\n","iter[    6530/   10000], Loss/Closs_src=0.2550, lr=0.000420\n","iter[    6540/   10000], Loss/Closs_src=0.2814, lr=0.000420\n","iter[    6550/   10000], Loss/Closs_src=0.2212, lr=0.000420\n","iter[    6560/   10000], Loss/Closs_src=0.2714, lr=0.000419\n","iter[    6570/   10000], Loss/Closs_src=0.3255, lr=0.000419\n","iter[    6580/   10000], Loss/Closs_src=0.2895, lr=0.000419\n","iter[    6590/   10000], Loss/Closs_src=0.2597, lr=0.000418\n","iter[    6600/   10000], Loss/Closs_src=0.2440, lr=0.000418\n","iter[    6610/   10000], Loss/Closs_src=0.2722, lr=0.000418\n","iter[    6620/   10000], Loss/Closs_src=0.2484, lr=0.000417\n","iter[    6630/   10000], Loss/Closs_src=0.2865, lr=0.000417\n","iter[    6640/   10000], Loss/Closs_src=0.2682, lr=0.000417\n","iter[    6650/   10000], Loss/Closs_src=0.3238, lr=0.000416\n","iter[    6660/   10000], Loss/Closs_src=0.2872, lr=0.000416\n","iter[    6670/   10000], Loss/Closs_src=0.2160, lr=0.000416\n","iter[    6680/   10000], Loss/Closs_src=0.4017, lr=0.000415\n","iter[    6690/   10000], Loss/Closs_src=0.2182, lr=0.000415\n","iter[    6700/   10000], Loss/Closs_src=0.3621, lr=0.000415\n","iter[    6710/   10000], Loss/Closs_src=0.2428, lr=0.000414\n","iter[    6720/   10000], Loss/Closs_src=0.2488, lr=0.000414\n","iter[    6730/   10000], Loss/Closs_src=0.2587, lr=0.000414\n","iter[    6740/   10000], Loss/Closs_src=0.2265, lr=0.000413\n","iter[    6750/   10000], Loss/Closs_src=0.2723, lr=0.000413\n","iter[    6760/   10000], Loss/Closs_src=0.3938, lr=0.000413\n","iter[    6770/   10000], Loss/Closs_src=0.2664, lr=0.000412\n","iter[    6780/   10000], Loss/Closs_src=0.3076, lr=0.000412\n","iter[    6790/   10000], Loss/Closs_src=0.2884, lr=0.000412\n","iter[    6800/   10000], Loss/Closs_src=0.2438, lr=0.000412\n","iter[    6810/   10000], Loss/Closs_src=0.2160, lr=0.000411\n","iter[    6820/   10000], Loss/Closs_src=0.2688, lr=0.000411\n","iter[    6830/   10000], Loss/Closs_src=0.2930, lr=0.000411\n","iter[    6840/   10000], Loss/Closs_src=0.3194, lr=0.000410\n","iter[    6850/   10000], Loss/Closs_src=0.3216, lr=0.000410\n","iter[    6860/   10000], Loss/Closs_src=0.2569, lr=0.000410\n","iter[    6870/   10000], Loss/Closs_src=0.3234, lr=0.000409\n","iter[    6880/   10000], Loss/Closs_src=0.2586, lr=0.000409\n","iter[    6890/   10000], Loss/Closs_src=0.2488, lr=0.000409\n","iter[    6900/   10000], Loss/Closs_src=0.3056, lr=0.000408\n","iter[    6910/   10000], Loss/Closs_src=0.2736, lr=0.000408\n","iter[    6920/   10000], Loss/Closs_src=0.2743, lr=0.000408\n","iter[    6930/   10000], Loss/Closs_src=0.2694, lr=0.000408\n","iter[    6940/   10000], Loss/Closs_src=0.3296, lr=0.000407\n","iter[    6950/   10000], Loss/Closs_src=0.2316, lr=0.000407\n","iter[    6960/   10000], Loss/Closs_src=0.2226, lr=0.000407\n","iter[    6970/   10000], Loss/Closs_src=0.2776, lr=0.000406\n","iter[    6980/   10000], Loss/Closs_src=0.2092, lr=0.000406\n","iter[    6990/   10000], Loss/Closs_src=0.2401, lr=0.000406\n","iter[    7000/   10000], Loss/Closs_src=0.2987, lr=0.000405\n","Elapsed Time: 0:02:36.512900\n","Start Evaluation at 7000\n","\n","MNIST Accuracy of Each class\n","0                        : 34.90%\n","1                        : 63.44%\n","2                        : 59.79%\n","3                        : 91.68%\n","4                        : 92.97%\n","5                        : 73.21%\n","6                        : 22.96%\n","7                        : 80.84%\n","8                        : 49.59%\n","9                        : 28.05%\n","\n","MNIST mean Accuracy: 59.74%\n","MNIST Accuracy: 59.88%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","Best Accs: 31.22% 68.37% 57.27% 84.16% 85.54% 71.86% 31.73% 82.88% 72.07% 50.74% \n","Best mean Accuracy: 63.59%\n","Best Accuracy: 63.74%\n","iter[    7010/   10000], Loss/Closs_src=0.2510, lr=0.000405\n","iter[    7020/   10000], Loss/Closs_src=0.2250, lr=0.000405\n","iter[    7030/   10000], Loss/Closs_src=0.2656, lr=0.000404\n","iter[    7040/   10000], Loss/Closs_src=0.2366, lr=0.000404\n","iter[    7050/   10000], Loss/Closs_src=0.2575, lr=0.000404\n","iter[    7060/   10000], Loss/Closs_src=0.1507, lr=0.000404\n","iter[    7070/   10000], Loss/Closs_src=0.2033, lr=0.000403\n","iter[    7080/   10000], Loss/Closs_src=0.3131, lr=0.000403\n","iter[    7090/   10000], Loss/Closs_src=0.3039, lr=0.000403\n","iter[    7100/   10000], Loss/Closs_src=0.2548, lr=0.000402\n","iter[    7110/   10000], Loss/Closs_src=0.2538, lr=0.000402\n","iter[    7120/   10000], Loss/Closs_src=0.2442, lr=0.000402\n","iter[    7130/   10000], Loss/Closs_src=0.2289, lr=0.000401\n","iter[    7140/   10000], Loss/Closs_src=0.2868, lr=0.000401\n","iter[    7150/   10000], Loss/Closs_src=0.2598, lr=0.000401\n","iter[    7160/   10000], Loss/Closs_src=0.2958, lr=0.000401\n","iter[    7170/   10000], Loss/Closs_src=0.2751, lr=0.000400\n","iter[    7180/   10000], Loss/Closs_src=0.2050, lr=0.000400\n","iter[    7190/   10000], Loss/Closs_src=0.2358, lr=0.000400\n","iter[    7200/   10000], Loss/Closs_src=0.2134, lr=0.000399\n","iter[    7210/   10000], Loss/Closs_src=0.2723, lr=0.000399\n","iter[    7220/   10000], Loss/Closs_src=0.3196, lr=0.000399\n","iter[    7230/   10000], Loss/Closs_src=0.2481, lr=0.000399\n","iter[    7240/   10000], Loss/Closs_src=0.3207, lr=0.000398\n","iter[    7250/   10000], Loss/Closs_src=0.2253, lr=0.000398\n","iter[    7260/   10000], Loss/Closs_src=0.2307, lr=0.000398\n","iter[    7270/   10000], Loss/Closs_src=0.2137, lr=0.000397\n","iter[    7280/   10000], Loss/Closs_src=0.2781, lr=0.000397\n","iter[    7290/   10000], Loss/Closs_src=0.2410, lr=0.000397\n","iter[    7300/   10000], Loss/Closs_src=0.2305, lr=0.000396\n","iter[    7310/   10000], Loss/Closs_src=0.2729, lr=0.000396\n","iter[    7320/   10000], Loss/Closs_src=0.2834, lr=0.000396\n","iter[    7330/   10000], Loss/Closs_src=0.2243, lr=0.000396\n","iter[    7340/   10000], Loss/Closs_src=0.1798, lr=0.000395\n","iter[    7350/   10000], Loss/Closs_src=0.2855, lr=0.000395\n","iter[    7360/   10000], Loss/Closs_src=0.1717, lr=0.000395\n","iter[    7370/   10000], Loss/Closs_src=0.2041, lr=0.000394\n","iter[    7380/   10000], Loss/Closs_src=0.2240, lr=0.000394\n","iter[    7390/   10000], Loss/Closs_src=0.2064, lr=0.000394\n","iter[    7400/   10000], Loss/Closs_src=0.3157, lr=0.000394\n","iter[    7410/   10000], Loss/Closs_src=0.2309, lr=0.000393\n","iter[    7420/   10000], Loss/Closs_src=0.3069, lr=0.000393\n","iter[    7430/   10000], Loss/Closs_src=0.2724, lr=0.000393\n","iter[    7440/   10000], Loss/Closs_src=0.2299, lr=0.000392\n","iter[    7450/   10000], Loss/Closs_src=0.2770, lr=0.000392\n","iter[    7460/   10000], Loss/Closs_src=0.2309, lr=0.000392\n","iter[    7470/   10000], Loss/Closs_src=0.1993, lr=0.000392\n","iter[    7480/   10000], Loss/Closs_src=0.2809, lr=0.000391\n","iter[    7490/   10000], Loss/Closs_src=0.3296, lr=0.000391\n","iter[    7500/   10000], Loss/Closs_src=0.2026, lr=0.000391\n","Elapsed Time: 0:02:46.091114\n","Start Evaluation at 7500\n","\n","MNIST Accuracy of Each class\n","0                        : 28.57%\n","1                        : 84.67%\n","2                        : 55.81%\n","3                        : 92.18%\n","4                        : 84.22%\n","5                        : 72.42%\n","6                        : 19.83%\n","7                        : 76.17%\n","8                        : 43.84%\n","9                        : 49.45%\n","\n","MNIST mean Accuracy: 60.72%\n","MNIST Accuracy: 61.20%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","Best Accs: 31.22% 68.37% 57.27% 84.16% 85.54% 71.86% 31.73% 82.88% 72.07% 50.74% \n","Best mean Accuracy: 63.59%\n","Best Accuracy: 63.74%\n","iter[    7510/   10000], Loss/Closs_src=0.2411, lr=0.000391\n","iter[    7520/   10000], Loss/Closs_src=0.2588, lr=0.000390\n","iter[    7530/   10000], Loss/Closs_src=0.2603, lr=0.000390\n","iter[    7540/   10000], Loss/Closs_src=0.2320, lr=0.000390\n","iter[    7550/   10000], Loss/Closs_src=0.2014, lr=0.000389\n","iter[    7560/   10000], Loss/Closs_src=0.3053, lr=0.000389\n","iter[    7570/   10000], Loss/Closs_src=0.2518, lr=0.000389\n","iter[    7580/   10000], Loss/Closs_src=0.2719, lr=0.000389\n","iter[    7590/   10000], Loss/Closs_src=0.2459, lr=0.000388\n","iter[    7600/   10000], Loss/Closs_src=0.2934, lr=0.000388\n","iter[    7610/   10000], Loss/Closs_src=0.2730, lr=0.000388\n","iter[    7620/   10000], Loss/Closs_src=0.1834, lr=0.000387\n","iter[    7630/   10000], Loss/Closs_src=0.2139, lr=0.000387\n","iter[    7640/   10000], Loss/Closs_src=0.2444, lr=0.000387\n","iter[    7650/   10000], Loss/Closs_src=0.2235, lr=0.000387\n","iter[    7660/   10000], Loss/Closs_src=0.2297, lr=0.000386\n","iter[    7670/   10000], Loss/Closs_src=0.2697, lr=0.000386\n","iter[    7680/   10000], Loss/Closs_src=0.1940, lr=0.000386\n","iter[    7690/   10000], Loss/Closs_src=0.3174, lr=0.000386\n","iter[    7700/   10000], Loss/Closs_src=0.3033, lr=0.000385\n","iter[    7710/   10000], Loss/Closs_src=0.1896, lr=0.000385\n","iter[    7720/   10000], Loss/Closs_src=0.2298, lr=0.000385\n","iter[    7730/   10000], Loss/Closs_src=0.1993, lr=0.000384\n","iter[    7740/   10000], Loss/Closs_src=0.2240, lr=0.000384\n","iter[    7750/   10000], Loss/Closs_src=0.1972, lr=0.000384\n","iter[    7760/   10000], Loss/Closs_src=0.2441, lr=0.000384\n","iter[    7770/   10000], Loss/Closs_src=0.2206, lr=0.000383\n","iter[    7780/   10000], Loss/Closs_src=0.3110, lr=0.000383\n","iter[    7790/   10000], Loss/Closs_src=0.2589, lr=0.000383\n","iter[    7800/   10000], Loss/Closs_src=0.2940, lr=0.000383\n","iter[    7810/   10000], Loss/Closs_src=0.2198, lr=0.000382\n","iter[    7820/   10000], Loss/Closs_src=0.2244, lr=0.000382\n","iter[    7830/   10000], Loss/Closs_src=0.2649, lr=0.000382\n","iter[    7840/   10000], Loss/Closs_src=0.2275, lr=0.000382\n","iter[    7850/   10000], Loss/Closs_src=0.2227, lr=0.000381\n","iter[    7860/   10000], Loss/Closs_src=0.2510, lr=0.000381\n","iter[    7870/   10000], Loss/Closs_src=0.2637, lr=0.000381\n","iter[    7880/   10000], Loss/Closs_src=0.2916, lr=0.000381\n","iter[    7890/   10000], Loss/Closs_src=0.2741, lr=0.000380\n","iter[    7900/   10000], Loss/Closs_src=0.2587, lr=0.000380\n","iter[    7910/   10000], Loss/Closs_src=0.2060, lr=0.000380\n","iter[    7920/   10000], Loss/Closs_src=0.1907, lr=0.000379\n","iter[    7930/   10000], Loss/Closs_src=0.1558, lr=0.000379\n","iter[    7940/   10000], Loss/Closs_src=0.1835, lr=0.000379\n","iter[    7950/   10000], Loss/Closs_src=0.1908, lr=0.000379\n","iter[    7960/   10000], Loss/Closs_src=0.2310, lr=0.000378\n","iter[    7970/   10000], Loss/Closs_src=0.1973, lr=0.000378\n","iter[    7980/   10000], Loss/Closs_src=0.2025, lr=0.000378\n","iter[    7990/   10000], Loss/Closs_src=0.2278, lr=0.000378\n","iter[    8000/   10000], Loss/Closs_src=0.2079, lr=0.000377\n","Elapsed Time: 0:02:56.617278\n","Start Evaluation at 8000\n","\n","MNIST Accuracy of Each class\n","0                        : 38.27%\n","1                        : 67.84%\n","2                        : 59.11%\n","3                        : 89.41%\n","4                        : 84.52%\n","5                        : 77.35%\n","6                        : 28.71%\n","7                        : 85.21%\n","8                        : 43.63%\n","9                        : 34.19%\n","\n","MNIST mean Accuracy: 60.82%\n","MNIST Accuracy: 60.99%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","Best Accs: 31.22% 68.37% 57.27% 84.16% 85.54% 71.86% 31.73% 82.88% 72.07% 50.74% \n","Best mean Accuracy: 63.59%\n","Best Accuracy: 63.74%\n","iter[    8010/   10000], Loss/Closs_src=0.2838, lr=0.000377\n","iter[    8020/   10000], Loss/Closs_src=0.2666, lr=0.000377\n","iter[    8030/   10000], Loss/Closs_src=0.1891, lr=0.000377\n","iter[    8040/   10000], Loss/Closs_src=0.3391, lr=0.000376\n","iter[    8050/   10000], Loss/Closs_src=0.2328, lr=0.000376\n","iter[    8060/   10000], Loss/Closs_src=0.3014, lr=0.000376\n","iter[    8070/   10000], Loss/Closs_src=0.2716, lr=0.000376\n","iter[    8080/   10000], Loss/Closs_src=0.2416, lr=0.000375\n","iter[    8090/   10000], Loss/Closs_src=0.1812, lr=0.000375\n","iter[    8100/   10000], Loss/Closs_src=0.2723, lr=0.000375\n","iter[    8110/   10000], Loss/Closs_src=0.2458, lr=0.000375\n","iter[    8120/   10000], Loss/Closs_src=0.2284, lr=0.000374\n","iter[    8130/   10000], Loss/Closs_src=0.2186, lr=0.000374\n","iter[    8140/   10000], Loss/Closs_src=0.2045, lr=0.000374\n","iter[    8150/   10000], Loss/Closs_src=0.2646, lr=0.000374\n","iter[    8160/   10000], Loss/Closs_src=0.1935, lr=0.000373\n","iter[    8170/   10000], Loss/Closs_src=0.3250, lr=0.000373\n","iter[    8180/   10000], Loss/Closs_src=0.2726, lr=0.000373\n","iter[    8190/   10000], Loss/Closs_src=0.2698, lr=0.000373\n","iter[    8200/   10000], Loss/Closs_src=0.2973, lr=0.000372\n","iter[    8210/   10000], Loss/Closs_src=0.2548, lr=0.000372\n","iter[    8220/   10000], Loss/Closs_src=0.1798, lr=0.000372\n","iter[    8230/   10000], Loss/Closs_src=0.2646, lr=0.000372\n","iter[    8240/   10000], Loss/Closs_src=0.2164, lr=0.000371\n","iter[    8250/   10000], Loss/Closs_src=0.2734, lr=0.000371\n","iter[    8260/   10000], Loss/Closs_src=0.2782, lr=0.000371\n","iter[    8270/   10000], Loss/Closs_src=0.2093, lr=0.000371\n","iter[    8280/   10000], Loss/Closs_src=0.2411, lr=0.000370\n","iter[    8290/   10000], Loss/Closs_src=0.2507, lr=0.000370\n","iter[    8300/   10000], Loss/Closs_src=0.2472, lr=0.000370\n","iter[    8310/   10000], Loss/Closs_src=0.1940, lr=0.000370\n","iter[    8320/   10000], Loss/Closs_src=0.2310, lr=0.000369\n","iter[    8330/   10000], Loss/Closs_src=0.1861, lr=0.000369\n","iter[    8340/   10000], Loss/Closs_src=0.1862, lr=0.000369\n","iter[    8350/   10000], Loss/Closs_src=0.2437, lr=0.000369\n","iter[    8360/   10000], Loss/Closs_src=0.2207, lr=0.000368\n","iter[    8370/   10000], Loss/Closs_src=0.2550, lr=0.000368\n","iter[    8380/   10000], Loss/Closs_src=0.2823, lr=0.000368\n","iter[    8390/   10000], Loss/Closs_src=0.2225, lr=0.000368\n","iter[    8400/   10000], Loss/Closs_src=0.2365, lr=0.000367\n","iter[    8410/   10000], Loss/Closs_src=0.3431, lr=0.000367\n","iter[    8420/   10000], Loss/Closs_src=0.2039, lr=0.000367\n","iter[    8430/   10000], Loss/Closs_src=0.2321, lr=0.000367\n","iter[    8440/   10000], Loss/Closs_src=0.2541, lr=0.000366\n","iter[    8450/   10000], Loss/Closs_src=0.2209, lr=0.000366\n","iter[    8460/   10000], Loss/Closs_src=0.3043, lr=0.000366\n","iter[    8470/   10000], Loss/Closs_src=0.1800, lr=0.000366\n","iter[    8480/   10000], Loss/Closs_src=0.2635, lr=0.000365\n","iter[    8490/   10000], Loss/Closs_src=0.2712, lr=0.000365\n","iter[    8500/   10000], Loss/Closs_src=0.1870, lr=0.000365\n","Elapsed Time: 0:03:07.398368\n","Start Evaluation at 8500\n","\n","MNIST Accuracy of Each class\n","0                        : 47.65%\n","1                        : 58.33%\n","2                        : 61.92%\n","3                        : 92.67%\n","4                        : 92.77%\n","5                        : 63.23%\n","6                        : 25.68%\n","7                        : 76.17%\n","8                        : 37.27%\n","9                        : 25.67%\n","\n","MNIST mean Accuracy: 58.14%\n","MNIST Accuracy: 58.30%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","Best Accs: 31.22% 68.37% 57.27% 84.16% 85.54% 71.86% 31.73% 82.88% 72.07% 50.74% \n","Best mean Accuracy: 63.59%\n","Best Accuracy: 63.74%\n","iter[    8510/   10000], Loss/Closs_src=0.2570, lr=0.000365\n","iter[    8520/   10000], Loss/Closs_src=0.2672, lr=0.000365\n","iter[    8530/   10000], Loss/Closs_src=0.2072, lr=0.000364\n","iter[    8540/   10000], Loss/Closs_src=0.2410, lr=0.000364\n","iter[    8550/   10000], Loss/Closs_src=0.3020, lr=0.000364\n","iter[    8560/   10000], Loss/Closs_src=0.2405, lr=0.000364\n","iter[    8570/   10000], Loss/Closs_src=0.2961, lr=0.000363\n","iter[    8580/   10000], Loss/Closs_src=0.2781, lr=0.000363\n","iter[    8590/   10000], Loss/Closs_src=0.2261, lr=0.000363\n","iter[    8600/   10000], Loss/Closs_src=0.2040, lr=0.000363\n","iter[    8610/   10000], Loss/Closs_src=0.2639, lr=0.000362\n","iter[    8620/   10000], Loss/Closs_src=0.3023, lr=0.000362\n","iter[    8630/   10000], Loss/Closs_src=0.2776, lr=0.000362\n","iter[    8640/   10000], Loss/Closs_src=0.2562, lr=0.000362\n","iter[    8650/   10000], Loss/Closs_src=0.2488, lr=0.000361\n","iter[    8660/   10000], Loss/Closs_src=0.2805, lr=0.000361\n","iter[    8670/   10000], Loss/Closs_src=0.1922, lr=0.000361\n","iter[    8680/   10000], Loss/Closs_src=0.2320, lr=0.000361\n","iter[    8690/   10000], Loss/Closs_src=0.2379, lr=0.000361\n","iter[    8700/   10000], Loss/Closs_src=0.2528, lr=0.000360\n","iter[    8710/   10000], Loss/Closs_src=0.2655, lr=0.000360\n","iter[    8720/   10000], Loss/Closs_src=0.1952, lr=0.000360\n","iter[    8730/   10000], Loss/Closs_src=0.2778, lr=0.000360\n","iter[    8740/   10000], Loss/Closs_src=0.2231, lr=0.000359\n","iter[    8750/   10000], Loss/Closs_src=0.2759, lr=0.000359\n","iter[    8760/   10000], Loss/Closs_src=0.2019, lr=0.000359\n","iter[    8770/   10000], Loss/Closs_src=0.2420, lr=0.000359\n","iter[    8780/   10000], Loss/Closs_src=0.2675, lr=0.000358\n","iter[    8790/   10000], Loss/Closs_src=0.2479, lr=0.000358\n","iter[    8800/   10000], Loss/Closs_src=0.1871, lr=0.000358\n","iter[    8810/   10000], Loss/Closs_src=0.2438, lr=0.000358\n","iter[    8820/   10000], Loss/Closs_src=0.1883, lr=0.000358\n","iter[    8830/   10000], Loss/Closs_src=0.2626, lr=0.000357\n","iter[    8840/   10000], Loss/Closs_src=0.2923, lr=0.000357\n","iter[    8850/   10000], Loss/Closs_src=0.1796, lr=0.000357\n","iter[    8860/   10000], Loss/Closs_src=0.2751, lr=0.000357\n","iter[    8870/   10000], Loss/Closs_src=0.2458, lr=0.000356\n","iter[    8880/   10000], Loss/Closs_src=0.2220, lr=0.000356\n","iter[    8890/   10000], Loss/Closs_src=0.2193, lr=0.000356\n","iter[    8900/   10000], Loss/Closs_src=0.2610, lr=0.000356\n","iter[    8910/   10000], Loss/Closs_src=0.1932, lr=0.000356\n","iter[    8920/   10000], Loss/Closs_src=0.2845, lr=0.000355\n","iter[    8930/   10000], Loss/Closs_src=0.2050, lr=0.000355\n","iter[    8940/   10000], Loss/Closs_src=0.3145, lr=0.000355\n","iter[    8950/   10000], Loss/Closs_src=0.1831, lr=0.000355\n","iter[    8960/   10000], Loss/Closs_src=0.2504, lr=0.000354\n","iter[    8970/   10000], Loss/Closs_src=0.2083, lr=0.000354\n","iter[    8980/   10000], Loss/Closs_src=0.2509, lr=0.000354\n","iter[    8990/   10000], Loss/Closs_src=0.2330, lr=0.000354\n","iter[    9000/   10000], Loss/Closs_src=0.2483, lr=0.000354\n","Elapsed Time: 0:03:18.306445\n","Start Evaluation at 9000\n","\n","MNIST Accuracy of Each class\n","0                        : 44.18%\n","1                        : 68.46%\n","2                        : 60.76%\n","3                        : 85.94%\n","4                        : 93.79%\n","5                        : 82.06%\n","6                        : 29.54%\n","7                        : 80.25%\n","8                        : 53.80%\n","9                        : 34.59%\n","\n","MNIST mean Accuracy: 63.34%\n","MNIST Accuracy: 63.39%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","Best Accs: 31.22% 68.37% 57.27% 84.16% 85.54% 71.86% 31.73% 82.88% 72.07% 50.74% \n","Best mean Accuracy: 63.59%\n","Best Accuracy: 63.74%\n","iter[    9010/   10000], Loss/Closs_src=0.2947, lr=0.000353\n","iter[    9020/   10000], Loss/Closs_src=0.2695, lr=0.000353\n","iter[    9030/   10000], Loss/Closs_src=0.2639, lr=0.000353\n","iter[    9040/   10000], Loss/Closs_src=0.2747, lr=0.000353\n","iter[    9050/   10000], Loss/Closs_src=0.2255, lr=0.000352\n","iter[    9060/   10000], Loss/Closs_src=0.1886, lr=0.000352\n","iter[    9070/   10000], Loss/Closs_src=0.2717, lr=0.000352\n","iter[    9080/   10000], Loss/Closs_src=0.3660, lr=0.000352\n","iter[    9090/   10000], Loss/Closs_src=0.2244, lr=0.000352\n","iter[    9100/   10000], Loss/Closs_src=0.2406, lr=0.000351\n","iter[    9110/   10000], Loss/Closs_src=0.1532, lr=0.000351\n","iter[    9120/   10000], Loss/Closs_src=0.2340, lr=0.000351\n","iter[    9130/   10000], Loss/Closs_src=0.2660, lr=0.000351\n","iter[    9140/   10000], Loss/Closs_src=0.2065, lr=0.000350\n","iter[    9150/   10000], Loss/Closs_src=0.2330, lr=0.000350\n","iter[    9160/   10000], Loss/Closs_src=0.2115, lr=0.000350\n","iter[    9170/   10000], Loss/Closs_src=0.2158, lr=0.000350\n","iter[    9180/   10000], Loss/Closs_src=0.2135, lr=0.000350\n","iter[    9190/   10000], Loss/Closs_src=0.1740, lr=0.000349\n","iter[    9200/   10000], Loss/Closs_src=0.1876, lr=0.000349\n","iter[    9210/   10000], Loss/Closs_src=0.2744, lr=0.000349\n","iter[    9220/   10000], Loss/Closs_src=0.1789, lr=0.000349\n","iter[    9230/   10000], Loss/Closs_src=0.2595, lr=0.000349\n","iter[    9240/   10000], Loss/Closs_src=0.1982, lr=0.000348\n","iter[    9250/   10000], Loss/Closs_src=0.1836, lr=0.000348\n","iter[    9260/   10000], Loss/Closs_src=0.1943, lr=0.000348\n","iter[    9270/   10000], Loss/Closs_src=0.2360, lr=0.000348\n","iter[    9280/   10000], Loss/Closs_src=0.3041, lr=0.000347\n","iter[    9290/   10000], Loss/Closs_src=0.1535, lr=0.000347\n","iter[    9300/   10000], Loss/Closs_src=0.2330, lr=0.000347\n","iter[    9310/   10000], Loss/Closs_src=0.2467, lr=0.000347\n","iter[    9320/   10000], Loss/Closs_src=0.2849, lr=0.000347\n","iter[    9330/   10000], Loss/Closs_src=0.2324, lr=0.000346\n","iter[    9340/   10000], Loss/Closs_src=0.2351, lr=0.000346\n","iter[    9350/   10000], Loss/Closs_src=0.1853, lr=0.000346\n","iter[    9360/   10000], Loss/Closs_src=0.2159, lr=0.000346\n","iter[    9370/   10000], Loss/Closs_src=0.2030, lr=0.000346\n","iter[    9380/   10000], Loss/Closs_src=0.2383, lr=0.000345\n","iter[    9390/   10000], Loss/Closs_src=0.2137, lr=0.000345\n","iter[    9400/   10000], Loss/Closs_src=0.1971, lr=0.000345\n","iter[    9410/   10000], Loss/Closs_src=0.1714, lr=0.000345\n","iter[    9420/   10000], Loss/Closs_src=0.2420, lr=0.000345\n","iter[    9430/   10000], Loss/Closs_src=0.1628, lr=0.000344\n","iter[    9440/   10000], Loss/Closs_src=0.2097, lr=0.000344\n","iter[    9450/   10000], Loss/Closs_src=0.1712, lr=0.000344\n","iter[    9460/   10000], Loss/Closs_src=0.1948, lr=0.000344\n","iter[    9470/   10000], Loss/Closs_src=0.2336, lr=0.000344\n","iter[    9480/   10000], Loss/Closs_src=0.2186, lr=0.000343\n","iter[    9490/   10000], Loss/Closs_src=0.1830, lr=0.000343\n","iter[    9500/   10000], Loss/Closs_src=0.2149, lr=0.000343\n","Elapsed Time: 0:03:29.211536\n","Start Evaluation at 9500\n","\n","MNIST Accuracy of Each class\n","0                        : 33.88%\n","1                        : 59.47%\n","2                        : 63.86%\n","3                        : 82.57%\n","4                        : 88.70%\n","5                        : 81.17%\n","6                        : 19.94%\n","7                        : 85.70%\n","8                        : 44.35%\n","9                        : 28.64%\n","\n","MNIST mean Accuracy: 58.83%\n","MNIST Accuracy: 58.88%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","Best Accs: 31.22% 68.37% 57.27% 84.16% 85.54% 71.86% 31.73% 82.88% 72.07% 50.74% \n","Best mean Accuracy: 63.59%\n","Best Accuracy: 63.74%\n","iter[    9510/   10000], Loss/Closs_src=0.2414, lr=0.000343\n","iter[    9520/   10000], Loss/Closs_src=0.2387, lr=0.000342\n","iter[    9530/   10000], Loss/Closs_src=0.1793, lr=0.000342\n","iter[    9540/   10000], Loss/Closs_src=0.2480, lr=0.000342\n","iter[    9550/   10000], Loss/Closs_src=0.2487, lr=0.000342\n","iter[    9560/   10000], Loss/Closs_src=0.2545, lr=0.000342\n","iter[    9570/   10000], Loss/Closs_src=0.2702, lr=0.000341\n","iter[    9580/   10000], Loss/Closs_src=0.2042, lr=0.000341\n","iter[    9590/   10000], Loss/Closs_src=0.2232, lr=0.000341\n","iter[    9600/   10000], Loss/Closs_src=0.1611, lr=0.000341\n","iter[    9610/   10000], Loss/Closs_src=0.2040, lr=0.000341\n","iter[    9620/   10000], Loss/Closs_src=0.1832, lr=0.000340\n","iter[    9630/   10000], Loss/Closs_src=0.2131, lr=0.000340\n","iter[    9640/   10000], Loss/Closs_src=0.2553, lr=0.000340\n","iter[    9650/   10000], Loss/Closs_src=0.2619, lr=0.000340\n","iter[    9660/   10000], Loss/Closs_src=0.2042, lr=0.000340\n","iter[    9670/   10000], Loss/Closs_src=0.2176, lr=0.000339\n","iter[    9680/   10000], Loss/Closs_src=0.1533, lr=0.000339\n","iter[    9690/   10000], Loss/Closs_src=0.2907, lr=0.000339\n","iter[    9700/   10000], Loss/Closs_src=0.2130, lr=0.000339\n","iter[    9710/   10000], Loss/Closs_src=0.2370, lr=0.000339\n","iter[    9720/   10000], Loss/Closs_src=0.1988, lr=0.000338\n","iter[    9730/   10000], Loss/Closs_src=0.1807, lr=0.000338\n","iter[    9740/   10000], Loss/Closs_src=0.2116, lr=0.000338\n","iter[    9750/   10000], Loss/Closs_src=0.2542, lr=0.000338\n","iter[    9760/   10000], Loss/Closs_src=0.2541, lr=0.000338\n","iter[    9770/   10000], Loss/Closs_src=0.2530, lr=0.000337\n","iter[    9780/   10000], Loss/Closs_src=0.1673, lr=0.000337\n","iter[    9790/   10000], Loss/Closs_src=0.1677, lr=0.000337\n","iter[    9800/   10000], Loss/Closs_src=0.2299, lr=0.000337\n","iter[    9810/   10000], Loss/Closs_src=0.2372, lr=0.000337\n","iter[    9820/   10000], Loss/Closs_src=0.2761, lr=0.000336\n","iter[    9830/   10000], Loss/Closs_src=0.2299, lr=0.000336\n","iter[    9840/   10000], Loss/Closs_src=0.2558, lr=0.000336\n","iter[    9850/   10000], Loss/Closs_src=0.2773, lr=0.000336\n","iter[    9860/   10000], Loss/Closs_src=0.2174, lr=0.000336\n","iter[    9870/   10000], Loss/Closs_src=0.2100, lr=0.000335\n","iter[    9880/   10000], Loss/Closs_src=0.2529, lr=0.000335\n","iter[    9890/   10000], Loss/Closs_src=0.2037, lr=0.000335\n","iter[    9900/   10000], Loss/Closs_src=0.2273, lr=0.000335\n","iter[    9910/   10000], Loss/Closs_src=0.1829, lr=0.000335\n","iter[    9920/   10000], Loss/Closs_src=0.1436, lr=0.000334\n","iter[    9930/   10000], Loss/Closs_src=0.2079, lr=0.000334\n","iter[    9940/   10000], Loss/Closs_src=0.1769, lr=0.000334\n","iter[    9950/   10000], Loss/Closs_src=0.2112, lr=0.000334\n","iter[    9960/   10000], Loss/Closs_src=0.1918, lr=0.000334\n","iter[    9970/   10000], Loss/Closs_src=0.1738, lr=0.000334\n","iter[    9980/   10000], Loss/Closs_src=0.1892, lr=0.000333\n","iter[    9990/   10000], Loss/Closs_src=0.2014, lr=0.000333\n","iter[   10000/   10000], Loss/Closs_src=0.2346, lr=0.000333\n","Elapsed Time: 0:03:40.901473\n","Start Evaluation at 10000\n","\n","MNIST Accuracy of Each class\n","0                        : 26.02%\n","1                        : 74.10%\n","2                        : 71.22%\n","3                        : 90.20%\n","4                        : 88.49%\n","5                        : 74.66%\n","6                        : 18.89%\n","7                        : 77.53%\n","8                        : 44.25%\n","9                        : 49.95%\n","\n","MNIST mean Accuracy: 61.53%\n","MNIST Accuracy: 61.90%\n","\n","Best {MNIST} Accuracy of Each class\n","0                        : 31.22%\n","1                        : 68.37%\n","2                        : 57.27%\n","3                        : 84.16%\n","4                        : 85.54%\n","5                        : 71.86%\n","6                        : 31.73%\n","7                        : 82.88%\n","8                        : 72.07%\n","9                        : 50.74%\n","\n","Best Accs: 31.22% 68.37% 57.27% 84.16% 85.54% 71.86% 31.73% 82.88% 72.07% 50.74% \n","Best mean Accuracy: 63.59%\n","Best Accuracy: 63.74%\n","Total Time:  0:03:43.414560\n"]}],"source":["no_adapt()"]},{"cell_type":"markdown","metadata":{"id":"L9_R0968QkEk"},"source":["# Train with DSBN\n","\n","DSBN 코드는 domain adaptation을 위해 다음의 방법들을 사용했습니다.\n","\n","<img src=\"https://ifh.cc/g/Jrshtr.png\">\n","\n","    1. DSBN\n","    2. Adversarial loss\n","    3. Semantic Matching loss\n","\n","Train code를 통해 추가적으로 Adversarial loss를 구현해보도록 하겠습니다.\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"KLe8E3MRQkEk","executionInfo":{"status":"ok","timestamp":1692342916841,"user_tz":-540,"elapsed":3,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["def train(model, discriminator, centroids,\n","          dataloaders, optimizers, lr_scheduler, ce_loss, bce_loss, start_time):\n","    source_train_loader, target_train_loader, target_val_loader = dataloaders\n","    source_train_loader_iter, target_train_loader_iter, target_val_loader_iter = map(iter, dataloaders)\n","    src_centroid, trg_centroid = centroids\n","    optimizer, optimizer_D = optimizers\n","\n","    best_accuracy = 0.0\n","    monitor = Monitor()\n","    for i_iter in range(start_iter, end_iter+1):\n","        try:\n","            x_s, y_s = next(source_train_loader_iter)\n","        except StopIteration:\n","            source_train_loader_iter = iter(source_train_loader)\n","            x_s, y_s = next(source_train_loader_iter)\n","\n","        try:\n","            x_t, y_t = next(target_train_loader_iter)\n","        except StopIteration:\n","            target_train_loader_iter = iter(target_train_loader)\n","            x_t, y_t = next(target_train_loader_iter)\n","\n","        x_s, y_s, x_t, y_t = x_s.cuda(), y_s.cuda(), x_t.cuda(), y_t.cuda()\n","        current_lr = lr_scheduler.current_lr(i_iter)\n","        adaptation_lambda = adaptation_factor(i_iter / float(end_iter),\n","                                              gamma=adaptation_gamma)\n","\n","        # init optimizer\n","        optimizer.zero_grad()\n","        lr_scheduler(optimizer, i_iter)\n","        optimizer_D.zero_grad()\n","        lr_scheduler(optimizer_D, i_iter)\n","\n","        ########################################################################################################\n","        #                                               Train G                                                #\n","        ########################################################################################################\n","        for param in discriminator.parameters():\n","            param.requires_grad = False\n","\n","        src_domain_id = torch.zeros(x_s.shape[0], dtype=torch.long).cuda()\n","        trg_domain_id = torch.ones(x_t.shape[0], dtype=torch.long).cuda()\n","\n","        pred_s, f_s = model(x_s, src_domain_id, with_ft=True)\n","        pred_t, f_t = model(x_t, trg_domain_id, with_ft=True)\n","\n","        Closs_src = ce_loss(pred_s, y_s)\n","        monitor.update({\"Loss/Closs_src\": float(Closs_src)})\n","\n","        Floss = Closs_src\n","\n","        ############ TODO #############\n","        '''\n","        discriminator를 사용해 loss를 Gloss를 구하시오.\n","        discriminator:\n","            Input: f_s or f_t\n","\n","        loss:\n","            domain_loss_adjust_factor * bce_loss( , )\n","        '''\n","        # Dout_s =\n","        Dout_s = discriminator(f_s)\n","        source_label = torch.zeros_like(Dout_s).cuda()\n","        # loss_adv_src =\n","        loss_adv_src = domain_loss_adjust_factor * bce_loss(Dout_s, source_label)\n","\n","        # Dout_t =\n","        Dout_t = discriminator(f_t)\n","        target_label = torch.ones_like(Dout_t).cuda()\n","        # loss_adv_trg =\n","        # loss_adv_trg = adaptation_lambda * bce_loss(Dout_t, target_label)\n","        loss_adv_trg = domain_loss_adjust_factor * bce_loss(Dout_t, target_label)\n","\n","        ############ TODO #############\n","\n","        Gloss =  - (loss_adv_src + loss_adv_trg) / 2\n","        monitor.update({'Loss/Gloss': float(Gloss)})\n","\n","        Floss = Floss + adaptation_lambda * Gloss\n","\n","        # pseudo label generation\n","        pred_t_pseudo = []\n","        with torch.no_grad():\n","            model.eval()\n","            pred_t_pseudo = model(x_t, trg_domain_id, with_ft=False)\n","            model.train(True)\n","\n","        # moving semantic loss\n","        current_src_centroid = src_centroid(f_s, y_s)\n","        current_trg_centroid = trg_centroid(f_t, torch.argmax(pred_t_pseudo, 1))\n","\n","        semantic_loss = semantic_loss_calc(current_src_centroid, current_trg_centroid)\n","        monitor.update({'Loss/SMloss': float(semantic_loss)})\n","\n","        Floss = Floss + adaptation_lambda * semantic_loss\n","\n","        # Floss backward\n","        Floss.backward()\n","        optimizer.step()\n","        ########################################################################################################\n","        #                                               Train D                                                #\n","        ########################################################################################################\n","        for param in discriminator.parameters():\n","            param.requires_grad = True\n","\n","        ############ TODO #############\n","        '''\n","        위에서 Gloss를 구하는 과정을 참고해 Dloss를 구하시오.\n","\n","        Hint: discriminator input으로 들어가는 feature들은 detach 해줘야 합니다.\n","        '''\n","        # Dout_s =\n","        # source_label =\n","        # loss_adv_src =\n","\n","        # Dout_t =\n","        # target_label =\n","        # loss_adv_trg =\n","\n","        Dout_s = discriminator(f_s.detach())\n","        source_label = torch.zeros_like(Dout_s).cuda()\n","        #loss_adv_src = adaptation_lambda * bce_loss(Dout_s, source_label)\n","        loss_adv_src = domain_loss_adjust_factor * bce_loss(Dout_s, source_label)\n","\n","        Dout_t = discriminator(f_t.detach())\n","        target_label = torch.ones_like(Dout_t).cuda()\n","        loss_adv_trg = domain_loss_adjust_factor * bce_loss(Dout_t, target_label)\n","\n","\n","        ############ TODO #############\n","\n","        Dloss = (loss_adv_src + loss_adv_trg) / 2\n","        monitor.update({'Loss/Dloss': float(Dloss)})\n","        Dloss = adaptation_lambda * Dloss\n","        Dloss.backward()\n","        optimizer_D.step()\n","\n","        src_centroid.centroids.data = current_src_centroid.data\n","        trg_centroid.centroids.data = current_trg_centroid.data\n","\n","        if i_iter % disp_interval == 0  and i_iter != 0:\n","            disp_msg = 'iter[{:8d}/{:8d}], '.format(i_iter, end_iter)\n","            disp_msg += str(monitor)\n","            disp_msg += ', lambda={:.6f}'.format(adaptation_lambda)\n","            disp_msg += ', lr={:.6f}'.format(current_lr)\n","            print(disp_msg)\n","\n","            monitor.reset()\n","\n","        if i_iter % save_interval == 0 and i_iter != 0:\n","            print(\"Elapsed Time: {}\".format(datetime.datetime.now() - start_time))\n","            print(\"Start Evaluation at {:d}\".format(i_iter))\n","\n","            model.eval()\n","\n","            pred_vals = []\n","            y_vals = []\n","            x_val = None\n","            y_val = None\n","            pred_val = None\n","\n","            with torch.no_grad():\n","                for i, (x_val, y_val) in enumerate(target_val_loader):\n","                    y_vals.append(y_val.cpu())\n","                    x_val = x_val.cuda()\n","                    y_val = y_val.cuda()\n","\n","                    pred_val = model(x_val, trg_domain_id, with_ft=False)\n","                    pred_vals.append(pred_val.cpu())\n","\n","            pred_vals = torch.cat(pred_vals, 0)\n","            y_vals = torch.cat(y_vals, 0)\n","            total_val_accuracy = float(accuracy(pred_vals, y_vals, topk=(1,))[0])\n","\n","            val_accuracy_each_c = [(c_name, float(accuracy_of_c(pred_vals, y_vals,\n","                                                                class_idx=c, topk=(1,))[0]))\n","                                   for c, c_name in enumerate(range(num_classes))]\n","            print('\\nMNIST Accuracy of Each class')\n","            print(''.join([\"{:<25}: {:.2f}%\\n\".format(c_name, 100 * c_val_acc)\n","                            for c_name, c_val_acc in val_accuracy_each_c]))\n","\n","            mean_val_accuracy = float(\n","                torch.mean(torch.FloatTensor([c_val_acc for _, c_val_acc in val_accuracy_each_c])))\n","\n","            print('MNIST mean Accuracy: {:.2f}%'.format(100 * mean_val_accuracy))\n","            print(\"MNIST Accuracy: {:.2f}%\".format(total_val_accuracy * 100))\n","\n","            model.train()\n","\n","            val_accuracy = total_val_accuracy\n","\n","            del x_val, y_val, pred_val, pred_vals, y_vals\n","\n","            if val_accuracy > best_accuracy:\n","                #save best model\n","                best_accuracy = val_accuracy\n","                best_accuracy_each_c = val_accuracy_each_c\n","                best_mean_val_accuracy = mean_val_accuracy\n","                best_total_val_accuracy = total_val_accuracy\n","\n","                model = model.cuda()\n","                discriminator = discriminator.cuda()\n","                src_centroid = src_centroid.cuda()\n","                trg_centroid = trg_centroid.cuda()\n","\n","            print('\\nBest {MNIST} Accuracy of Each class')\n","            print(''.join([\"{:<25}: {:.2f}%\\n\".format(c_name, 100 * c_val_acc)\n","                           for c_name, c_val_acc in best_accuracy_each_c]))\n","            print('Best Accs: ' + ''.join([\"{:.2f}% \".format(100 * c_val_acc)\n","                                           for _, c_val_acc in best_accuracy_each_c]))\n","            print('Best mean Accuracy: {:.2f}%'.format(100 * best_mean_val_accuracy))\n","            print('Best Accuracy: {:.2f}%'.format(100 * best_total_val_accuracy))\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"-WsTT65SQkEl","executionInfo":{"status":"ok","timestamp":1692342916841,"user_tz":-540,"elapsed":3,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["def main():\n","    start_time = datetime.datetime.now()\n","\n","    # make save_dir\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","\n","    dataloaders = get_dataloaders()\n","\n","    ###################################################################################################################\n","    #                                               Model Loading                                                     #\n","    ###################################################################################################################\n","    model = DSBNLeNet(num_classes = num_classes,\n","                      num_domains = num_domains)\n","\n","    model.train(True)\n","    model = model.cuda()\n","    params = get_optimizer_params(model,\n","                                  lr = learning_rate,\n","                                  weight_decay=weight_decay,\n","                                  base_weight_factor=base_weight_factor)\n","\n","    discriminator = Discriminator(in_features=num_classes).cuda()\n","    D_params = get_optimizer_params(model,\n","                                    lr = learning_rate,\n","                                    weight_decay=weight_decay,\n","                                    base_weight_factor=None)\n","    ### For sm_loss\n","    src_centroid = Centroids(num_classes, num_classes).cuda()\n","    trg_centroid = Centroids(num_classes, num_classes).cuda()\n","    centroids = [src_centroid, trg_centroid]\n","\n","    ###################################################################################################################\n","    #                                               Train Configurations                                              #\n","    ###################################################################################################################\n","    ce_loss = nn.CrossEntropyLoss()\n","    bce_loss = nn.BCEWithLogitsLoss()\n","\n","    lr_scheduler = LRScheduler(learning_rate, end_iter, base_weight_factor=base_weight_factor)\n","\n","    optimizer = optim.Adam(params, betas=(0.9, 0.999))\n","    optimizer_D = optim.Adam(D_params, betas=(0.9, 0.999))\n","    optimizers = [optimizer, optimizer_D]\n","\n","    train(model, discriminator, centroids, dataloaders, optimizers, lr_scheduler, ce_loss, bce_loss, start_time)\n","    print('Total Time:  {}'.format((datetime.datetime.now() - start_time)))\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"aERaZIj4QkEl","colab":{"base_uri":"https://localhost:8080/","height":352},"executionInfo":{"status":"error","timestamp":1692342919480,"user_tz":-540,"elapsed":2641,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"fc7bae9e-4c93-4fbb-cb37-2e71651d3433"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: ./data/svhn/train_32x32.mat\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-e1826b9b609b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#                                               Model Loading                                                     #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m###################################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     model = DSBNLeNet(num_classes = num_classes,\n\u001b[0m\u001b[1;32m     14\u001b[0m                       num_domains = num_domains)\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-d030710637c6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, weights_init_path, num_domains)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_domains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_domains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DSBNLeNet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_init_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-d030710637c6>\u001b[0m in \u001b[0;36msetup_net\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDomainSpecificBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_domains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'DomainSpecificBatchNorm2d'"]}],"source":["main()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1xE95d0ZjNpMqjKSCXLVWSkA75mejVBKW","timestamp":1692334341616},{"file_id":"1elQN0lqn5nFT00An5pCKeU7S3gfzhf5X","timestamp":1692120287988}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}