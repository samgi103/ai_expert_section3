{"cells":[{"cell_type":"markdown","metadata":{"id":"dFETUmNgQkEQ"},"source":["# Domain Specific Batch Normalization (DSBN)\n","\n"," > Domain-Specific Batch Normalization for Unsupervised Domain Adaptation (DSBN) (https://arxiv.org/abs/1906.03950)\n","\n","## Overview\n","\n","DSBN은 domain마다 고유의 batch normalization (BN) layer를 사용해, 각 BN layer가 맡은 domain의 정보를 BN parameter를 통해 학습합니다.\n","\n","그 후, normalization을 하면서 domain-specific한 정보를 제거하고, 모델로 하여금 domain-invariant feature를 학습할 수 있게 한 method입니다.\n","\n","DSBN은 2 stage로 학습이 되는데, 이번 실습은 그 중 첫 번째 stage에 대한 학습을 구현하는 실습입니다.\n","\n","<img src = \"https://github.com/wgchang/DSBN/raw/master/captions/dsbn.jpg\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkoZK7JWQkEV"},"outputs":[],"source":["import argparse\n","import logging\n","import pprint\n","import datetime\n","import sys\n","import random\n","from collections import defaultdict\n","import math\n","\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","from torch.utils import data\n","from torchvision.datasets import MNIST, SVHN\n","from torchvision import transforms\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"63i2ltPuQkEY"},"source":["일관적인 학습 결과를 위해 random seed를 고정하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0rinIsBQkEZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692117333393,"user_tz":-540,"elapsed":18,"user":{"displayName":"Jungyun Choi","userId":"11532088005133064485"}},"outputId":"6d37503c-0f9c-4e8e-ea5e-99881d33b54c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ransom Seed: 2023\n"]}],"source":["# basic random seed\n","import os\n","import random\n","import numpy as np\n","\n","random_seed= 2023\n","\n","def seedBasic(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","\n","# torch random seed\n","import torch\n","def seedTorch(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# basic + torch\n","def seedEverything(seed):\n","    seedBasic(seed)\n","    seedTorch(seed)\n","    print(f\"Ransom Seed: {seed}\")\n","\n","\n","seedEverything(2023)"]},{"cell_type":"markdown","metadata":{"id":"8foX3V75QkEZ"},"source":["# 모델 구현하기\n","\n","DSBN을 구현하고, 이를 이용해 DSBN을 포함한 LeNet을 구현해봅니다.\n","\n","<br>\n","<br>\n","\n","\n","## DSBN(Domain Specific Batch Normalization) 구현\n","먼저, 이번 실습에서 가장 핵심이 되는 DSBN을 정의해줍니다.\n","torch에 이미 정의된 BatchNorm2d를 이용하여 Domain Specific한 Batch Normalization을 구현할 수 있습니다.\n","\n","- nn.BatchNorm2d\n","\n","<img src=\"https://ifh.cc/g/gdkh0f.png\">\n","\n","  - num_features : input size (N,C,H,W)에서 C를 의미합니다.\n","  - eps : Numerical stability를 위해 분모에 추가된 값으로서 default는 0.1입니다. (위의 식에서 ϵ에 해당되며 분모가 0이 되어 NaN이 되는것을 방지)\n","  - momentum : running_mean과 running_var의 계산을 위해 사용되는 값으로서 default는 0.1입니다. (Trainining을 수행시에는 batch단위의 평균과 분산으로 batch-norm을 수행하고, Test를 수행시에는 축적된 running_mean/variance를 사용합니다)\n","  - affine : Boolean value이며, True로 설정시에, 이 모듈에 학습가능한 affine parameter가 있습니다. Default는 True입니다. (False로 설정시에 위의식에서 γ=1, β=0)\n","  - track_running_stats : Boolean value이며, True로 설정시에 이 모듈은 running mean과 variance를 track하고 False로 설정시에는 이 모듈이 track하지 않고, running_mean과 running_var를 None으로 초기화하며, 이 값이 None인 경우에는 이 모듈은 항상 batch statistics를 사용합니다. Default는 True입니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yP1MifRxQkEa"},"outputs":[],"source":["class DomainSpecificBatchNorm2d(nn.Module):\n","    _version = 2\n","\n","    def __init__(self, num_features, num_classes, eps=1e-5, momentum=0.1, affine=True,\n","                 track_running_stats=True):\n","        super(DomainSpecificBatchNorm2d, self).__init__()\n","        ############ TODO #############\n","        '''\n","        https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n","        위 링크에서 BatchNorm2d를 참고하여 다음의 self.bns를 채우세요.\n","        '''\n","        self.bns = []\n","\n","        ############ TODO #############\n","\n","    def reset_running_stats(self):\n","        for bn in self.bns:\n","            bn.reset_running_stats()\n","\n","    def reset_parameters(self):\n","        for bn in self.bns:\n","            bn.reset_parameters()\n","\n","    def _check_input_dim(self, input):\n","        if input.dim() != 4:\n","            raise ValueError('expected 4D input (got {}D input)'\n","                             .format(input.dim()))\n","\n","    def forward(self, x, domain_label):\n","        self._check_input_dim(x)\n","        bn = self.bns[domain_label[0]]\n","        return bn(x), domain_label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0pyKhaLQkEb"},"outputs":[],"source":["def init_weights(obj):\n","    for m in obj.modules():\n","        if isinstance(m, nn.Conv2d):\n","            # init.xavier_normal_(m.weight)\n","            m.weight.data.normal_(0, 0.01).clamp_(min=-0.02, max=0.02)\n","            try:\n","                m.bias.data.zero_()\n","            except AttributeError:\n","                # no bias\n","                pass\n","        if isinstance(m, nn.Linear):\n","            m.weight.data.normal_(0, 0.01).clamp_(min=-0.02, max=0.02)\n","            try:\n","                m.bias.data.zero_()\n","            except AttributeError:\n","                # no bias\n","                pass\n","        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n","            m.reset_parameters()\n","        elif isinstance(m, nn.Embedding):\n","            init.normal_(m.weight, 0, 0.01)"]},{"cell_type":"markdown","source":["## LeNet 구현하기\n","다음으로 LeNet을 구현해봅니다. LeNet의 기본 구조는 다음과 같습니다.\n","\n","<img src='https://miro.medium.com/v2/resize:fit:1204/format:webp/1*9MRcNBz9uHXplXzd3ii6VQ.png'>\n","\n","이번 실습에서는 LeNet의 기본 구조를 따라가되, 몇 가지 파라미터를 수정하고 Batch Normalization 모듈을 추가하여 실험을 진행할 것입니다."],"metadata":{"id":"Eim0B1ic5TEz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfnEiH0XQkEc"},"outputs":[],"source":["class LeNet(nn.Module):\n","    \"\"\"\"Network used for MNIST or USPS experiments.\"\"\"\n","\n","    def __init__(self, num_classes=10, weights_init_path=None):\n","        super(LeNet, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_channels = 3\n","        self.image_size = 28\n","        self.name = 'LeNet'\n","        self.setup_net()\n","\n","        if weights_init_path is not None:\n","            init_weights(self)\n","            self.load(weights_init_path)\n","        else:\n","            init_weights(self)\n","\n","    def setup_net(self):\n","        self.conv1 = nn.Conv2d(self.num_channels, 20, kernel_size=5)\n","        self.bn1 = nn.BatchNorm2d(20)\n","        self.pool1 = nn.MaxPool2d(2)\n","        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n","        self.bn2 = nn.BatchNorm2d(50)\n","        self.pool2 = nn.MaxPool2d(2)\n","\n","        self.fc1 = nn.Linear(50 * 4 * 4, 500)\n","        self.fc2 = nn.Linear(500, self.num_classes)\n","\n","    def forward(self, x, with_ft=False):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.pool1(F.relu(x))\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.pool2(F.relu(x))\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        x = self.fc2(F.relu(x))\n","        feat = x\n","\n","        if with_ft:\n","            return x, feat\n","        else:\n","            return x\n","\n","    def load(self, init_path):\n","        net_init_dict = torch.load(init_path)\n","        init_weights(self)\n","        updated_state_dict = self.state_dict()\n","        print('load {} params.'.format(init_path))\n","        for k, v in updated_state_dict.items():\n","            if k in net_init_dict:\n","                if v.shape == net_init_dict[k].shape:\n","                    updated_state_dict[k] = net_init_dict[k]\n","                else:\n","                    print(\n","                        \"{0} params' shape not the same as pretrained params. Initialize with default settings.\".format(\n","                            k))\n","            else:\n","                print(\"{0} params does not exist. Initialize with default settings.\".format(k))\n","        self.load_state_dict(updated_state_dict)"]},{"cell_type":"markdown","source":["## DSBN을 포함한 LeNet 구현하기\n","이제 DSBN을 LeNet을 구현해봅니다."],"metadata":{"id":"dqsIpK3L8Phr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RO_0gEmVQkEd"},"outputs":[],"source":["class DSBNLeNet(nn.Module):\n","    \"\"\"\"Network used for MNIST or USPS experiments. Conditional Batch Normalization is added.\"\"\"\n","\n","    def __init__(self, num_classes=10, weights_init_path=None, num_domains=2):\n","        super(DSBNLeNet, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_channels = 3\n","        self.image_size = 28\n","        self.num_domains = num_domains\n","        self.name = 'DSBNLeNet'\n","        self.setup_net()\n","\n","        if weights_init_path is not None:\n","            init_weights(self)\n","            self.load(weights_init_path)\n","        else:\n","            init_weights(self)\n","\n","    def setup_net(self):\n","        ############ TODO #############\n","        '''\n","        위 LeNet을 참고하여 DSBN을 포함시킨 DSBNLeNet을 완성하세요.\n","        '''\n","\n","\n","\n","\n","        ############ TODO #############\n","\n","    def forward(self, x, y, with_ft=False):\n","        x = self.conv1(x)\n","        x, _ = self.bn1(x, y)\n","        x = self.pool1(F.relu(x))\n","        x = self.conv2(x)\n","        x, _ = self.bn2(x, y)\n","        x = self.pool2(F.relu(x))\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        x = self.fc2(F.relu(x))\n","        feat = x\n","\n","        if with_ft:\n","            return x, feat\n","        else:\n","            return x\n","\n","    def load(self, init_path):\n","        net_init_dict = torch.load(init_path)\n","        init_weights(self)\n","        updated_state_dict = self.state_dict()\n","        print('load {} params.'.format(init_path))\n","        for k, v in updated_state_dict.items():\n","            if k in net_init_dict:\n","                if v.shape == net_init_dict[k].shape:\n","                    updated_state_dict[k] = net_init_dict[k]\n","                else:\n","                    print(\n","                        \"{0} params' shape not the same as pretrained params. Initialize with default settings.\".format(\n","                            k))\n","            else:\n","                print(\"{0} params does not exist. Initialize with default settings.\".format(k))\n","        self.load_state_dict(updated_state_dict)"]},{"cell_type":"markdown","metadata":{"id":"GJ6bhTTiQkEe"},"source":["학습에 필요한 추가적인 model들도 정의해줍니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mVGpqBdQkEe"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, in_features):\n","        super(Discriminator, self).__init__()\n","        self.in_features = in_features\n","\n","        self.discriminator = nn.Sequential(\n","            nn.Linear(self.in_features, 500),\n","            nn.ReLU(),\n","            nn.Linear(500, 500),\n","            nn.ReLU(),\n","            nn.Linear(500, 1)\n","        )\n","\n","        init_weights(self)\n","\n","    def forward(self, x):\n","        return self.discriminator(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yv6CauO7QkEf"},"outputs":[],"source":["class Centroids(nn.Module):\n","  '''\n","  Learning Semantic Representations for Unsupervised Domain Adaptation\n","  http://proceedings.mlr.press/v80/xie18c/xie18c.pdf\n","  '''\n","  def __init__(self, feature_dim, num_classes, decay_const=0.3):\n","      super(Centroids, self).__init__()\n","      self.decay_const = decay_const\n","      self.num_classes = num_classes\n","      self.centroids = nn.Parameter(torch.randn(num_classes, feature_dim))\n","      self.centroids.requires_grad = False\n","      self.reset_parameters()\n","\n","  def reset_parameters(self):\n","      self.centroids.data.zero_()\n","\n","  def forward(self, x, y, y_mask=None):\n","      classes = torch.unique(y)\n","      current_centroids = []\n","      for c in range(self.num_classes):\n","          if c in classes:\n","              if y_mask is not None:\n","                  avg_c = torch.sum(x[(y == c) & y_mask, :], dim=0) / torch.sum((y == c) & y_mask).float()\n","              else:\n","                  avg_c = torch.sum(x[(y == c), :], dim=0) / torch.sum((y == c)).float()\n","              current_centroids.append(avg_c * self.decay_const + (1 - self.decay_const) * self.centroids[c:c + 1, :])\n","          else:\n","              current_centroids.append(self.centroids[c:c + 1, :])\n","      current_centroids = torch.cat(current_centroids, 0)\n","      return current_centroids\n"]},{"cell_type":"markdown","metadata":{"id":"9x5-QVKrQkEf"},"source":["# DataLoader 구현하기\n","\n","이번 실습을 위한 domain adaptation setting은 svhn --> mnist 입니다.\n","<br>\n","<br>\n","\n","### MNIST\n","MNIST 데이터셋은 손으로 쓴 숫자 이미지로 이루어진 대형 데이터셋이며,\n","\n","60,000개의 Training dataset과 10,000개의 Test dataset으로 이루어져 있습니다.\n","\n","<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/440px-MnistExamples.png'>\n","\n","<br>\n","<br>\n","\n","### SVHN\n","SVHN은 Google Street View에서 수집된 숫자 데이터셋으로,\n","\n","MNIST와 마찬가지로 0부터 9까지 숫자 10개의 이미지로 이루어진 데이터셋 입니다.\n","\n","<img src='https://production-media.paperswithcode.com/datasets/SVHN-0000000424-c12734ed_mMXUnWD.jpg' width=\"500\" height=\"300\">\n","\n","\n","데이터에 대해서 더 자세하게 알고 싶다면, 다음의 링크를 참조해주세요.\n","\n","https://kjhov195.github.io/2020-02-09-image_dataset_1/"]},{"cell_type":"markdown","metadata":{"id":"ATVjyy9TQkEf"},"source":["### source dataset으로 svhn, target dataset으로 mnist를 구현해주시길 바랍니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKPa_miaQkEg"},"outputs":[],"source":["MNIST_DIR = './data/mnist'\n","SVHN_DIR  = './data/svhn'\n","batch_size = 40\n","num_workers = 2\n","\n","def get_dataloaders(src_data_path = './data/svhn', trg_data_path = './data/mnist'):\n","    ############ TODO #############\n","    '''\n","    source_transform:\n","        1. resize to 28 by 28\n","        2. convert to tensor\n","        3. normalize by mean (0.5, 0.5, 0.5) and std (0.5, 0.5, 0.5)\n","\n","    target_transform:\n","        1. convert to RGB from Grayscale (Hint: Use transforms.Lambda)\n","        2. convert to tensor\n","        3. normalize by mean (0.5, 0.5, 0.5) and std (0.5, 0.5, 0.5)\n","\n","    source_train_dataset:\n","        SVHN을 이용해 구현\n","        (참고: https://pytorch.org/vision/main/generated/torchvision.datasets.SVHN.html#torchvision.datasets.SVHN)\n","\n","    target_train_dataset & target_val_dataset:\n","        MNIST로 구현\n","        (참고: https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html)\n","    '''\n","\n","    source_transform =\n","    target_transform =\n","\n","    source_train_dataset =\n","    target_train_dataset =\n","    target_val_dataset =\n","\n","    ############ TODO #############\n","\n","    source_train_dataloader = data.DataLoader(source_train_dataset,\n","                                              batch_size = batch_size,\n","                                              shuffle = True,\n","                                              num_workers=num_workers,\n","                                              drop_last=True,\n","                                              pin_memory=True)\n","\n","    target_train_dataloader = data.DataLoader(target_train_dataset,\n","                                              batch_size = batch_size,\n","                                              shuffle = True,\n","                                              num_workers=num_workers,\n","                                              drop_last=True,\n","                                              pin_memory=True)\n","\n","    target_val_dataloader = data.DataLoader(target_val_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers=num_workers,\n","                                            drop_last=True,\n","                                            pin_memory=True)\n","\n","    return source_train_dataloader, target_train_dataloader, target_val_dataloader"]},{"cell_type":"markdown","metadata":{"id":"y5dnPC_1QkEg"},"source":["# Utils for training\n","\n","학습에 필요한 다양한 기능들을 구현한 부분입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMH2wHnGQkEg"},"outputs":[],"source":["def get_optimizer_params(modules, lr, weight_decay=0.0005, base_weight_factor=0.1):\n","    weights = []\n","    biases = []\n","    base_weights = []\n","    base_biases = []\n","\n","    module = modules\n","    for key, value in dict(module.named_parameters()).items():\n","        if value.requires_grad:\n","            if 'fc' in key or 'score' in key:\n","                if 'bias' in key:\n","                    biases += [value]\n","                else:\n","                    weights += [value]\n","            else:\n","                if 'bias' in key:\n","                    base_biases += [value]\n","                else:\n","                    base_weights += [value]\n","    if base_weight_factor:\n","        params = [\n","            {'params': weights, 'lr': lr, 'weight_decay': weight_decay},\n","            {'params': biases, 'lr': lr },\n","            {'params': base_weights, 'lr': lr * base_weight_factor, 'weight_decay': weight_decay},\n","            {'params': base_biases, 'lr': lr * base_weight_factor},\n","        ]\n","    else:\n","        params = [\n","            {'params': base_weights + weights, 'lr': lr, 'weight_decay': weight_decay},\n","            {'params': base_biases + biases, 'lr': lr},\n","        ]\n","    return params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJhl9KArQkEh"},"outputs":[],"source":["class Monitor:\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self._cummulated_losses = defaultdict(lambda: 0.0)\n","        self._total_counts = defaultdict(lambda: 0)\n","\n","    def update(self, losses_dict):\n","        for key in losses_dict:\n","            self._cummulated_losses[key] += losses_dict[key]\n","            self._total_counts[key] += 1\n","\n","    @property\n","    def cummulated_losses(self):\n","        return self._cummulated_losses\n","\n","    @property\n","    def total_counts(self):\n","        return self._total_counts\n","\n","    @property\n","    def losses(self):\n","        losses = {}\n","        for k, v in self._cummulated_losses.items():\n","            if self._total_counts[k] > 0:\n","                losses[k] = v / float(self._total_counts[k])\n","            else:\n","                losses[k] = 0.0\n","        return losses\n","\n","    def __repr__(self):\n","        sorted_loss_keys = sorted([k for k in self._cummulated_losses.keys()])\n","        losses = self.losses\n","        repr_str = ''\n","        for key in sorted_loss_keys:\n","            repr_str += ', {0}={1:.4f}'.format(key, losses[key])\n","        return repr_str[2:]\n","\n","\n","def one_hot_encoding(y, n_classes):\n","    tensor_size = [y.size(i) for i in range(len(y.size()))]\n","    if tensor_size[-1] != 1:\n","        tensor_size += [1]\n","    tensor_size = tuple(tensor_size)\n","    y_one_hot = torch.zeros(tensor_size[:-1] + (n_classes,)).to(y.device).scatter_(len(tensor_size) - 1,\n","                                                                                   y.view(tensor_size), 1)\n","    return y_one_hot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0w6qtkkkQkEh"},"outputs":[],"source":["def lr_poly(base_lr, i_iter, alpha=10, beta=0.75, num_steps=250000):\n","    if i_iter < 0:\n","        return base_lr\n","    return base_lr / ((1 + alpha * float(i_iter) / num_steps) ** (beta))\n","\n","class LRScheduler:\n","    def __init__(self, learning_rate, num_steps=200000, alpha=10,\n","                 beta=0.75, base_weight_factor=False):\n","        self.learning_rate = learning_rate\n","        self.num_steps = num_steps\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.base_weight_factor = base_weight_factor\n","\n","    def __call__(self, optimizer, i_iter):\n","        lr_i_iter = i_iter\n","        lr = self.learning_rate\n","\n","        if len(optimizer.param_groups) == 1:\n","            optimizer.param_groups[0]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","        elif len(optimizer.param_groups) == 2:\n","            optimizer.param_groups[0]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","            optimizer.param_groups[1]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","        elif len(optimizer.param_groups) == 4:\n","            optimizer.param_groups[0]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","            optimizer.param_groups[1]['lr'] = lr_poly(lr, lr_i_iter, alpha=self.alpha, beta=self.beta,\n","                                                      num_steps=self.num_steps)\n","            optimizer.param_groups[2]['lr'] = self.base_weight_factor * lr_poly(lr, lr_i_iter, alpha=self.alpha,\n","                                                                                beta=self.beta,\n","                                                                                num_steps=self.num_steps)\n","            optimizer.param_groups[3]['lr'] = self.base_weight_factor * lr_poly(lr, lr_i_iter, alpha=self.alpha,\n","                                                                                beta=self.beta,\n","                                                                                num_steps=self.num_steps)\n","        else:\n","            raise RuntimeError('Wrong optimizer param groups')\n","\n","    def current_lr(self, i_iter):\n","        return lr_poly(self.learning_rate, i_iter, alpha=self.alpha, beta=self.beta, num_steps=self.num_steps)\n"]},{"cell_type":"markdown","metadata":{"id":"CFs0KmUkQkEi"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Qpwqjn4QkEi"},"outputs":[],"source":["save_dir = './logs'\n","num_classes = 10\n","num_domains = 2 # source + target\n","num_source_domains = 1\n","num_target_domains = 1\n","\n","start_iter = 1\n","end_iter = 30000\n","adaptation_gamma = 10\n","\n","learning_rate = 0.001\n","weight_decay = 0.0\n","base_weight_factor = 0.1\n","\n","best_accuracy = 0.0\n","best_accuracy_each_c = 0.0\n","best_mean_val_accuracy = 0.0\n","best_total_val_accuracy = 0.0\n","\n","disp_interval = 10\n","save_interval = 500\n","domain_loss_adjust_factor = 0.1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Df1HemmrQkEi"},"outputs":[],"source":["def adaptation_factor(p, gamma=10):\n","    p = max(min(p, 1.0), 0.0)\n","    den = 1.0 + math.exp(-gamma * p)\n","    lamb = 2.0 / den - 1.0\n","    return min(lamb, 1.0)\n","\n","def semantic_loss_calc(x, y, mean=True):\n","    loss = (x - y) ** 2\n","    if mean:\n","        return torch.mean(loss)\n","    else:\n","        return loss\n","\n","def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n","    maxk = max(topk)\n","    num_samples = target.size(0)\n","\n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].view(-1).float().sum(0)\n","        res.append(correct_k.div_(num_samples))\n","    return res\n","\n","\n","def accuracy_of_c(output, target, class_idx, topk=(1,)):\n","    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n","    maxk = max(topk)\n","    # num_samples = target.size(0)\n","    selection = target == class_idx\n","    target_selected = target[selection]\n","    output_selected = output[selection]\n","    num_samples = torch.sum(selection).float()\n","\n","    _, pred = output_selected.topk(maxk, 1, True, True)\n","    pred = pred.t().float()\n","    correct = pred.eq((target_selected.view(1, -1).expand_as(pred)).float())\n","\n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].view(-1).float().sum(0)\n","        res.append(correct_k.div_(num_samples))\n","    return res"]},{"cell_type":"markdown","metadata":{"id":"HzWISwUMQkEi"},"source":["# Train with no adaptation\n","\n","### Domain Adaptation 없이 학습 시켰을 때 결과를 보도록 하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrdiI5ESQkEj"},"outputs":[],"source":["def train_no_adapt(model, dataloaders, optimizer, lr_scheduler, ce_loss, start_time):\n","    source_train_loader, target_train_loader, target_val_loader = dataloaders\n","    source_train_loader_iter, target_train_loader_iter, target_val_loader_iter = map(iter, dataloaders)\n","\n","    best_accuracy = 0.0\n","    monitor = Monitor()\n","    for i_iter in range(start_iter, 10001):\n","        try:\n","            x_s, y_s = next(source_train_loader_iter)\n","        except StopIteration:\n","            source_train_loader_iter = iter(source_train_loader)\n","            x_s, y_s = next(source_train_loader_iter)\n","\n","        x_s, y_s = x_s.cuda(), y_s.cuda()\n","        current_lr = lr_scheduler.current_lr(i_iter)\n","\n","        # init optimizer\n","        optimizer.zero_grad()\n","        lr_scheduler(optimizer, i_iter)\n","\n","        ########################################################################################################\n","        #                                               Train                                                  #\n","        ########################################################################################################\n","\n","        pred_s, f_s = model(x_s, with_ft=True)\n","\n","        Closs_src = ce_loss(pred_s, y_s)\n","        monitor.update({\"Loss/Closs_src\": float(Closs_src)})\n","\n","        Floss = Closs_src\n","\n","        # Floss backward\n","        Floss.backward()\n","        optimizer.step()\n","\n","\n","        if i_iter % disp_interval == 0  and i_iter != 0:\n","            disp_msg = 'iter[{:8d}/{:8d}], '.format(i_iter, 10000)\n","            disp_msg += str(monitor)\n","            disp_msg += ', lr={:.6f}'.format(current_lr)\n","            print(disp_msg)\n","\n","            monitor.reset()\n","\n","        if i_iter % save_interval == 0 and i_iter != 0:\n","            print(\"Elapsed Time: {}\".format(datetime.datetime.now() - start_time))\n","            print(\"Start Evaluation at {:d}\".format(i_iter))\n","\n","            model.eval()\n","\n","            pred_vals = []\n","            y_vals = []\n","            x_val = None\n","            y_val = None\n","            pred_val = None\n","\n","            with torch.no_grad():\n","                for i, (x_val, y_val) in enumerate(target_val_loader):\n","                    y_vals.append(y_val.cpu())\n","                    x_val = x_val.cuda()\n","                    y_val = y_val.cuda()\n","\n","                    pred_val = model(x_val, with_ft=False)\n","                    pred_vals.append(pred_val.cpu())\n","\n","            pred_vals = torch.cat(pred_vals, 0)\n","            y_vals = torch.cat(y_vals, 0)\n","            total_val_accuracy = float(accuracy(pred_vals, y_vals, topk=(1,))[0])\n","\n","            val_accuracy_each_c = [(c_name, float(accuracy_of_c(pred_vals, y_vals,\n","                                                                class_idx=c, topk=(1,))[0]))\n","                                   for c, c_name in enumerate(range(num_classes))]\n","            print('\\nMNIST Accuracy of Each class')\n","            print(''.join([\"{:<25}: {:.2f}%\\n\".format(c_name, 100 * c_val_acc)\n","                            for c_name, c_val_acc in val_accuracy_each_c]))\n","\n","            mean_val_accuracy = float(\n","                torch.mean(torch.FloatTensor([c_val_acc for _, c_val_acc in val_accuracy_each_c])))\n","\n","            print('MNIST mean Accuracy: {:.2f}%'.format(100 * mean_val_accuracy))\n","            print(\"MNIST Accuracy: {:.2f}%\".format(total_val_accuracy * 100))\n","\n","            model.train()\n","\n","            val_accuracy = total_val_accuracy\n","\n","            del x_val, y_val, pred_val, pred_vals, y_vals\n","\n","            if val_accuracy > best_accuracy:\n","                #save best model\n","                best_accuracy = val_accuracy\n","                best_accuracy_each_c = val_accuracy_each_c\n","                best_mean_val_accuracy = mean_val_accuracy\n","                best_total_val_accuracy = total_val_accuracy\n","\n","                model = model.cuda()\n","\n","            print('\\nBest {MNIST} Accuracy of Each class')\n","            print(''.join([\"{:<25}: {:.2f}%\\n\".format(c_name, 100 * c_val_acc)\n","                           for c_name, c_val_acc in best_accuracy_each_c]))\n","            print('Best Accs: ' + ''.join([\"{:.2f}% \".format(100 * c_val_acc)\n","                                           for _, c_val_acc in best_accuracy_each_c]))\n","            print('Best mean Accuracy: {:.2f}%'.format(100 * best_mean_val_accuracy))\n","            print('Best Accuracy: {:.2f}%'.format(100 * best_total_val_accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyGt8fY-QkEj"},"outputs":[],"source":["def no_adapt():\n","    start_time = datetime.datetime.now()\n","\n","    # make save_dir\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","\n","    dataloaders = get_dataloaders()\n","\n","    ###################################################################################################################\n","    #                                               Model Loading                                                     #\n","    ###################################################################################################################\n","    model = LeNet(num_classes = num_classes)\n","\n","    model.train(True)\n","    model = model.cuda()\n","    params = get_optimizer_params(model,\n","                                  lr = learning_rate,\n","                                  weight_decay=weight_decay,\n","                                  base_weight_factor=base_weight_factor)\n","\n","    ###################################################################################################################\n","    #                                               Train Configurations                                              #\n","    ###################################################################################################################\n","    ce_loss = nn.CrossEntropyLoss()\n","\n","    lr_scheduler = LRScheduler(learning_rate, end_iter, base_weight_factor=base_weight_factor)\n","\n","    optimizer = optim.Adam(params, betas=(0.9, 0.999))\n","\n","    train_no_adapt(model, dataloaders, optimizer, lr_scheduler, ce_loss, start_time)\n","    print('Total Time:  {}'.format((datetime.datetime.now() - start_time)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wvCaI1-QkEj"},"outputs":[],"source":["no_adapt()"]},{"cell_type":"markdown","metadata":{"id":"L9_R0968QkEk"},"source":["# Train with DSBN\n","\n","DSBN 코드는 domain adaptation을 위해 다음의 방법들을 사용했습니다.\n","\n","<img src=\"https://ifh.cc/g/Jrshtr.png\">\n","\n","    1. DSBN\n","    2. Adversarial loss\n","    3. Semantic Matching loss\n","\n","Train code를 통해 추가적으로 Adversarial loss를 구현해보도록 하겠습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLe8E3MRQkEk"},"outputs":[],"source":["def train(model, discriminator, centroids,\n","          dataloaders, optimizers, lr_scheduler, ce_loss, bce_loss, start_time):\n","    source_train_loader, target_train_loader, target_val_loader = dataloaders\n","    source_train_loader_iter, target_train_loader_iter, target_val_loader_iter = map(iter, dataloaders)\n","    src_centroid, trg_centroid = centroids\n","    optimizer, optimizer_D = optimizers\n","\n","    best_accuracy = 0.0\n","    monitor = Monitor()\n","    for i_iter in range(start_iter, end_iter+1):\n","        try:\n","            x_s, y_s = next(source_train_loader_iter)\n","        except StopIteration:\n","            source_train_loader_iter = iter(source_train_loader)\n","            x_s, y_s = next(source_train_loader_iter)\n","\n","        try:\n","            x_t, y_t = next(target_train_loader_iter)\n","        except StopIteration:\n","            target_train_loader_iter = iter(target_train_loader)\n","            x_t, y_t = next(target_train_loader_iter)\n","\n","        x_s, y_s, x_t, y_t = x_s.cuda(), y_s.cuda(), x_t.cuda(), y_t.cuda()\n","        current_lr = lr_scheduler.current_lr(i_iter)\n","        adaptation_lambda = adaptation_factor(i_iter / float(end_iter),\n","                                              gamma=adaptation_gamma)\n","\n","        # init optimizer\n","        optimizer.zero_grad()\n","        lr_scheduler(optimizer, i_iter)\n","        optimizer_D.zero_grad()\n","        lr_scheduler(optimizer_D, i_iter)\n","\n","        ########################################################################################################\n","        #                                               Train G                                                #\n","        ########################################################################################################\n","        for param in discriminator.parameters():\n","            param.requires_grad = False\n","\n","        src_domain_id = torch.zeros(x_s.shape[0], dtype=torch.long).cuda()\n","        trg_domain_id = torch.ones(x_t.shape[0], dtype=torch.long).cuda()\n","\n","        pred_s, f_s = model(x_s, src_domain_id, with_ft=True)\n","        pred_t, f_t = model(x_t, trg_domain_id, with_ft=True)\n","\n","        Closs_src = ce_loss(pred_s, y_s)\n","        monitor.update({\"Loss/Closs_src\": float(Closs_src)})\n","\n","        Floss = Closs_src\n","\n","        ############ TODO #############\n","        '''\n","        discriminator를 사용해 loss를 Gloss를 구하시오.\n","        discriminator:\n","            Input: f_s or f_t\n","\n","        loss:\n","            domain_loss_adjust_factor * bce_loss( , )\n","        '''\n","        Dout_s =\n","        source_label = torch.zeros_like(Dout_s).cuda()\n","        loss_adv_src =\n","\n","        Dout_t =\n","        target_label = torch.ones_like(Dout_t).cuda()\n","        loss_adv_trg =\n","\n","        ############ TODO #############\n","\n","        Gloss =  - (loss_adv_src + loss_adv_trg) / 2\n","        monitor.update({'Loss/Gloss': float(Gloss)})\n","\n","        Floss = Floss + adaptation_lambda * Gloss\n","\n","        # pseudo label generation\n","        pred_t_pseudo = []\n","        with torch.no_grad():\n","            model.eval()\n","            pred_t_pseudo = model(x_t, trg_domain_id, with_ft=False)\n","            model.train(True)\n","\n","        # moving semantic loss\n","        current_src_centroid = src_centroid(f_s, y_s)\n","        current_trg_centroid = trg_centroid(f_t, torch.argmax(pred_t_pseudo, 1))\n","\n","        semantic_loss = semantic_loss_calc(current_src_centroid, current_trg_centroid)\n","        monitor.update({'Loss/SMloss': float(semantic_loss)})\n","\n","        Floss = Floss + adaptation_lambda * semantic_loss\n","\n","        # Floss backward\n","        Floss.backward()\n","        optimizer.step()\n","        ########################################################################################################\n","        #                                               Train D                                                #\n","        ########################################################################################################\n","        for param in discriminator.parameters():\n","            param.requires_grad = True\n","\n","        ############ TODO #############\n","        '''\n","        위에서 Gloss를 구하는 과정을 참고해 Dloss를 구하시오.\n","\n","        Hint: discriminator input으로 들어가는 feature들은 detach 해줘야 합니다.\n","        '''\n","        Dout_s =\n","        source_label =\n","        loss_adv_src =\n","\n","        Dout_t =\n","        target_label =\n","        loss_adv_trg =\n","        ############ TODO #############\n","\n","        Dloss = (loss_adv_src + loss_adv_trg) / 2\n","        monitor.update({'Loss/Dloss': float(Dloss)})\n","        Dloss = adaptation_lambda * Dloss\n","        Dloss.backward()\n","        optimizer_D.step()\n","\n","        src_centroid.centroids.data = current_src_centroid.data\n","        trg_centroid.centroids.data = current_trg_centroid.data\n","\n","        if i_iter % disp_interval == 0  and i_iter != 0:\n","            disp_msg = 'iter[{:8d}/{:8d}], '.format(i_iter, end_iter)\n","            disp_msg += str(monitor)\n","            disp_msg += ', lambda={:.6f}'.format(adaptation_lambda)\n","            disp_msg += ', lr={:.6f}'.format(current_lr)\n","            print(disp_msg)\n","\n","            monitor.reset()\n","\n","        if i_iter % save_interval == 0 and i_iter != 0:\n","            print(\"Elapsed Time: {}\".format(datetime.datetime.now() - start_time))\n","            print(\"Start Evaluation at {:d}\".format(i_iter))\n","\n","            model.eval()\n","\n","            pred_vals = []\n","            y_vals = []\n","            x_val = None\n","            y_val = None\n","            pred_val = None\n","\n","            with torch.no_grad():\n","                for i, (x_val, y_val) in enumerate(target_val_loader):\n","                    y_vals.append(y_val.cpu())\n","                    x_val = x_val.cuda()\n","                    y_val = y_val.cuda()\n","\n","                    pred_val = model(x_val, trg_domain_id, with_ft=False)\n","                    pred_vals.append(pred_val.cpu())\n","\n","            pred_vals = torch.cat(pred_vals, 0)\n","            y_vals = torch.cat(y_vals, 0)\n","            total_val_accuracy = float(accuracy(pred_vals, y_vals, topk=(1,))[0])\n","\n","            val_accuracy_each_c = [(c_name, float(accuracy_of_c(pred_vals, y_vals,\n","                                                                class_idx=c, topk=(1,))[0]))\n","                                   for c, c_name in enumerate(range(num_classes))]\n","            print('\\nMNIST Accuracy of Each class')\n","            print(''.join([\"{:<25}: {:.2f}%\\n\".format(c_name, 100 * c_val_acc)\n","                            for c_name, c_val_acc in val_accuracy_each_c]))\n","\n","            mean_val_accuracy = float(\n","                torch.mean(torch.FloatTensor([c_val_acc for _, c_val_acc in val_accuracy_each_c])))\n","\n","            print('MNIST mean Accuracy: {:.2f}%'.format(100 * mean_val_accuracy))\n","            print(\"MNIST Accuracy: {:.2f}%\".format(total_val_accuracy * 100))\n","\n","            model.train()\n","\n","            val_accuracy = total_val_accuracy\n","\n","            del x_val, y_val, pred_val, pred_vals, y_vals\n","\n","            if val_accuracy > best_accuracy:\n","                #save best model\n","                best_accuracy = val_accuracy\n","                best_accuracy_each_c = val_accuracy_each_c\n","                best_mean_val_accuracy = mean_val_accuracy\n","                best_total_val_accuracy = total_val_accuracy\n","\n","                model = model.cuda()\n","                discriminator = discriminator.cuda()\n","                src_centroid = src_centroid.cuda()\n","                trg_centroid = trg_centroid.cuda()\n","\n","            print('\\nBest {MNIST} Accuracy of Each class')\n","            print(''.join([\"{:<25}: {:.2f}%\\n\".format(c_name, 100 * c_val_acc)\n","                           for c_name, c_val_acc in best_accuracy_each_c]))\n","            print('Best Accs: ' + ''.join([\"{:.2f}% \".format(100 * c_val_acc)\n","                                           for _, c_val_acc in best_accuracy_each_c]))\n","            print('Best mean Accuracy: {:.2f}%'.format(100 * best_mean_val_accuracy))\n","            print('Best Accuracy: {:.2f}%'.format(100 * best_total_val_accuracy))\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WsTT65SQkEl"},"outputs":[],"source":["def main():\n","    start_time = datetime.datetime.now()\n","\n","    # make save_dir\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","\n","    dataloaders = get_dataloaders()\n","\n","    ###################################################################################################################\n","    #                                               Model Loading                                                     #\n","    ###################################################################################################################\n","    model = DSBNLeNet(num_classes = num_classes,\n","                      num_domains = num_domains)\n","\n","    model.train(True)\n","    model = model.cuda()\n","    params = get_optimizer_params(model,\n","                                  lr = learning_rate,\n","                                  weight_decay=weight_decay,\n","                                  base_weight_factor=base_weight_factor)\n","\n","    discriminator = Discriminator(in_features=num_classes).cuda()\n","    D_params = get_optimizer_params(model,\n","                                    lr = learning_rate,\n","                                    weight_decay=weight_decay,\n","                                    base_weight_factor=None)\n","    ### For sm_loss\n","    src_centroid = Centroids(num_classes, num_classes).cuda()\n","    trg_centroid = Centroids(num_classes, num_classes).cuda()\n","    centroids = [src_centroid, trg_centroid]\n","\n","    ###################################################################################################################\n","    #                                               Train Configurations                                              #\n","    ###################################################################################################################\n","    ce_loss = nn.CrossEntropyLoss()\n","    bce_loss = nn.BCEWithLogitsLoss()\n","\n","    lr_scheduler = LRScheduler(learning_rate, end_iter, base_weight_factor=base_weight_factor)\n","\n","    optimizer = optim.Adam(params, betas=(0.9, 0.999))\n","    optimizer_D = optim.Adam(D_params, betas=(0.9, 0.999))\n","    optimizers = [optimizer, optimizer_D]\n","\n","    train(model, discriminator, centroids, dataloaders, optimizers, lr_scheduler, ce_loss, bce_loss, start_time)\n","    print('Total Time:  {}'.format((datetime.datetime.now() - start_time)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aERaZIj4QkEl"},"outputs":[],"source":["main()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1xE95d0ZjNpMqjKSCXLVWSkA75mejVBKW","timestamp":1692334341616},{"file_id":"1elQN0lqn5nFT00An5pCKeU7S3gfzhf5X","timestamp":1692120287988}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}