{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SoyjLZ0QKOvI"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NbihVOWKP7v"},"outputs":[],"source":["!pip3 install torch torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qww0dbgGJkzN"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import os\n","import numpy as np\n","import sklearn\n","import sklearn.datasets\n","from sklearn.utils import shuffle as util_shuffle\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation, PillowWriter\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E42HU540Ig-l"},"outputs":[],"source":["# Dataset iterator\n","def make_dataset(rng=None, size=200):\n","    if rng is None:\n","      rng = np.random.RandomState()\n","\n","    radial_std = 0.3\n","    tangential_std = 0.1\n","    num_classes = 5\n","    num_per_class = size // 5\n","    rate = 0.25\n","    rads = np.linspace(0, 2 * np.pi, num_classes, endpoint=False)\n","\n","    features = rng.randn(num_classes*num_per_class, 2) \\\n","        * np.array([radial_std, tangential_std])\n","    features[:, 0] += 1.\n","    labels = np.repeat(np.arange(num_classes), num_per_class)\n","\n","    angles = rads[labels] + rate * np.exp(features[:, 0])\n","    rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n","    rotations = np.reshape(rotations.T, (-1, 2, 2))\n","    condition = labels\n","    data = 2 * np.einsum(\"ti,tij->tj\", features, rotations)\n","    data, condition = util_shuffle(data, condition)\n","\n","    return data, condition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FhK7PhtJhaM"},"outputs":[],"source":["data, _ = make_dataset(size=10000)\n","dataset = torch.tensor(data).float()\n","\n","plt.clf()\n","plt.scatter(data[:, 0], data[:, 1], alpha=0.5, color='red', edgecolor='white', s=40)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dK2-Em3vK_u7"},"outputs":[],"source":["def beta_schedule(beta1, beta2, T, schedule='sigmoid'):\n","    if schedule == 'linear':\n","        betas = torch.linspace(beta1, beta2, T)\n","    elif schedule == \"quad\":\n","        betas = torch.linspace(beta1 ** 0.5, beta2 ** 0.5, T) ** 2\n","    elif schedule == \"sigmoid\":\n","        betas = torch.linspace(-6, 6, T)\n","        betas = torch.sigmoid(betas) * (beta2 - beta1) + beta1\n","    return betas\n","\n","def ddpm_schedules(beta1, beta2, T, schedule='sigmoid'):\n","    \"\"\"\n","    Returns pre-computed schedules for DDPM sampling, training process.\n","    \"\"\"\n","    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n","\n","    beta_t = beta_schedule(beta1, beta2, T, schedule)\n","    # beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n","    sqrt_beta_t = torch.sqrt(beta_t)\n","    alpha_t = 1 - beta_t\n","    log_alpha_t = torch.log(alpha_t)\n","    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n","\n","    sqrtab = torch.sqrt(alphabar_t)\n","    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n","\n","    sqrtmab = torch.sqrt(1 - alphabar_t)\n","    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n","\n","    return {\n","        \"beta_t\": beta_t,    # \\beta_t\n","        \"alpha_t\": alpha_t,  # \\alpha_t\n","        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n","        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n","        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n","        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n","        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n","        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n","    }\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yH610dwYLD2b"},"outputs":[],"source":["# ddpm scheduling check\n","\n","import matplotlib.pyplot as plt\n","\n","n_T = 100\n","\n","ddpm_scheduling_dict = ddpm_schedules(1e-5, 1e-2, n_T)\n","\n","beta = ddpm_scheduling_dict['beta_t']\n","alpha = ddpm_scheduling_dict['alpha_t']\n","alpha_bar = ddpm_scheduling_dict['alphabar_t']\n","\n","fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,6))\n","\n","axes[0].plot(np.arange(len(beta)), beta)\n","axes[0].set_xlabel('timesteps')\n","axes[0].set_ylabel('beta')\n","\n","axes[1].plot(np.arange(len(alpha)), alpha)\n","axes[1].set_xlabel('timesteps')\n","axes[1].set_ylabel('alpha')\n","\n","axes[2].plot(np.arange(len(alpha_bar)), alpha_bar)\n","axes[2].set_xlabel('timesteps')\n","axes[2].set_ylabel('alpha_bar')\n","\n","plt.show()\n","plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bsRVHxDPsvV"},"outputs":[],"source":["sample_batch = dataset\n","\n","forward_list = []\n","forward_list2 = []\n","\n","x = sample_batch\n","for t in range(n_T):\n","    x = # ToDo: Write the function of x by using the forward process distribution\n","    if t % (n_T//5) == 0:\n","        forward_list.append(x.detach().cpu())\n","\n","for t in range(n_T):\n","    if t % (n_T//5) == 0:\n","        x = # ToDo: Write the function of sample_batch by using the reparameterization trick\n","        forward_list2.append(x.detach().cpu())\n","\n","plt.clf()\n","fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(25, 4))\n","for i in range(5):\n","  axes[i].scatter(forward_list[i][:, 0].cpu(), forward_list[i][:, 1].cpu(), alpha=0.5, color='red', edgecolor='white', s=40)\n","plt.show()\n","plt.close()\n","\n","plt.clf()\n","fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(25, 4))\n","for i in range(5):\n","  axes[i].scatter(forward_list2[i][:, 0].cpu(), forward_list2[i][:, 1].cpu(), alpha=0.5, color='red', edgecolor='white', s=40)\n","plt.show()\n","plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qr8rS9aQJ0lz"},"outputs":[],"source":["class ConditionalLinear(nn.Module):\n","    def __init__(self, num_in, num_out, n_steps):\n","        super(ConditionalLinear, self).__init__()\n","        self.num_out = num_out\n","        self.lin = nn.Linear(num_in, num_out)\n","        self.embed = nn.Embedding(n_steps, num_out)\n","        self.embed.weight.data.uniform_()\n","\n","    def forward(self, x, t):\n","        out = self.lin(x)\n","        gamma = self.embed(t)\n","        out = gamma.view(-1, self.num_out) * out\n","        return out\n","\n","class TimeConditionalModel(nn.Module):\n","    def __init__(self, n_steps):\n","        super(TimeConditionalModel, self).__init__()\n","        self.lin1 = ConditionalLinear(2, 128, n_steps)\n","        self.lin2 = ConditionalLinear(128, 128, n_steps)\n","        self.lin3 = ConditionalLinear(128, 128, n_steps)\n","        self.lin4 = nn.Linear(128, 2)\n","\n","    def forward(self, x, t):\n","        x = F.softplus(self.lin1(x, t))\n","        x = F.softplus(self.lin2(x, t))\n","        x = F.softplus(self.lin3(x, t))\n","        return self.lin4(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATpEgzzyLMXU"},"outputs":[],"source":["class DDPM(nn.Module):\n","    def __init__(self, nn_model, betas, n_T, device):\n","        super(DDPM, self).__init__()\n","        self.nn_model = nn_model.to(device)\n","\n","        # register_buffer allows accessing dictionary produced by ddpm_schedules\n","        # e.g. can access self.sqrtab later\n","        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n","            self.register_buffer(k, v)\n","\n","        self.n_T = n_T\n","        self.device = device\n","        self.loss_mse = nn.MSELoss()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        this method is used in training, so samples t and noise randomly\n","        \"\"\"\n","\n","        _ts = torch.randint(0, self.n_T, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)\n","        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n","\n","        x_t = (\n","            self.sqrtab[_ts, None] * x\n","            + self.sqrtmab[_ts, None] * noise\n","        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n","        # We should predict the \"error term\" from this x_t. Loss is what we return.\n","\n","        # return MSE between added noise, and our predicted noise\n","        eps = self.nn_model(x_t, _ts)\n","        return self.loss_mse() # ToDo: Write the loss function to train the network\n","\n","    def sample(self, n_sample, size, device):\n","        # sampling the fake_data\n","        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n","        x_i_store = [] # keep track of generated steps in case want to plot something\n","        print()\n","        for i in range(self.n_T-1, -1, -1):\n","            t_is = torch.tensor([i]).to(device)\n","            t_is = t_is.repeat(n_sample,1)\n","\n","            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n","\n","            # split predictions and compute weighting\n","            eps = self.nn_model(x_i, t_is)\n","            x_i = (\n","                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n","                + self.sqrt_beta_t[i] * z\n","            )\n","            if i%20==0 or i==self.n_T or i<8:\n","                x_i_store.append(x_i.detach().cpu().numpy())\n","\n","        x_i_store = np.array(x_i_store)\n","        return x_i, x_i_store"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb_g5t6gLeRR"},"outputs":[],"source":["device='cuda:0'\n","n_epoch = 1000                             # total training epoch\n","batch_size = 256                          # number of data in each iteration\n","n_T = 200                                 # total timesteps of diffusion process\n","lrate = 1e-3                              # learning rate\n","save_dir = './toy_data_results/'\n","os.makedirs(save_dir, exist_ok=True)\n","\n","ddpm = DDPM(nn_model=TimeConditionalModel(n_steps=n_T), betas=(1e-5, 1e-2), n_T=n_T, device=device)\n","ddpm.to(device)\n","\n","optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuJR2DS9Paih"},"outputs":[],"source":["plt.ioff()\n","\n","for ep in range(n_epoch):\n","    print(f'epoch {ep}')\n","    ddpm.train()\n","\n","    permutation = torch.randperm(dataset.size()[0])\n","    pbar = tqdm(list(range(0, dataset.size()[0], batch_size)))\n","    loss_ema = None\n","    for i in pbar:\n","        indices = permutation[i:i+batch_size]\n","        x = dataset[indices].to(device)\n","        loss = ddpm(x)\n","        optim.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(ddpm.parameters(), 1.)\n","        if loss_ema is None:\n","            loss_ema = loss.item()\n","        else:\n","            loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n","        pbar.set_description(f\"loss: {loss_ema:.4f}\")\n","        optim.step()\n","\n","    # for eval, save an image of currently generated samples (top rows)\n","    # followed by real images (bottom rows)\n","    ddpm.eval()\n","    if ep%100==0 or ep == int(n_epoch-1):\n","        with torch.no_grad():\n","            x_gen, x_gen_store = ddpm.sample(10000, (2,), device)\n","            plt.clf()\n","            plt.figure(figsize=(16, 12))\n","            plt.scatter(x_gen[:, 0].cpu(), x_gen[:, 1].cpu(), alpha=0.5, color='red', edgecolor='white', s=40)\n","            plt.savefig(save_dir + f\"samples_ep{ep}.png\")\n","            print('visualize samples at ' + save_dir + f\"samples_ep{ep}.png\")\n","\n","            # create gif of images evolving over time, based on x_gen_store\n","            fig, axs = plt.subplots(nrows=1, ncols=1, sharex=True,sharey=True,figsize=(8,8))\n","            def animate_diff(i, x_gen_store):\n","                print(f'gif animating frame {i} of {x_gen_store.shape[0]}', end='\\r')\n","                plots = []\n","                axs.clear()\n","                plots.append(axs.scatter(x_gen_store[i, :, 0], x_gen_store[i, :, 1], alpha=0.5, color='red', edgecolor='white', s=40))\n","                return plots\n","            # fig, axs = plt.subplots(sharex=True,sharey=True,figsize=(8,8))\n","            ani = FuncAnimation(fig, animate_diff, fargs=[x_gen_store],  interval=200, blit=False, repeat=True, frames=x_gen_store.shape[0])\n","            ani.save(save_dir + f\"gif_ep{ep}.gif\", dpi=100, writer=PillowWriter(fps=5))\n","            print('saved image at ' + save_dir + f\"gif_ep{ep}.gif\")\n","            plt.close('all')\n","\n","    # optionally save model\n","    if ep == int(n_epoch-1):\n","        torch.save(ddpm.state_dict(), save_dir + f\"model_{ep}.pth\")\n","        print('saved model at ' + save_dir + f\"model_{ep}.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tARbDtJDttsP"},"outputs":[],"source":["num_samples = 10000\n","\n","x = torch.randn((num_samples, 2), device=device) # xT ~ N(0, I)\n","x_store = []\n","with torch.no_grad():\n","    for i in range(n_T-1, -1, -1):\n","        t_is = torch.tensor([i]).to(device)\n","        t_is = t_is.repeat(num_samples, 1)\n","\n","        eps = ddpm.nn_model(x, t_is)\n","        x = # ToDo: Write the function of x by using the backward process distribution\n","        if i % 20 == 0 or i == n_T or i < 8:\n","            x_store.append(x.detach().cpu().numpy())\n","\n","plt.clf()\n","plt.scatter(x[:, 0].cpu(), x[:, 1].cpu(), alpha=0.5, color='red', edgecolor='white', s=40)\n","plt.show()\n","plt.close()"]},{"cell_type":"code","source":[],"metadata":{"id":"gyD8U6tb4MnA"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyN6tzA/4W00wTcZC7MbG+6u"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}