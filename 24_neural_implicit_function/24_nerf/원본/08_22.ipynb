{"cells":[{"cell_type":"markdown","source":["# NeRF ì‹¤ìŠµìë£Œ\n","\n","8ì›” 22ì¼ AI Expert ê³¼ì •ì„ ìœ„í•´ ì œì‘ëœ colabì…ë‹ˆë‹¤.\n","ë³¸ ìë£Œì—ì„œëŠ” NeRFì˜ ê¸°ì´ˆ ë° í™œìš©ì— ëŒ€í•´ íƒêµ¬í•©ë‹ˆë‹¤. NeRFì— ê´€í•œ ì´ë¡ ì  ë°°ê²½ì€ ë‹¤ìŒ ë…¼ë¬¸ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”. [Implicit Neural Activations with Periodic Activation Functions](https://arxiv.org/pdf/2210.00379.pdf).\n","\n","ë³¸ ì‹¤ìŠµì€ ë‹¤ìŒì˜ ìˆœì„œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n","* TinyNeRF\n","* NeRFStudio\n","* ThreeStudio\n","\n","**ë³¸ ì½”ë© íŒŒì¼ì„ ë³¸ì¸ ê³„ì •ì˜ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥í•˜ì‹œë©´ ìˆ˜ì • í›„ ì €ì¥ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤: ë©”ë‰´ë°”ì˜ File --> Save a copy in Drive**\n","\n","**GPUë¥¼ í™œì„±í™”í•˜ì˜€ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”: ë©”ë‰´ë°”ì˜ Edit --> Notebook Setting --> T4 GPU ì„ íƒ**"],"metadata":{"id":"pKYHBlBi_msh"}},{"cell_type":"markdown","source":["# 1. TinyNeRF\n","\n","ë¹ ë¥¸ í•™ìŠµ ë° ì‹œê°í™”ë¥¼ ìœ„í•´ ì„±ëŠ¥ì„ ë‚®ì¶˜ NeRF ëª¨ë¸\n","* ê¸°ì¡´ NeRFì— ë¹„í•´ 20ë°°ê°€ëŸ‰ì˜ ì ì€ íŒŒë¼ë¯¸í„°\n","* 5D inputì´ view directionì„ í¬í•¨í•˜ì§€ ì•ŠìŒ\n","* Hierarchical Samplingì„ ì§„í–‰í•˜ì§€ ì•ŠìŒ"],"metadata":{"id":"wAgZJrG3CaBr"}},{"cell_type":"code","source":["import os,sys\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","if not os.path.exists('tiny_nerf_data.npz'):\n","    !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz"],"metadata":{"id":"KeYif4s5C3-n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692670440491,"user_tz":-540,"elapsed":10916,"user":{"displayName":"Seunguk Do","userId":"05447261647661672061"}},"outputId":"5f8bb572-da80-4431-e1e4-16ed867967db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-08-22 02:13:58--  http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\n","Resolving cseweb.ucsd.edu (cseweb.ucsd.edu)... 132.239.8.30\n","Connecting to cseweb.ucsd.edu (cseweb.ucsd.edu)|132.239.8.30|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cseweb.ucsd.edu//~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz [following]\n","--2023-08-22 02:13:59--  https://cseweb.ucsd.edu//~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\n","Connecting to cseweb.ucsd.edu (cseweb.ucsd.edu)|132.239.8.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12727482 (12M)\n","Saving to: â€˜tiny_nerf_data.npzâ€™\n","\n","tiny_nerf_data.npz  100%[===================>]  12.14M  17.4MB/s    in 0.7s    \n","\n","2023-08-22 02:14:00 (17.4 MB/s) - â€˜tiny_nerf_data.npzâ€™ saved [12727482/12727482]\n","\n"]}]},{"cell_type":"code","source":["#Search for GPU to run on\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#Load in data\n","rawData = np.load(\"tiny_nerf_data.npz\")\n","images = rawData[\"images\"]\n","poses = rawData[\"poses\"]\n","focal = rawData[\"focal\"]\n","H, W = images.shape[1:3]\n","H = int(H)\n","W = int(W)\n","print(images.shape, poses.shape, focal)\n","\n","testimg, testpose = images[99], poses[99]\n","plt.imshow(testimg)\n","plt.show()\n","images = torch.Tensor(images).to(device)\n","poses = torch.Tensor(poses).to(device)\n","testimg = torch.Tensor(testimg).to(device)\n","testpose = torch.Tensor(testpose).to(device)"],"metadata":{"id":"UMjbXlumD-v6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_rays(H, W, focal, pose):\n","  i, j = torch.meshgrid(\n","      torch.arange(W, dtype=torch.float32),\n","      torch.arange(H, dtype=torch.float32)\n","      )\n","  i = i.t()\n","  j = j.t()\n","  dirs = torch.stack(\n","      [(i-W*0.5)/focal,\n","       -(j-H*0.5)/focal,\n","       -torch.ones_like(i)], -1).to(device)\n","  rays_d = torch.sum(dirs[..., np.newaxis, :] * pose[:3, :3], -1)\n","  rays_o = pose[:3,-1].expand(rays_d.shape)\n","  return rays_o, rays_d"],"metadata":{"id":"DwElRchlEhvG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def positional_encoder(x, L_embed=6):\n","  rets = [x]\n","  for i in range(L_embed):\n","    for fn in [torch.sin, torch.cos]:\n","      rets.append(fn(2.**i *x))#(2^i)*x\n","  return torch.cat(rets, -1)\n","\n","def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n","  cumprod = torch.cumprod(tensor, -1)\n","  cumprod = torch.roll(cumprod, 1, -1)\n","  cumprod[..., 0] = 1.\n","  return cumprod\n","\n","def render(model, rays_o, rays_d, near, far, n_samples, rand=False):\n","  def batchify(fn, chunk=1024*32):\n","      return lambda inputs: torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n","\n","  z = torch.linspace(near, far, n_samples).to(device)\n","  if rand:\n","    mids = 0.5 * (z[..., 1:] + z[...,:-1])\n","    upper = torch.cat([mids, z[...,-1:]], -1)\n","    lower = torch.cat([z[...,:1], mids], -1)\n","    t_rand = torch.rand(z.shape).to(device)\n","    z = lower + (upper-lower)*t_rand\n","\n","  points = rays_o[..., None,:] + rays_d[..., None,:] * z[...,:,None]\n","\n","  flat_points = torch.reshape(points, [-1, points.shape[-1]])\n","  flat_points = positional_encoder(flat_points)\n","  raw = batchify(model)(flat_points)\n","  raw = torch.reshape(raw, list(points.shape[:-1]) + [4])\n","\n","  #Compute opacitices and color\n","  sigma = F.relu(raw[..., 3])\n","  rgb = torch.sigmoid(raw[..., :3])\n","\n","  #Volume Rendering\n","  one_e_10 = torch.tensor([1e10], dtype=rays_o.dtype).to(device)\n","  dists = torch.cat((z[..., 1:] - z[..., :-1],\n","                  one_e_10.expand(z[..., :1].shape)), dim=-1)\n","  alpha = 1. - torch.exp(-sigma * dists)\n","  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n","\n","  rgb_map = (weights[...,None]* rgb).sum(dim=-2)\n","  depth_map = (weights * z).sum(dim=-1)\n","  acc_map = weights.sum(dim=-1)\n","  return rgb_map, depth_map, acc_map\n"],"metadata":{"id":"PUN2AS7VFNkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#helper functions\n","mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.])).to(device)\n","\n","def train(model, optimizer, n_iters = 3001):\n","  #Track loss over time for graphing\n","  psnrs = []\n","  iternums = []\n","  plot_step = 500\n","  n_samples = 64\n","  for i in range(n_iters):\n","    #Choose random image and use it for training\n","    images_idx = np.random.randint(images.shape[0])\n","    target = images[images_idx]\n","    pose = poses[images_idx]\n","\n","    #Core optimizer loop\n","    rays_o, rays_d = get_rays(H, W, focal, pose)\n","    rgb, disp, acc = render(model, rays_o, rays_d, near=2., far=6., n_samples=n_samples, rand=True)\n","    optimizer.zero_grad()\n","    image_loss = torch.nn.functional.mse_loss(rgb, target)\n","    image_loss.backward()\n","    optimizer.step()\n","\n","    if i%plot_step==0:\n","      #Render shown image above as model begins to learn\n","      with torch.no_grad():\n","        rays_o, rays_d = get_rays(H, W, focal, testpose)\n","        rgb, depth, acc = render(model, rays_o, rays_d, near=2., far=6., n_samples=n_samples)\n","        loss = torch.nn.functional.mse_loss(rgb, testimg)\n","        psnr = mse2psnr(loss).cpu()\n","\n","        psnrs.append(psnr)\n","        iternums.append(i)\n","\n","        plt.figure(figsize=(10,5))\n","        plt.subplot(121)\n","        #copy from gpu memory to cpu\n","        picture = rgb.cpu()\n","        plt.imshow(picture)\n","        plt.title(f'Iterations: {i}')\n","        plt.subplot(122)\n","        plt.plot(iternums, psnrs)\n","        plt.title('PSNR')\n","        plt.show()"],"metadata":{"id":"gsBaOWyRFRc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VeryTinyNerfModel(torch.nn.Module):\n","  def __init__(self, filter_size=128, num_encoding_functions=6):\n","    super(VeryTinyNerfModel, self).__init__()\n","    # Input layer (default: 39 -> 128)\n","    self.layer1 = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n","    # Layer 2 (default: 128 -> 128)\n","    self.layer2 = torch.nn.Linear(filter_size, filter_size)\n","    # Layer 3 (default: 128 -> 4)\n","    self.layer3 = torch.nn.Linear(filter_size, 4)\n","    # Short hand for torch.nn.functional.relu\n","    self.relu = torch.nn.functional.relu\n","\n","  def forward(self, x):\n","    x = self.relu(self.layer1(x))\n","    x = self.relu(self.layer2(x))\n","    x = self.layer3(x)\n","    return x"],"metadata":{"id":"d1oqQwlXFU92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run all the actual code\n","nerf = VeryTinyNerfModel()\n","nerf = nn.DataParallel(nerf).to(device)\n","optimizer = torch.optim.Adam(nerf.parameters(), lr=5e-3, eps = 1e-7)\n","train(nerf, optimizer)"],"metadata":{"id":"SGypem0xFYSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","from ipywidgets import interactive, widgets\n","\n","\n","trans_t = lambda t : torch.tensor([\n","    [1,0,0,0],\n","    [0,1,0,0],\n","    [0,0,1,t],\n","    [0,0,0,1],\n","], dtype=torch.float32)\n","\n","rot_phi = lambda phi : torch.tensor([\n","    [1,0,0,0],\n","    [0,np.cos(phi),-np.sin(phi),0],\n","    [0,np.sin(phi), np.cos(phi),0],\n","    [0,0,0,1],\n","], dtype=torch.float32)\n","\n","rot_theta = lambda th : torch.tensor([\n","    [np.cos(th),0,-np.sin(th),0],\n","    [0,1,0,0],\n","    [np.sin(th),0, np.cos(th),0],\n","    [0,0,0,1],\n","], dtype=torch.float32)\n","\n","\n","def pose_spherical(theta, phi, radius):\n","    c2w = trans_t(radius)\n","    c2w = rot_phi(phi/180.*np.pi) @ c2w\n","    c2w = rot_theta(theta/180.*np.pi) @ c2w\n","    c2w = torch.tensor([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]], dtype=torch.float32) @ c2w\n","    return c2w\n","\n","\n","def f(**kwargs):\n","    c2w = pose_spherical(**kwargs).cuda()\n","    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n","    rgb, depth, acc = render(nerf, rays_o, rays_d, near=2., far=6., n_samples=64)\n","    img = np.clip(rgb.cpu().detach().numpy(),0,1)\n","\n","    plt.figure(2, figsize=(20,6))\n","    plt.imshow(img)\n","    plt.show()\n","\n","\n","sldr = lambda v, mi, ma: widgets.FloatSlider(\n","    value=v,\n","    min=mi,\n","    max=ma,\n","    step=.01,\n",")\n","\n","names = [\n","    ['theta', [100., 0., 360]],\n","    ['phi', [-30., -90, 0]],\n","    ['radius', [4., 3., 5.]],\n","]\n","\n","interactive_plot = interactive(f, **{s[0] : sldr(*s[1]) for s in names})\n","output = interactive_plot.children[-1]\n","output.layout.height = '350px'\n","interactive_plot"],"metadata":{"id":"7cFCaTulIC4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2. NeRFStudio\n","\n","ë‹¤ì–‘í•œ NeRF ëª¨ë¸ì„ ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì œì‘ëœ í”Œë«í¼.\n","\n","* ì—¬ëŸ¬ ì¢…ë¥˜ì˜ NeRF ëª¨ë¸ (dynamic nerf, editing nerf, 3d diffusion model, fast nerf)ì„ í¬í•¨\n","* ê°•ë ¥í•œ Visualizerë¥¼ ì§€ì›í•˜ì—¬, ì›í•˜ëŠ” ë·° ì´ë¯¸ì§€ë¥¼ ê°„í¸í•˜ê²Œ ë Œë”ë§ ê°€ëŠ¥\n","* Dataloader, ray sampler, encoder ë“±, nerfì˜ ê° ëª¨ë“ˆì„ ì†ì‰½ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆì–´ì„œ ìƒˆë¡œìš´ ëª¨ë¸ ê°œë°œ ìš©ì´"],"metadata":{"id":"SfkIsBFJFcxG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oyLHl8QfYwP"},"outputs":[],"source":["#@markdown Install Nerfstudio and Dependencies (~8 min)\n","%cd /content/\n","!pip install --upgrade pip\n","!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n","\n","# Installing TinyCuda\n","%cd /content/\n","!gdown \"https://drive.google.com/u/1/uc?id=1-7x7qQfB7bIw2zV4Lr6-yhvMpjXC84Q5&confirm=t\"\n","!pip install tinycudann-1.7-cp310-cp310-linux_x86_64.whl\n","\n","# Installing COLMAP\n","%cd /content/\n","!gdown \"https://drive.google.com/u/0/uc?id=15WngFRNar_b8CaPR5R-hvQ3eAnlyk_SL&confirm=t\"\n","!sudo apt-get install \\\n","    build-essential \\\n","    libboost-program-options-dev \\\n","    libboost-filesystem-dev \\\n","    libboost-graph-dev \\\n","    libboost-system-dev \\\n","    libboost-test-dev \\\n","    libeigen3-dev \\\n","    libflann-dev \\\n","    libfreeimage-dev \\\n","    libmetis-dev \\\n","    libgoogle-glog-dev \\\n","    libgflags-dev \\\n","    libsqlite3-dev \\\n","    libglew-dev \\\n","    qtbase5-dev \\\n","    libqt5opengl5-dev \\\n","    libcgal-dev \\\n","    libceres-dev\n","!unzip -o local.zip -d /usr/\n","!chmod +x /usr/local/bin/colmap\n","\n","# Install nerfstudio\n","%cd /content/\n","!pip install tensorboard<2.13\n","!pip install git+https://github.com/nerfstudio-project/nerfstudio.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msVLprI4gRA4"},"outputs":[],"source":["#@markdown <h1> Downloading and Processing Data</h1>\n","#@markdown <h3>Pick the preset scene or upload your own images/video</h3>\n","import os\n","import glob\n","from google.colab import files\n","from IPython.core.display import display, HTML\n","\n","scene = '\\uD83D\\uDE9C dozer' #@param ['ğŸ–¼ poster', 'ğŸšœ dozer', 'ğŸŒ„ desolation', 'ğŸ“¤ upload your images' , 'ğŸ¥ upload your own video', 'ğŸ”º upload Polycam data', 'ğŸ’½ upload your own Record3D data']\n","scene = ' '.join(scene.split(' ')[1:])\n","\n","if scene == \"upload Polycam data\":\n","    %cd /content/\n","    !mkdir -p /content/data/nerfstudio/custom_data\n","    %cd /content/data/nerfstudio/custom_data/\n","    uploaded = files.upload()\n","    dir = os.getcwd()\n","    if len(uploaded.keys()) > 1:\n","        print(\"ERROR, upload a single .zip file when processing Polycam data\")\n","    dataset_dir = [os.path.join(dir, f) for f in uploaded.keys()][0]\n","    !ns-process-data polycam --data $dataset_dir --output-dir /content/data/nerfstudio/custom_data/\n","    scene = \"custom_data\"\n","elif scene == 'upload your own Record3D data':\n","    display(HTML('<h3>Zip your Record3D folder, and upload.</h3>'))\n","    display(HTML('<h3>More information on Record3D can be found <a href=\"https://docs.nerf.studio/en/latest/quickstart/custom_dataset.html#record3d-capture\" target=\"_blank\">here</a>.</h3>'))\n","    %cd /content/\n","    !mkdir -p /content/data/nerfstudio/custom_data\n","    %cd /content/data/nerfstudio/custom_data/\n","    uploaded = files.upload()\n","    dir = os.getcwd()\n","    preupload_datasets = [os.path.join(dir, f) for f in uploaded.keys()]\n","    record_3d_zipfile = preupload_datasets[0]\n","    !unzip $record_3d_zipfile -d /content/data/nerfstudio/custom_data\n","    custom_data_directory = glob.glob('/content/data/nerfstudio/custom_data/*')[0]\n","    !ns-process-data record3d --data $custom_data_directory --output-dir /content/data/nerfstudio/custom_data/\n","    scene = \"custom_data\"\n","elif scene in ['upload your images', 'upload your own video']:\n","    display(HTML('<h3>Select your custom data</h3>'))\n","    display(HTML('<p/>You can select multiple images by pressing ctrl, cmd or shift and click.<p>'))\n","    display(HTML('<p/>Note: This may take time, especially on higher resolution inputs, so we recommend to download dataset after creation.<p>'))\n","    !mkdir -p /content/data/nerfstudio/custom_data\n","    if scene == 'upload your images':\n","        !mkdir -p /content/data/nerfstudio/custom_data/raw_images\n","        %cd /content/data/nerfstudio/custom_data/raw_images\n","        uploaded = files.upload()\n","        dir = os.getcwd()\n","    else:\n","        %cd /content/data/nerfstudio/custom_data/\n","        uploaded = files.upload()\n","        dir = os.getcwd()\n","    preupload_datasets = [os.path.join(dir, f) for f in uploaded.keys()]\n","    del uploaded\n","    %cd /content/\n","\n","    if scene == 'upload your images':\n","        !ns-process-data images --data /content/data/nerfstudio/custom_data/raw_images --output-dir /content/data/nerfstudio/custom_data/\n","    else:\n","        video_path = preupload_datasets[0]\n","        !ns-process-data video --data $video_path --output-dir /content/data/nerfstudio/custom_data/\n","\n","    scene = \"custom_data\"\n","else:\n","    %cd /content/\n","    !ns-download-data nerfstudio --capture-name=$scene\n","\n","print(\"Data Processing Succeeded!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoKDxqEcjmfC"},"outputs":[],"source":["#@markdown <h1>Set up and Start Viewer</h1>\n","\n","%cd /content\n","\n","# Install localtunnel\n","# We are using localtunnel https://github.com/localtunnel/localtunnel but ngrok could also be used\n","!npm install -g localtunnel\n","\n","# Tunnel port 7007, the default for\n","!rm url.txt 2> /dev/null\n","get_ipython().system_raw('lt --port 7007 >> url.txt 2>&1 &')\n","\n","import time\n","time.sleep(3) # the previous command needs time to write to url.txt\n","\n","\n","with open('url.txt') as f:\n","  lines = f.readlines()\n","websocket_url = lines[0].split(\": \")[1].strip().replace(\"https\", \"wss\")\n","# from nerfstudio.utils.io import load_from_json\n","# from pathlib import Path\n","# json_filename = \"nerfstudio/nerfstudio/viewer/app/package.json\"\n","# version = load_from_json(Path(json_filename))[\"version\"]\n","url = f\"https://viewer.nerf.studio/?websocket_url={websocket_url}\"\n","print(url)\n","print(\"You may need to click Refresh Page after you start training!\")\n","from IPython import display\n","display.IFrame(src=url, height=800, width=\"100%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_N8_cLfjoXD"},"outputs":[],"source":["#@markdown <h1>Start Training</h1>\n","\n","%cd /content\n","if os.path.exists(f\"data/nerfstudio/{scene}/transforms.json\"):\n","    !ns-train nerfacto --viewer.websocket-port 7007 nerfstudio-data --data data/nerfstudio/$scene --downscale-factor 4\n","else:\n","    from IPython.core.display import display, HTML\n","    display(HTML('<h3 style=\"color:red\">Error: Data processing did not complete</h3>'))\n","    display(HTML('<h3>Please re-run `Downloading and Processing Data`, or view the FAQ for more info.</h3>'))"]},{"cell_type":"code","source":["!ns-render camera-path --load-config outputs/unnamed/nerfacto/2023-08-21_125010/config.yml --camera-path-filename outputs/unnamed/nerfacto/2023-08-21_125010/camera_paths/2023-08-21_125010.json --output-path renders/2023-08-21_125010/2023-08-21_125010.mp4"],"metadata":{"id":"a4cSj-NfSbUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"WGt8ukG6Htg3","outputId":"fa946890-c7d8-4e46-a54e-7231bc5a2059"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2;36m[19:48:48]\u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split train.                                          \u001b]8;id=527413;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=243595;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n","\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split test.                                           \u001b]8;id=109270;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=464675;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n","\u001b[2KLoading data batch \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\u001b[2KLoading data batch \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h/usr/local/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading latest checkpoint from load_dir\n","âœ… Done loading checkpoint from \n","outputs/data-nerfstudio-poster/nerfacto/\u001b[1;36m2022\u001b[0m-\u001b[1;36m10\u001b[0m-29_192844/nerfstudio_models/step-\u001b[1;36m000014000.\u001b[0mckpt\n","\u001b[1;32mCreating trajectory video\u001b[0m\n","\u001b[2KğŸ¥ Rendering ğŸ¥ \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[31m0.14 fps\u001b[0m \u001b[33m11:47\u001b[0m\n","\u001b[2K\u001b[32m(  â—   )\u001b[0m \u001b[33mSaving video\u001b[0m\n","\u001b[1A\u001b[2K\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0m\u001b[32m ğŸ‰ ğŸ‰ ğŸ‰ Success ğŸ‰ ğŸ‰ ğŸ‰\u001b[0m\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n","                                           \u001b[32mSaved video to renders/output.mp4\u001b[0m                                            \n","\u001b[0m"]}],"source":["#@title # Render Video { vertical-output: true }\n","#@markdown <h3>Export the camera path from within the viewer, then run this cell.</h3>\n","#@markdown <h5>The rendered video should be at renders/output.mp4!</h5>\n","\n","\n","base_dir = \"/content/outputs/unnamed/nerfacto/\"\n","training_run_dir = base_dir + os.listdir(base_dir)[0]\n","\n","from IPython.core.display import display, HTML\n","display(HTML('<h3>Upload the camera path JSON.</h3>'))\n","%cd $training_run_dir\n","uploaded = files.upload()\n","uploaded_camera_path_filename = list(uploaded.keys())[0]\n","\n","config_filename = training_run_dir + \"/config.yml\"\n","camera_path_filename = training_run_dir + \"/\" + uploaded_camera_path_filename\n","camera_path_filename = camera_path_filename.replace(\" \", \"\\\\ \").replace(\"(\", \"\\\\(\").replace(\")\", \"\\\\)\")\n","\n","%cd /content/\n","!ns-render camera-path --load-config $config_filename --camera-path-filename $camera_path_filename --output-path renders/output.mp4"]},{"cell_type":"code","source":["#@title # ì¤‘ì§€ëœ training ì¬ê°œ\n","base_dir = \"/content/outputs/unnamed/nerfacto/\"\n","training_run_dir = base_dir + os.listdir(base_dir)[0] + '/nerfstudio_models'\n","\n","%cd /content/\n","!ns-train nerfacto --load-dir {training_run_dir} --viewer.websocket-port 7007 nerfstudio-data --data data/nerfstudio/$scene --downscale-factor 4"],"metadata":{"id":"DbfAa7PsTKYf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### NeRFStudioë¥¼ ì´ìš©í•œ ë‹¤ì–‘í•œ ëª¨ë¸ì˜ í™œìš©\n","* nerfactoëŠ” ì—¬ëŸ¬ ë…¼ë¬¸ì—ì„œ componentë¥¼ ì¡°í•©í•˜ì—¬ ë§Œë“  pipelineì„\n","* nerfstudioëŠ” nerfacto ì´ì™¸ì— ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì„ ì§€ì›\n","  - Instant-NGP\n","  - [Instruct-NeRF2NeRF](https://docs.nerf.studio/en/latest/nerfology/methods/in2n.html)\n","  - K-Planes\n","  - [LERF](https://docs.nerf.studio/en/latest/nerfology/methods/lerf.html)\n","  - Mip-NeRF\n","  - NeRF\n","  - Nerfacto\n","  - Nerfbusters\n","  - NeRFPlayer\n","  - Tetra-NeRF\n","  - TensoRF\n","  - [Generfacto](https://docs.nerf.studio/en/latest/nerfology/methods/generfacto.html)"],"metadata":{"id":"LELvyfR8ic0k"}},{"cell_type":"markdown","source":["#### Training TensoRF"],"metadata":{"id":"wNIPUivp6Uwc"}},{"cell_type":"code","source":["!ns-train tensorf --viewer.websocket-port 7007 nerfstudio-data --data data/nerfstudio/$scene --downscale-factor 4"],"metadata":{"id":"Q6B1zNaz3U-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. ThreeStudio"],"metadata":{"id":"mG-xOZ77JjLQ"}},{"cell_type":"markdown","source":["#### Clone threestudio repo"],"metadata":{"id":"5B_rws4sJwUo"}},{"cell_type":"code","source":["!git clone https://github.com/threestudio-project/threestudio.git\n","%cd threestudio"],"metadata":{"id":"v1Zc1qyMJkyQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Install Dependencies"],"metadata":{"id":"ZPuJvKnnJ2XG"}},{"cell_type":"code","source":["!pip install ninja\n","!pip install lightning==2.0.0 omegaconf==2.3.0 jaxtyping typeguard diffusers transformers accelerate opencv-python tensorboard matplotlib imageio imageio[ffmpeg] trimesh bitsandbytes sentencepiece safetensors huggingface_hub libigl xatlas networkx pysdf PyMCubes wandb torchmetrics controlnet_aux\n","!pip install einops kornia taming-transformers-rom1504 git+https://github.com/openai/CLIP.git # zero123\n","!pip install open3d plotly # mesh visualization\n","!pip install git+https://github.com/ashawkey/envlight.git\n","!pip install git+https://github.com/KAIR-BAIR/nerfacc.git@v0.5.2\n","!pip install git+https://github.com/NVlabs/nvdiffrast.git\n","!pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch"],"metadata":{"id":"Civ70_R8JoxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Login to HuggingFace"],"metadata":{"id":"dsIt3cWqJ_Eb"}},{"cell_type":"code","source":["from huggingface_hub import interpreter_login\n","\n","interpreter_login()"],"metadata":{"id":"2TCTVlGvJuGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Generate 3D Model"],"metadata":{"id":"Ofd1Ss9xKF1E"}},{"cell_type":"code","source":["prompt = \"a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes\"\n","!python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=\"$prompt\" trainer.max_steps=1000 system.prompt_processor.spawn=false"],"metadata":{"id":"KA1NN4VhKHA9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Display the rendered Video"],"metadata":{"id":"Qnetbxd0KMph"}},{"cell_type":"code","source":["from IPython.display import HTML\n","from base64 import b64encode\n","def display_video(video_path):\n","  mp4 = open(video_path,'rb').read()\n","  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","  return HTML(\"\"\"\n","  <video width=1000 controls>\n","    <source src=\"%s\" type=\"video/mp4\">\n","  </video>\n","  \"\"\" % data_url)"],"metadata":{"id":"-TJEnCcTKJ4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# you will see the path to the saving directory at the end of the training logs\n","# replace save_dir below with that path\n","save_dir = 'path/to/save/dir'\n","\n","import os\n","import glob\n","video_path = glob.glob(os.path.join(save_dir, \"*-test.mp4\"))[0]\n","display_video(video_path)"],"metadata":{"id":"YhEbSTB4KRWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Extract the object mesh.\n","\n","Here we use an empirical threshold value. You can also first try system.geometry.isosurface_threshold=auto and visualize it. Then you can manually adjust the threshold according to the automatically determined value shown in the logs. Increase it if there are too many floaters and decrease it if the geometry is incomplete.\n","\n","The extraction process takes around 2 mins on T4."],"metadata":{"id":"FZAa6uk1KVM6"}},{"cell_type":"code","source":["!python launch.py --config $save_dir/../configs/parsed.yaml --export --gpu 0 resume=$save_dir/../ckpts/last.ckpt system.exporter_type=mesh-exporter system.exporter.context_type=cuda system.geometry.isosurface_threshold=15.0"],"metadata":{"id":"9YipGdbgKWp5"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Y9n98qQYSMOF_J5CQcxRZ33RQs0rziTG","timestamp":1692678659732},{"file_id":"https://github.com/nerfstudio-project/nerfstudio/blob/main/colab/demo.ipynb","timestamp":1692614006517}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.13"},"vscode":{"interpreter":{"hash":"c59f626636933ef1dc834fb3684b382f705301c5306cf8436d2da634c2289783"}}},"nbformat":4,"nbformat_minor":0}