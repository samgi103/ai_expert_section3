{"cells":[{"cell_type":"markdown","source":["# NeRF 실습자료\n","\n","8월 22일 AI Expert 과정을 위해 제작된 colab입니다.\n","본 자료에서는 NeRF의 기초 및 활용에 대해 탐구합니다. NeRF에 관한 이론적 배경은 다음 논문을 참고해주세요. [Implicit Neural Activations with Periodic Activation Functions](https://arxiv.org/pdf/2210.00379.pdf).\n","\n","본 실습은 다음의 순서로 구성되어 있습니다.\n","* TinyNeRF\n","* NeRFStudio\n","* ThreeStudio\n","\n","**본 코랩 파일을 본인 계정의 구글 드라이브에 저장하시면 수정 후 저장이 가능합니다: 메뉴바의 File --> Save a copy in Drive**\n","\n","**GPU를 활성화하였는지 확인해주세요: 메뉴바의 Edit --> Notebook Setting --> T4 GPU 선택**"],"metadata":{"id":"pKYHBlBi_msh"}},{"cell_type":"markdown","source":["# 1. TinyNeRF\n","\n","빠른 학습 및 시각화를 위해 성능을 낮춘 NeRF 모델\n","* 기존 NeRF에 비해 20배가량의 적은 파라미터\n","* 5D input이 view direction을 포함하지 않음\n","* Hierarchical Sampling을 진행하지 않음"],"metadata":{"id":"wAgZJrG3CaBr"}},{"cell_type":"code","source":["import os,sys\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","if not os.path.exists('tiny_nerf_data.npz'):\n","    !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz"],"metadata":{"id":"KeYif4s5C3-n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692670440491,"user_tz":-540,"elapsed":10916,"user":{"displayName":"Seunguk Do","userId":"05447261647661672061"}},"outputId":"5f8bb572-da80-4431-e1e4-16ed867967db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-08-22 02:13:58--  http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\n","Resolving cseweb.ucsd.edu (cseweb.ucsd.edu)... 132.239.8.30\n","Connecting to cseweb.ucsd.edu (cseweb.ucsd.edu)|132.239.8.30|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cseweb.ucsd.edu//~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz [following]\n","--2023-08-22 02:13:59--  https://cseweb.ucsd.edu//~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\n","Connecting to cseweb.ucsd.edu (cseweb.ucsd.edu)|132.239.8.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12727482 (12M)\n","Saving to: ‘tiny_nerf_data.npz’\n","\n","tiny_nerf_data.npz  100%[===================>]  12.14M  17.4MB/s    in 0.7s    \n","\n","2023-08-22 02:14:00 (17.4 MB/s) - ‘tiny_nerf_data.npz’ saved [12727482/12727482]\n","\n"]}]},{"cell_type":"code","source":["#Search for GPU to run on\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#Load in data\n","rawData = np.load(\"tiny_nerf_data.npz\")\n","images = rawData[\"images\"]\n","poses = rawData[\"poses\"]\n","focal = rawData[\"focal\"]\n","H, W = images.shape[1:3]\n","H = int(H)\n","W = int(W)\n","print(images.shape, poses.shape, focal)\n","\n","testimg, testpose = images[99], poses[99]\n","plt.imshow(testimg)\n","plt.show()\n","images = torch.Tensor(images).to(device)\n","poses = torch.Tensor(poses).to(device)\n","testimg = torch.Tensor(testimg).to(device)\n","testpose = torch.Tensor(testpose).to(device)"],"metadata":{"id":"UMjbXlumD-v6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_rays(H, W, focal, pose):\n","  i, j = torch.meshgrid(\n","      torch.arange(W, dtype=torch.float32),\n","      torch.arange(H, dtype=torch.float32)\n","      )\n","  i = i.t()\n","  j = j.t()\n","  dirs = torch.stack(\n","      [(i-W*0.5)/focal,\n","       -(j-H*0.5)/focal,\n","       -torch.ones_like(i)], -1).to(device)\n","  rays_d = torch.sum(dirs[..., np.newaxis, :] * pose[:3, :3], -1)\n","  rays_o = pose[:3,-1].expand(rays_d.shape)\n","  return rays_o, rays_d"],"metadata":{"id":"DwElRchlEhvG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def positional_encoder(x, L_embed=6):\n","  rets = [x]\n","  for i in range(L_embed):\n","    for fn in [torch.sin, torch.cos]:\n","      rets.append(fn(2.**i *x))#(2^i)*x\n","  return torch.cat(rets, -1)\n","\n","def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n","  cumprod = torch.cumprod(tensor, -1)\n","  cumprod = torch.roll(cumprod, 1, -1)\n","  cumprod[..., 0] = 1.\n","  return cumprod\n","\n","def render(model, rays_o, rays_d, near, far, n_samples, rand=False):\n","  def batchify(fn, chunk=1024*32):\n","      return lambda inputs: torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n","\n","  z = torch.linspace(near, far, n_samples).to(device)\n","  if rand:\n","    mids = 0.5 * (z[..., 1:] + z[...,:-1])\n","    upper = torch.cat([mids, z[...,-1:]], -1)\n","    lower = torch.cat([z[...,:1], mids], -1)\n","    t_rand = torch.rand(z.shape).to(device)\n","    z = lower + (upper-lower)*t_rand\n","\n","  points = rays_o[..., None,:] + rays_d[..., None,:] * z[...,:,None]\n","\n","  flat_points = torch.reshape(points, [-1, points.shape[-1]])\n","  flat_points = positional_encoder(flat_points)\n","  raw = batchify(model)(flat_points)\n","  raw = torch.reshape(raw, list(points.shape[:-1]) + [4])\n","\n","  #Compute opacitices and color\n","  sigma = F.relu(raw[..., 3])\n","  rgb = torch.sigmoid(raw[..., :3])\n","\n","  #Volume Rendering\n","  one_e_10 = torch.tensor([1e10], dtype=rays_o.dtype).to(device)\n","  dists = torch.cat((z[..., 1:] - z[..., :-1],\n","                  one_e_10.expand(z[..., :1].shape)), dim=-1)\n","  alpha = 1. - torch.exp(-sigma * dists)\n","  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n","\n","  rgb_map = (weights[...,None]* rgb).sum(dim=-2)\n","  depth_map = (weights * z).sum(dim=-1)\n","  acc_map = weights.sum(dim=-1)\n","  return rgb_map, depth_map, acc_map\n"],"metadata":{"id":"PUN2AS7VFNkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#helper functions\n","mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.])).to(device)\n","\n","def train(model, optimizer, n_iters = 3001):\n","  #Track loss over time for graphing\n","  psnrs = []\n","  iternums = []\n","  plot_step = 500\n","  n_samples = 64\n","  for i in range(n_iters):\n","    #Choose random image and use it for training\n","    images_idx = np.random.randint(images.shape[0])\n","    target = images[images_idx]\n","    pose = poses[images_idx]\n","\n","    #Core optimizer loop\n","    rays_o, rays_d = get_rays(H, W, focal, pose)\n","    rgb, disp, acc = render(model, rays_o, rays_d, near=2., far=6., n_samples=n_samples, rand=True)\n","    optimizer.zero_grad()\n","    image_loss = torch.nn.functional.mse_loss(rgb, target)\n","    image_loss.backward()\n","    optimizer.step()\n","\n","    if i%plot_step==0:\n","      #Render shown image above as model begins to learn\n","      with torch.no_grad():\n","        rays_o, rays_d = get_rays(H, W, focal, testpose)\n","        rgb, depth, acc = render(model, rays_o, rays_d, near=2., far=6., n_samples=n_samples)\n","        loss = torch.nn.functional.mse_loss(rgb, testimg)\n","        psnr = mse2psnr(loss).cpu()\n","\n","        psnrs.append(psnr)\n","        iternums.append(i)\n","\n","        plt.figure(figsize=(10,5))\n","        plt.subplot(121)\n","        #copy from gpu memory to cpu\n","        picture = rgb.cpu()\n","        plt.imshow(picture)\n","        plt.title(f'Iterations: {i}')\n","        plt.subplot(122)\n","        plt.plot(iternums, psnrs)\n","        plt.title('PSNR')\n","        plt.show()"],"metadata":{"id":"gsBaOWyRFRc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VeryTinyNerfModel(torch.nn.Module):\n","  def __init__(self, filter_size=128, num_encoding_functions=6):\n","    super(VeryTinyNerfModel, self).__init__()\n","    # Input layer (default: 39 -> 128)\n","    self.layer1 = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n","    # Layer 2 (default: 128 -> 128)\n","    self.layer2 = torch.nn.Linear(filter_size, filter_size)\n","    # Layer 3 (default: 128 -> 4)\n","    self.layer3 = torch.nn.Linear(filter_size, 4)\n","    # Short hand for torch.nn.functional.relu\n","    self.relu = torch.nn.functional.relu\n","\n","  def forward(self, x):\n","    x = self.relu(self.layer1(x))\n","    x = self.relu(self.layer2(x))\n","    x = self.layer3(x)\n","    return x"],"metadata":{"id":"d1oqQwlXFU92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run all the actual code\n","nerf = VeryTinyNerfModel()\n","nerf = nn.DataParallel(nerf).to(device)\n","optimizer = torch.optim.Adam(nerf.parameters(), lr=5e-3, eps = 1e-7)\n","train(nerf, optimizer)"],"metadata":{"id":"SGypem0xFYSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","from ipywidgets import interactive, widgets\n","\n","\n","trans_t = lambda t : torch.tensor([\n","    [1,0,0,0],\n","    [0,1,0,0],\n","    [0,0,1,t],\n","    [0,0,0,1],\n","], dtype=torch.float32)\n","\n","rot_phi = lambda phi : torch.tensor([\n","    [1,0,0,0],\n","    [0,np.cos(phi),-np.sin(phi),0],\n","    [0,np.sin(phi), np.cos(phi),0],\n","    [0,0,0,1],\n","], dtype=torch.float32)\n","\n","rot_theta = lambda th : torch.tensor([\n","    [np.cos(th),0,-np.sin(th),0],\n","    [0,1,0,0],\n","    [np.sin(th),0, np.cos(th),0],\n","    [0,0,0,1],\n","], dtype=torch.float32)\n","\n","\n","def pose_spherical(theta, phi, radius):\n","    c2w = trans_t(radius)\n","    c2w = rot_phi(phi/180.*np.pi) @ c2w\n","    c2w = rot_theta(theta/180.*np.pi) @ c2w\n","    c2w = torch.tensor([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]], dtype=torch.float32) @ c2w\n","    return c2w\n","\n","\n","def f(**kwargs):\n","    c2w = pose_spherical(**kwargs).cuda()\n","    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n","    rgb, depth, acc = render(nerf, rays_o, rays_d, near=2., far=6., n_samples=64)\n","    img = np.clip(rgb.cpu().detach().numpy(),0,1)\n","\n","    plt.figure(2, figsize=(20,6))\n","    plt.imshow(img)\n","    plt.show()\n","\n","\n","sldr = lambda v, mi, ma: widgets.FloatSlider(\n","    value=v,\n","    min=mi,\n","    max=ma,\n","    step=.01,\n",")\n","\n","names = [\n","    ['theta', [100., 0., 360]],\n","    ['phi', [-30., -90, 0]],\n","    ['radius', [4., 3., 5.]],\n","]\n","\n","interactive_plot = interactive(f, **{s[0] : sldr(*s[1]) for s in names})\n","output = interactive_plot.children[-1]\n","output.layout.height = '350px'\n","interactive_plot"],"metadata":{"id":"7cFCaTulIC4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2. NeRFStudio\n","\n","다양한 NeRF 모델을 쉽게 활용할 수 있도록 제작된 플랫폼.\n","\n","* 여러 종류의 NeRF 모델 (dynamic nerf, editing nerf, 3d diffusion model, fast nerf)을 포함\n","* 강력한 Visualizer를 지원하여, 원하는 뷰 이미지를 간편하게 렌더링 가능\n","* Dataloader, ray sampler, encoder 등, nerf의 각 모듈을 손쉽게 수정할 수 있어서 새로운 모델 개발 용이"],"metadata":{"id":"SfkIsBFJFcxG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oyLHl8QfYwP"},"outputs":[],"source":["#@markdown Install Nerfstudio and Dependencies (~8 min)\n","%cd /content/\n","!pip install --upgrade pip\n","!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n","\n","# Installing TinyCuda\n","%cd /content/\n","!gdown \"https://drive.google.com/u/1/uc?id=1-7x7qQfB7bIw2zV4Lr6-yhvMpjXC84Q5&confirm=t\"\n","!pip install tinycudann-1.7-cp310-cp310-linux_x86_64.whl\n","\n","# Installing COLMAP\n","%cd /content/\n","!gdown \"https://drive.google.com/u/0/uc?id=15WngFRNar_b8CaPR5R-hvQ3eAnlyk_SL&confirm=t\"\n","!sudo apt-get install \\\n","    build-essential \\\n","    libboost-program-options-dev \\\n","    libboost-filesystem-dev \\\n","    libboost-graph-dev \\\n","    libboost-system-dev \\\n","    libboost-test-dev \\\n","    libeigen3-dev \\\n","    libflann-dev \\\n","    libfreeimage-dev \\\n","    libmetis-dev \\\n","    libgoogle-glog-dev \\\n","    libgflags-dev \\\n","    libsqlite3-dev \\\n","    libglew-dev \\\n","    qtbase5-dev \\\n","    libqt5opengl5-dev \\\n","    libcgal-dev \\\n","    libceres-dev\n","!unzip -o local.zip -d /usr/\n","!chmod +x /usr/local/bin/colmap\n","\n","# Install nerfstudio\n","%cd /content/\n","!pip install tensorboard<2.13\n","!pip install git+https://github.com/nerfstudio-project/nerfstudio.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msVLprI4gRA4"},"outputs":[],"source":["#@markdown <h1> Downloading and Processing Data</h1>\n","#@markdown <h3>Pick the preset scene or upload your own images/video</h3>\n","import os\n","import glob\n","from google.colab import files\n","from IPython.core.display import display, HTML\n","\n","scene = '\\uD83D\\uDE9C dozer' #@param ['🖼 poster', '🚜 dozer', '🌄 desolation', '📤 upload your images' , '🎥 upload your own video', '🔺 upload Polycam data', '💽 upload your own Record3D data']\n","scene = ' '.join(scene.split(' ')[1:])\n","\n","if scene == \"upload Polycam data\":\n","    %cd /content/\n","    !mkdir -p /content/data/nerfstudio/custom_data\n","    %cd /content/data/nerfstudio/custom_data/\n","    uploaded = files.upload()\n","    dir = os.getcwd()\n","    if len(uploaded.keys()) > 1:\n","        print(\"ERROR, upload a single .zip file when processing Polycam data\")\n","    dataset_dir = [os.path.join(dir, f) for f in uploaded.keys()][0]\n","    !ns-process-data polycam --data $dataset_dir --output-dir /content/data/nerfstudio/custom_data/\n","    scene = \"custom_data\"\n","elif scene == 'upload your own Record3D data':\n","    display(HTML('<h3>Zip your Record3D folder, and upload.</h3>'))\n","    display(HTML('<h3>More information on Record3D can be found <a href=\"https://docs.nerf.studio/en/latest/quickstart/custom_dataset.html#record3d-capture\" target=\"_blank\">here</a>.</h3>'))\n","    %cd /content/\n","    !mkdir -p /content/data/nerfstudio/custom_data\n","    %cd /content/data/nerfstudio/custom_data/\n","    uploaded = files.upload()\n","    dir = os.getcwd()\n","    preupload_datasets = [os.path.join(dir, f) for f in uploaded.keys()]\n","    record_3d_zipfile = preupload_datasets[0]\n","    !unzip $record_3d_zipfile -d /content/data/nerfstudio/custom_data\n","    custom_data_directory = glob.glob('/content/data/nerfstudio/custom_data/*')[0]\n","    !ns-process-data record3d --data $custom_data_directory --output-dir /content/data/nerfstudio/custom_data/\n","    scene = \"custom_data\"\n","elif scene in ['upload your images', 'upload your own video']:\n","    display(HTML('<h3>Select your custom data</h3>'))\n","    display(HTML('<p/>You can select multiple images by pressing ctrl, cmd or shift and click.<p>'))\n","    display(HTML('<p/>Note: This may take time, especially on higher resolution inputs, so we recommend to download dataset after creation.<p>'))\n","    !mkdir -p /content/data/nerfstudio/custom_data\n","    if scene == 'upload your images':\n","        !mkdir -p /content/data/nerfstudio/custom_data/raw_images\n","        %cd /content/data/nerfstudio/custom_data/raw_images\n","        uploaded = files.upload()\n","        dir = os.getcwd()\n","    else:\n","        %cd /content/data/nerfstudio/custom_data/\n","        uploaded = files.upload()\n","        dir = os.getcwd()\n","    preupload_datasets = [os.path.join(dir, f) for f in uploaded.keys()]\n","    del uploaded\n","    %cd /content/\n","\n","    if scene == 'upload your images':\n","        !ns-process-data images --data /content/data/nerfstudio/custom_data/raw_images --output-dir /content/data/nerfstudio/custom_data/\n","    else:\n","        video_path = preupload_datasets[0]\n","        !ns-process-data video --data $video_path --output-dir /content/data/nerfstudio/custom_data/\n","\n","    scene = \"custom_data\"\n","else:\n","    %cd /content/\n","    !ns-download-data nerfstudio --capture-name=$scene\n","\n","print(\"Data Processing Succeeded!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoKDxqEcjmfC"},"outputs":[],"source":["#@markdown <h1>Set up and Start Viewer</h1>\n","\n","%cd /content\n","\n","# Install localtunnel\n","# We are using localtunnel https://github.com/localtunnel/localtunnel but ngrok could also be used\n","!npm install -g localtunnel\n","\n","# Tunnel port 7007, the default for\n","!rm url.txt 2> /dev/null\n","get_ipython().system_raw('lt --port 7007 >> url.txt 2>&1 &')\n","\n","import time\n","time.sleep(3) # the previous command needs time to write to url.txt\n","\n","\n","with open('url.txt') as f:\n","  lines = f.readlines()\n","websocket_url = lines[0].split(\": \")[1].strip().replace(\"https\", \"wss\")\n","# from nerfstudio.utils.io import load_from_json\n","# from pathlib import Path\n","# json_filename = \"nerfstudio/nerfstudio/viewer/app/package.json\"\n","# version = load_from_json(Path(json_filename))[\"version\"]\n","url = f\"https://viewer.nerf.studio/?websocket_url={websocket_url}\"\n","print(url)\n","print(\"You may need to click Refresh Page after you start training!\")\n","from IPython import display\n","display.IFrame(src=url, height=800, width=\"100%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_N8_cLfjoXD"},"outputs":[],"source":["#@markdown <h1>Start Training</h1>\n","\n","%cd /content\n","if os.path.exists(f\"data/nerfstudio/{scene}/transforms.json\"):\n","    !ns-train nerfacto --viewer.websocket-port 7007 nerfstudio-data --data data/nerfstudio/$scene --downscale-factor 4\n","else:\n","    from IPython.core.display import display, HTML\n","    display(HTML('<h3 style=\"color:red\">Error: Data processing did not complete</h3>'))\n","    display(HTML('<h3>Please re-run `Downloading and Processing Data`, or view the FAQ for more info.</h3>'))"]},{"cell_type":"code","source":["!ns-render camera-path --load-config outputs/unnamed/nerfacto/2023-08-21_125010/config.yml --camera-path-filename outputs/unnamed/nerfacto/2023-08-21_125010/camera_paths/2023-08-21_125010.json --output-path renders/2023-08-21_125010/2023-08-21_125010.mp4"],"metadata":{"id":"a4cSj-NfSbUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"WGt8ukG6Htg3","outputId":"fa946890-c7d8-4e46-a54e-7231bc5a2059"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2;36m[19:48:48]\u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split train.                                          \u001b]8;id=527413;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=243595;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n","\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split test.                                           \u001b]8;id=109270;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=464675;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n","\u001b[2KLoading data batch \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\u001b[2KLoading data batch \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h/usr/local/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading latest checkpoint from load_dir\n","✅ Done loading checkpoint from \n","outputs/data-nerfstudio-poster/nerfacto/\u001b[1;36m2022\u001b[0m-\u001b[1;36m10\u001b[0m-29_192844/nerfstudio_models/step-\u001b[1;36m000014000.\u001b[0mckpt\n","\u001b[1;32mCreating trajectory video\u001b[0m\n","\u001b[2K🎥 Rendering 🎥 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[31m0.14 fps\u001b[0m \u001b[33m11:47\u001b[0m\n","\u001b[2K\u001b[32m(  ●   )\u001b[0m \u001b[33mSaving video\u001b[0m\n","\u001b[1A\u001b[2K\u001b[92m────────────────────────────────────────────── \u001b[0m\u001b[32m 🎉 🎉 🎉 Success 🎉 🎉 🎉\u001b[0m\u001b[92m ──────────────────────────────────────────────\u001b[0m\n","                                           \u001b[32mSaved video to renders/output.mp4\u001b[0m                                            \n","\u001b[0m"]}],"source":["#@title # Render Video { vertical-output: true }\n","#@markdown <h3>Export the camera path from within the viewer, then run this cell.</h3>\n","#@markdown <h5>The rendered video should be at renders/output.mp4!</h5>\n","\n","\n","base_dir = \"/content/outputs/unnamed/nerfacto/\"\n","training_run_dir = base_dir + os.listdir(base_dir)[0]\n","\n","from IPython.core.display import display, HTML\n","display(HTML('<h3>Upload the camera path JSON.</h3>'))\n","%cd $training_run_dir\n","uploaded = files.upload()\n","uploaded_camera_path_filename = list(uploaded.keys())[0]\n","\n","config_filename = training_run_dir + \"/config.yml\"\n","camera_path_filename = training_run_dir + \"/\" + uploaded_camera_path_filename\n","camera_path_filename = camera_path_filename.replace(\" \", \"\\\\ \").replace(\"(\", \"\\\\(\").replace(\")\", \"\\\\)\")\n","\n","%cd /content/\n","!ns-render camera-path --load-config $config_filename --camera-path-filename $camera_path_filename --output-path renders/output.mp4"]},{"cell_type":"code","source":["#@title # 중지된 training 재개\n","base_dir = \"/content/outputs/unnamed/nerfacto/\"\n","training_run_dir = base_dir + os.listdir(base_dir)[0] + '/nerfstudio_models'\n","\n","%cd /content/\n","!ns-train nerfacto --load-dir {training_run_dir} --viewer.websocket-port 7007 nerfstudio-data --data data/nerfstudio/$scene --downscale-factor 4"],"metadata":{"id":"DbfAa7PsTKYf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### NeRFStudio를 이용한 다양한 모델의 활용\n","* nerfacto는 여러 논문에서 component를 조합하여 만든 pipeline임\n","* nerfstudio는 nerfacto 이외에 다양한 모델들을 지원\n","  - Instant-NGP\n","  - [Instruct-NeRF2NeRF](https://docs.nerf.studio/en/latest/nerfology/methods/in2n.html)\n","  - K-Planes\n","  - [LERF](https://docs.nerf.studio/en/latest/nerfology/methods/lerf.html)\n","  - Mip-NeRF\n","  - NeRF\n","  - Nerfacto\n","  - Nerfbusters\n","  - NeRFPlayer\n","  - Tetra-NeRF\n","  - TensoRF\n","  - [Generfacto](https://docs.nerf.studio/en/latest/nerfology/methods/generfacto.html)"],"metadata":{"id":"LELvyfR8ic0k"}},{"cell_type":"markdown","source":["#### Training TensoRF"],"metadata":{"id":"wNIPUivp6Uwc"}},{"cell_type":"code","source":["!ns-train tensorf --viewer.websocket-port 7007 nerfstudio-data --data data/nerfstudio/$scene --downscale-factor 4"],"metadata":{"id":"Q6B1zNaz3U-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. ThreeStudio"],"metadata":{"id":"mG-xOZ77JjLQ"}},{"cell_type":"markdown","source":["#### Clone threestudio repo"],"metadata":{"id":"5B_rws4sJwUo"}},{"cell_type":"code","source":["!git clone https://github.com/threestudio-project/threestudio.git\n","%cd threestudio"],"metadata":{"id":"v1Zc1qyMJkyQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Install Dependencies"],"metadata":{"id":"ZPuJvKnnJ2XG"}},{"cell_type":"code","source":["!pip install ninja\n","!pip install lightning==2.0.0 omegaconf==2.3.0 jaxtyping typeguard diffusers transformers accelerate opencv-python tensorboard matplotlib imageio imageio[ffmpeg] trimesh bitsandbytes sentencepiece safetensors huggingface_hub libigl xatlas networkx pysdf PyMCubes wandb torchmetrics controlnet_aux\n","!pip install einops kornia taming-transformers-rom1504 git+https://github.com/openai/CLIP.git # zero123\n","!pip install open3d plotly # mesh visualization\n","!pip install git+https://github.com/ashawkey/envlight.git\n","!pip install git+https://github.com/KAIR-BAIR/nerfacc.git@v0.5.2\n","!pip install git+https://github.com/NVlabs/nvdiffrast.git\n","!pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch"],"metadata":{"id":"Civ70_R8JoxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Login to HuggingFace"],"metadata":{"id":"dsIt3cWqJ_Eb"}},{"cell_type":"code","source":["from huggingface_hub import interpreter_login\n","\n","interpreter_login()"],"metadata":{"id":"2TCTVlGvJuGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Generate 3D Model"],"metadata":{"id":"Ofd1Ss9xKF1E"}},{"cell_type":"code","source":["prompt = \"a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes\"\n","!python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=\"$prompt\" trainer.max_steps=1000 system.prompt_processor.spawn=false"],"metadata":{"id":"KA1NN4VhKHA9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Display the rendered Video"],"metadata":{"id":"Qnetbxd0KMph"}},{"cell_type":"code","source":["from IPython.display import HTML\n","from base64 import b64encode\n","def display_video(video_path):\n","  mp4 = open(video_path,'rb').read()\n","  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","  return HTML(\"\"\"\n","  <video width=1000 controls>\n","    <source src=\"%s\" type=\"video/mp4\">\n","  </video>\n","  \"\"\" % data_url)"],"metadata":{"id":"-TJEnCcTKJ4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# you will see the path to the saving directory at the end of the training logs\n","# replace save_dir below with that path\n","save_dir = 'path/to/save/dir'\n","\n","import os\n","import glob\n","video_path = glob.glob(os.path.join(save_dir, \"*-test.mp4\"))[0]\n","display_video(video_path)"],"metadata":{"id":"YhEbSTB4KRWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Extract the object mesh.\n","\n","Here we use an empirical threshold value. You can also first try system.geometry.isosurface_threshold=auto and visualize it. Then you can manually adjust the threshold according to the automatically determined value shown in the logs. Increase it if there are too many floaters and decrease it if the geometry is incomplete.\n","\n","The extraction process takes around 2 mins on T4."],"metadata":{"id":"FZAa6uk1KVM6"}},{"cell_type":"code","source":["!python launch.py --config $save_dir/../configs/parsed.yaml --export --gpu 0 resume=$save_dir/../ckpts/last.ckpt system.exporter_type=mesh-exporter system.exporter.context_type=cuda system.geometry.isosurface_threshold=15.0"],"metadata":{"id":"9YipGdbgKWp5"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Y9n98qQYSMOF_J5CQcxRZ33RQs0rziTG","timestamp":1692678659732},{"file_id":"https://github.com/nerfstudio-project/nerfstudio/blob/main/colab/demo.ipynb","timestamp":1692614006517}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.13"},"vscode":{"interpreter":{"hash":"c59f626636933ef1dc834fb3684b382f705301c5306cf8436d2da634c2289783"}}},"nbformat":4,"nbformat_minor":0}