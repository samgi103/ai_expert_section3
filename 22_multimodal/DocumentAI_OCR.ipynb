{"cells":[{"cell_type":"markdown","metadata":{"id":"rFD0o09o3okf"},"source":["# **Donut 游꼴 : Document Understanding Transformer**\n","Donut 游꼴, Document understanding transformer, is a new method of document understanding that utilizes an **OCR-free** end-to-end Transformer model. Donut does not require off-the-shelf OCR engines/APIs, yet it shows state-of-the-art performances on various visual document understanding tasks, such as visual document classification or information extraction (a.k.a. document parsing). In addition, we present SynthDoG 游냤, Synthetic Document Generator, that helps the model pre-training to be flexible on various languages and domains."]},{"cell_type":"markdown","metadata":{"id":"OSV2tM96Dfni"},"source":["### **Setting**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuVr5OLKDZu0"},"outputs":[],"source":["!pip install transformers==4.25.1\n","!pip install pytorch-lightning==1.6.4\n","!pip install timm==0.5.4\n","!pip install gradio\n","!pip install donut-python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBXRX2dpPz-x"},"outputs":[],"source":["import argparse\n","import gradio as gr\n","import torch\n","from PIL import Image\n","\n","from donut import DonutModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIfS3KGvP1dl"},"outputs":[],"source":["def demo_process_vqa(input_img, question):\n","    global pretrained_model, task_prompt, task_name\n","    input_img = Image.fromarray(input_img)\n","    user_prompt = task_prompt.replace(\"{user_input}\", question)\n","    output = pretrained_model.inference(input_img, prompt=user_prompt)[\"predictions\"][0]\n","    return output\n","\n","\n","def demo_process(input_img):\n","    global pretrained_model, task_prompt, task_name\n","    input_img = Image.fromarray(input_img)\n","    output = pretrained_model.inference(image=input_img, prompt=task_prompt)[\"predictions\"][0]\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"96mAmPyGP4u-"},"source":["### **Document Classification**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HOsf7L8xP1bO"},"outputs":[],"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--task\", type=str, default=\"rvlcdip\")\n","parser.add_argument(\"--pretrained_path\", type=str, default=\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n","args, left_argv = parser.parse_known_args()\n","\n","task_name = args.task\n","if \"docvqa\" == task_name:\n","    task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n","else:\n","    task_prompt = f\"<s_{task_name}>\"\n","\n","pretrained_model = DonutModel.from_pretrained(args.pretrained_path)\n","\n","if torch.cuda.is_available():\n","    pretrained_model.half()\n","    device = torch.device(\"cuda\")\n","    pretrained_model.to(device)\n","else:\n","    pretrained_model.encoder.to(torch.bfloat16)\n","\n","pretrained_model.eval()\n","\n","demo = gr.Interface(\n","    fn=demo_process_vqa if task_name == \"docvqa\" else demo_process,\n","    inputs=[\"image\", \"text\"] if task_name == \"docvqa\" else \"image\",\n","    outputs=\"json\",\n","    title=f\"Donut 游꼴 demonstration for `{task_name}` task\",\n",")\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"S_KTUklKQJdA"},"source":["### **Document VQA**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4COJARbP1ZJ"},"outputs":[],"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--task\", type=str, default=\"docvqa\")\n","parser.add_argument(\"--pretrained_path\", type=str, default=\"naver-clova-ix/donut-base-finetuned-docvqa\")\n","args, left_argv = parser.parse_known_args()\n","\n","task_name = args.task\n","if \"docvqa\" == task_name:\n","    task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n","else:\n","    task_prompt = f\"<s_{task_name}>\"\n","\n","pretrained_model = DonutModel.from_pretrained(args.pretrained_path)\n","\n","if torch.cuda.is_available():\n","    pretrained_model.half()\n","    device = torch.device(\"cuda\")\n","    pretrained_model.to(device)\n","else:\n","    pretrained_model.encoder.to(torch.bfloat16)\n","\n","pretrained_model.eval()\n","\n","demo = gr.Interface(\n","    fn=demo_process_vqa if task_name == \"docvqa\" else demo_process,\n","    inputs=[\"image\", \"text\"] if task_name == \"docvqa\" else \"image\",\n","    outputs=\"json\",\n","    title=f\"Donut 游꼴 demonstration for `{task_name}` task\",\n",")\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"CRxJUtRInzr8"},"source":["# **DiffSTE : Diffusion models for Scene Text Editing**\n","edit scene text into different font styles and colors following given text instruction. Specifically, we propose to improve pre-trained diffusion models with a dual encoder design, which includes a character encoder for better text legibility and an instruction encoder for better style control. We then utilize an instruction tuning framework to train our model learn the mapping from the text instruction to the corresponding image with either the specified style or the style of the surrounding texts in the background. Such a training method further brings our model the zero-shot generalization ability to the following three scenarios: generating text with unseen font variation, e.g. italic and bold, mixing different fonts to construct a new font, and using more relaxed forms of natural language as the instructions to guide the generation task.\n","\n"]},{"cell_type":"markdown","source":["### **Setting**"],"metadata":{"id":"V826ZMLwKhpx"}},{"cell_type":"code","source":["# coord -> http://maschek.hu/imagemap/imgmap/\n","coord = \"117,108,233,144\""],"metadata":{"id":"_KeJe09bRnbG","executionInfo":{"status":"ok","timestamp":1692188922216,"user_tz":-540,"elapsed":1006,"user":{"displayName":"yejin","userId":"02809688929696565991"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# character masking\n","import cv2\n","import numpy as np\n","\n","img = '/content/DocumentAI_OCR/samsung.png'\n","image = cv2.imread(img)\n","filename = img.split('/'[-1])\n","\n","temp = tuple(map(int, coord.split(',')))\n","x, y, w, h = temp\n","\n","image2 = np.zeros((image.shape[0], image.shape[1]), dtype=\"uint8\")\n","cv2.rectangle(image2, (x, y), (w, h), 255, -1)\n","cv2.imwrite(f'/content/masked_testimg.png', image2)"],"metadata":{"id":"9_sVvfZcRFG8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iVMZi1f94hS2"},"source":["### **Scene Text Editing**"]},{"cell_type":"code","source":["!git clone https://github.com/UCSB-NLP-Chang/DiffSTE.git\n","%cd DiffSTE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fR-GdJBQUocc","executionInfo":{"status":"ok","timestamp":1692185680276,"user_tz":-540,"elapsed":1686,"user":{"displayName":"yejin","userId":"02809688929696565991"}},"outputId":"c56d8912-afbe-489e-8511-95d61d71f505"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'DiffSTE'...\n","remote: Enumerating objects: 251, done.\u001b[K\n","remote: Counting objects: 100% (251/251), done.\u001b[K\n","remote: Compressing objects: 100% (196/196), done.\u001b[K\n","remote: Total 251 (delta 52), reused 246 (delta 47), pack-reused 0\u001b[K\n","Receiving objects: 100% (251/251), 6.57 MiB | 22.82 MiB/s, done.\n","Resolving deltas: 100% (52/52), done.\n","/content/DiffSTE\n"]}]},{"cell_type":"code","source":["# requirements.txt -> flax==0.7.2"],"metadata":{"id":"lVIGSXUHIOyq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!curl -sSL https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh\n","!bash ./miniconda.sh -bfp /usr/local\n","!conda --version\n","!conda create --name DiffSTE python=3.8 -y"],"metadata":{"id":"-a-1QdCrUypk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pretrained model download\n","!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fc0RKGWo6MPSJIZNIA_UweTOPai64S9f' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1fc0RKGWo6MPSJIZNIA_UweTOPai64S9f\" -O diffste.ckpt && rm -rf ~/cookies.txt"],"metadata":{"id":"HjtPdB3sKV-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# style-free generation\n","%%bash\n","source activate DiffSTE\n","pip install -r requirements.txt\n","pip install jax --upgrade\n","\n","python generate.py \\\n","    --ckpt_path /content/DiffSTE/diffste.ckpt\\\n","    --in_image /content/DocumentAI_OCR/samsung.png \\\n","    --in_mask /content/masked_testimg.png \\\n","    --text DASU \\\n","    --out_dir /content/"],"metadata":{"id":"n8oFqqjFUynw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# style-conditional generation\n","%%bash\n","source activate DiffSTE\n","pip install -r requirements.txt\n","pip install jax --upgrade\n","\n","python generate.py \\\n","    --ckpt_path /content/DiffSTE/diffste.ckpt \\\n","    --in_image /content/DocumentAI_OCR/OMG.png \\\n","    --in_mask /content/DocumentAI_OCR/masked3.png \\\n","    --text QnA \\\n","    --font Caprasimo \\\n","    --color white \\\n","    --out_dir /content/"],"metadata":{"id":"kLVe_XUZUnnm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N7a8jSbz30sR"},"source":["# **EasyOCR**\n","Ready-to-use OCR with 80+ supported languages and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.\n","- opensource text detection & recognition model\n","- support language -> https://www.jaided.ai/easyocr/"]},{"cell_type":"markdown","metadata":{"id":"qbhcBHw33xgF"},"source":["### **Setting**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ca_V6Z8G3yNK"},"outputs":[],"source":["!pip install easyocr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FodOkDunf6dc"},"outputs":[],"source":["!unzip DocumentAI.zip"]},{"cell_type":"markdown","metadata":{"id":"Zkz4W9kOdawr"},"source":["### **Detection & Crop**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8F3cGMvWgP5U"},"outputs":[],"source":["def crop_box(ocr):\n","    max_diff = 0\n","    max_row = []\n","    for i in ocr:\n","        diff = i[0][2][1]-i[0][0][1]\n","        x_diff = i[0][2][0]-i[0][0][0]\n","        if diff > max_diff and x_diff > 0:\n","            max_diff = diff\n","            max_row = i[0]\n","    box = tuple(max_row[0] + max_row[2])\n","    return box"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QRAIlsqkjTFT"},"outputs":[],"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def img_show(img):\n","    img = np.array(img)\n","    plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"LH4Ud0aplJU7"},"source":["### **English OCR**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kj5fokzG3yfY"},"outputs":[],"source":["import easyocr\n","\n","en_imgpath = \"/content/DocumentAI/Bumblebee.jpg\"\n","en_img = Image.open(en_imgpath)\n","img_show(en_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVu-_rUUg7b0"},"outputs":[],"source":["en_reader = easyocr.Reader(['en'])\n","en_ocr = en_reader.readtext(en_imgpath)\n","box = crop_box(en_ocr)\n","text = en_img.crop(box)\n","img_show(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0eammAwmVg-"},"outputs":[],"source":["# recognition\n","print(en_ocr)"]},{"cell_type":"markdown","metadata":{"id":"sOK1NxHelmZH"},"source":["### **Korean OCR**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPLDMU98lp41"},"outputs":[],"source":["ko_imgpath = \"/content/DocumentAI/Busanhaeng.jpg\"\n","ko_img = Image.open(ko_imgpath)\n","img_show(ko_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mlx__2x1fE75"},"outputs":[],"source":["ko_reader = easyocr.Reader(['ko'])\n","ko_ocr = ko_reader.readtext(ko_imgpath)\n","box = crop_box(ko_ocr)\n","text = ko_img.crop(box)\n","img_show(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aINlMpL_mSMP"},"outputs":[],"source":["# recognition\n","print(ko_ocr)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1tOyD9ZQv-lwLqPB1jXRNbHeBljYLBNAU","authorship_tag":"ABX9TyM8D/D9tptwaZZ+kV5cgb7Y"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}