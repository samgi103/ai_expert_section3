{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Practice: Model compression in Deep Learning"],"metadata":{"id":"VWgdS661S03g"}},{"cell_type":"markdown","source":["## Typical process of the low-rank compression\n","#### Normal Training ⇒ Rank Selection *(excluded from this practice)* ⇒ Low-rank compression ⇒ Fine-tuning\n","\n","## Question\n","1. 코드에서 빈 부분을 채우세요.\n","2. 3가지의 rank setting에 대해서 성능 비교를 수행하세요.\n"," - R=[20, 100, 200, 8]\n"," - R=[15, 50, 100, 6]\n"," - R=[10, 10, 50, 3]\n","3. 3개의 compressed model에 대해 Fine-tuning을 수행한 뒤 성능 비교를 수행하세요."],"metadata":{"id":"R9yuhWKpS7EB"}},{"cell_type":"markdown","source":["## Library import"],"metadata":{"id":"5jR7stYXV1CX"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"umb9o8VBa4Ap","executionInfo":{"status":"ok","timestamp":1693984360344,"user_tz":-540,"elapsed":4647,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","from torch.nn.init import xavier_uniform_\n","from collections import OrderedDict\n","from torch.utils.data import TensorDataset, DataLoader\n","from torchvision import datasets\n","from torch import optim\n","import torchvision\n","import time\n","import torch\n","from torch import nn\n","from collections import OrderedDict\n","from scipy.linalg import svd\n","import numpy as np\n","import copy"]},{"cell_type":"markdown","source":["## Define a model"],"metadata":{"id":"bYhCoXQ-V_h6"}},{"cell_type":"code","source":["def _weights_init(m):\n","    classname = m.__class__.__name__\n","    if isinstance(m, nn.Linear):\n","        xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.0)\n","\n","class LambdaLayer(nn.Module):\n","    def __init__(self, lambd):\n","        super(LambdaLayer, self).__init__()\n","        self.lambd = lambd\n","\n","    def forward(self, x):\n","        return self.lambd(x)\n","\n","\n","class LeNet5(nn.Module):\n","    def __init__(self, dropout, nonlinearity):\n","        super(LeNet5, self).__init__()\n","        self.special = True\n","        filters = [(20, 5), (50, 5)]\n","        layers = [(800, 500), (500, 10)]\n","\n","        cfg = []\n","        cfg.append(['init_reshape', LambdaLayer(lambda x: x.view(x.size(0), 1,28,28))])\n","        for i, f in enumerate(filters):\n","            prev = 1 if i==0 else filters[i-1][0]\n","            cfg.append(('compressible_' + str(i), nn.Conv2d(prev, f[0], f[1])))\n","            cfg.append(('nonlineairy_'+str(i), nonlinearity()))\n","            cfg.append(('maxpool_'+str(i), nn.MaxPool2d(kernel_size=(2,2), stride=2)))\n","\n","\n","        cfg.append(['reshape', LambdaLayer(lambda x: x.view(x.size(0),-1))])\n","        for i, l in enumerate(layers):\n","            cfg.append(('compressible_' + str(i+len(filters)), nn.Linear(*l)))\n","            if i != len(layers)-1:\n","                # only non terminal layers have nonlinearity and (possible) dropouts\n","                cfg.append(('nonlinearity_' + str(i+len(filters)), nonlinearity()))\n","                if dropout:\n","                    cfg.append(('drop_'+str(i+len(filters)), nn.Dropout()))\n","\n","        self.output = nn.Sequential(OrderedDict(cfg))\n","        self.apply(_weights_init)\n","    def forward(self, input):\n","        h = self.output(input)\n","        return h\n","\n","def lenet5_classic():\n","    return LeNet5(dropout=False, nonlinearity=lambda: nn.ReLU(True))"],"metadata":{"id":"19Q4sWh80Kxu","executionInfo":{"status":"ok","timestamp":1693984363137,"user_tz":-540,"elapsed":355,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Define datasets"],"metadata":{"id":"n-y5CX01Wesp"}},{"cell_type":"code","source":["dataset = 'MNIST'\n","batch_size = 256\n","n_workers = 2\n","\n","def mnist_data():\n","    mnist_train = torchvision.datasets.MNIST(root='./datasets/', train=True, download=True)\n","    mnist_test = torchvision.datasets.MNIST(root='./datasets/', train=False, download=True)\n","\n","    train_data = mnist_train.data.to(torch.float) / 255.\n","    test_data = mnist_test.data.to(torch.float) / 255.\n","    mean_image = torch.mean(train_data, dim=0)\n","\n","    train_data -= mean_image\n","    test_data -= mean_image\n","\n","    train_labels = mnist_train.targets\n","    test_labels = mnist_test.targets\n","\n","    our_mnist = {\n","        'train_data': train_data, 'test_data': test_data,\n","        'train_labels': train_labels, 'test_labels': test_labels\n","    }\n","    return our_mnist\n","\n","data = mnist_data()\n","train_data = TensorDataset(data['train_data'], data['train_labels'])\n","test_data = TensorDataset(data['test_data'], data['test_labels'])\n","\n","train_loader = DataLoader(train_data, num_workers=n_workers, batch_size=batch_size, shuffle=True, pin_memory=False)\n","test_loader = DataLoader(test_data, num_workers=n_workers, batch_size=batch_size, shuffle=False, pin_memory=False)\n"],"metadata":{"id":"okdiL5xRWePh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693984367294,"user_tz":-540,"elapsed":2065,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"8b5ad66a-b251-4fda-a1df-d0a8dc145850"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 33967250.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 44650089.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 31328782.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 7556734.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n","\n"]}]},{"cell_type":"markdown","source":["## Normal Training"],"metadata":{"id":"ymYXA20XWIMe"}},{"cell_type":"code","source":["# Prepare a model\n","model = lenet5_classic()\n","model.cuda()\n","print(model)\n","\n","# Hyper-parameters for training\n","lr = 0.1\n","lr_decay = 0.99\n","momentum = 0.9\n","epochs = 100\n","start_epoch = 0\n","print_freq = 20\n","checkpoint = 20\n","\n","# Define an optimizer and a scheduler\n","optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, nesterov=True)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n","\n","# Define the funcions required to training\n","def my_eval(x, target, model):\n","    out_ = model.forward(x)\n","    return out_, torch.nn.functional.cross_entropy(out_, target)\n","\n","def format_time(seconds):\n","    if seconds < 60:\n","        return '{:.1f}s.'.format(seconds)\n","    if seconds < 3600:\n","        return '{:d}m. {}'.format(int(seconds//60), format_time(seconds%60))\n","    if seconds < 3600*24:\n","        return '{:d}h. {}'.format(int(seconds//3600), format_time(seconds%3600))\n","    return '{:d}d. {}'.format(int(seconds//(3600*24)), format_time(seconds%(3600*24)))\n","\n","def compute_acc_loss(forward_func, data_loader, model):\n","    correct_cnt, ave_loss = 0, 0\n","    for batch_idx, (x, target) in enumerate(data_loader):\n","        with torch.no_grad():\n","            target = target.cuda()\n","            score, loss = forward_func(x.cuda(), target, model)\n","            _, pred_label = torch.max(score.data, 1)\n","            correct_cnt += (pred_label == target.data).sum().item()\n","            ave_loss += loss.data.item() * len(x)\n","    accuracy = correct_cnt * 1.0 / len(data_loader.dataset)\n","    print(correct_cnt, len(data_loader.dataset))\n","    ave_loss /= len(data_loader.dataset)\n","    return accuracy, ave_loss\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0.0\n","        self.avg = 0.0\n","        self.sum = 0.0\n","        self.count = 0.0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","# Training\n","training_time = 0\n","epoch = 0\n","all_start_time = time.time()\n","epoch_time = AverageMeter()\n","\n","for epoch in range(start_epoch, epochs):\n","    start_time = time.time()\n","    model.train()\n","    for batch_idx, (x, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        x, target = x.cuda(), target.cuda()\n","        out = model.forward(x)\n","        loss = torch.nn.functional.cross_entropy(out, target)\n","        loss.backward()\n","        optimizer.step()\n","        break\n","    end_time = time.time()\n","    epoch_time.update(end_time - start_time)\n","    training_time = end_time - all_start_time\n","    model.eval()\n","    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n","    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n","    if (epoch+1) % print_freq == 0:\n","        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, model)\n","        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n","        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, model)\n","        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n","    scheduler.step()\n","\n","    if checkpoint and (epoch+1) % checkpoint == 0:\n","        # create and save checkpoint here\n","        to_save = {}\n","        to_save['model_state'] = model.state_dict()\n","        to_save['optimizer_state'] = optimizer.state_dict()\n","        to_save['lr'] = scheduler.get_last_lr()\n","        to_save['epoch'] = epoch + 1\n","        torch.save(to_save, './lenet5_checkpoint.pth.tar')"],"metadata":{"id":"CqBzq36B0Yy1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693984953684,"user_tz":-540,"elapsed":18332,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"5a5cc76b-42e2-4567-83ed-482b63296d74"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["LeNet5(\n","  (output): Sequential(\n","    (init_reshape): LambdaLayer()\n","    (compressible_0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n","    (nonlineairy_0): ReLU(inplace=True)\n","    (maxpool_0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (compressible_1): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n","    (nonlineairy_1): ReLU(inplace=True)\n","    (maxpool_1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (reshape): LambdaLayer()\n","    (compressible_2): Linear(in_features=800, out_features=500, bias=True)\n","    (nonlinearity_2): ReLU(inplace=True)\n","    (compressible_3): Linear(in_features=500, out_features=10, bias=True)\n","  )\n",")\n","Epoch 0 finished in 0.085s (avg.: 0.085s). Training for 0.1s.\n","\tLR: 0.1\n","Epoch 1 finished in 0.079s (avg.: 0.082s). Training for 0.2s.\n","\tLR: 0.099\n","Epoch 2 finished in 0.088s (avg.: 0.084s). Training for 0.3s.\n","\tLR: 0.09801\n","Epoch 3 finished in 0.086s (avg.: 0.085s). Training for 0.3s.\n","\tLR: 0.09703\n","Epoch 4 finished in 0.088s (avg.: 0.085s). Training for 0.4s.\n","\tLR: 0.09606\n","Epoch 5 finished in 0.087s (avg.: 0.086s). Training for 0.5s.\n","\tLR: 0.0951\n","Epoch 6 finished in 0.090s (avg.: 0.086s). Training for 0.6s.\n","\tLR: 0.09415\n","Epoch 7 finished in 0.087s (avg.: 0.086s). Training for 0.7s.\n","\tLR: 0.09321\n","Epoch 8 finished in 0.097s (avg.: 0.087s). Training for 0.8s.\n","\tLR: 0.09227\n","Epoch 9 finished in 0.095s (avg.: 0.088s). Training for 0.9s.\n","\tLR: 0.09135\n","Epoch 10 finished in 0.083s (avg.: 0.088s). Training for 1.0s.\n","\tLR: 0.09044\n","Epoch 11 finished in 0.088s (avg.: 0.088s). Training for 1.1s.\n","\tLR: 0.08953\n","Epoch 12 finished in 0.088s (avg.: 0.088s). Training for 1.1s.\n","\tLR: 0.08864\n","Epoch 13 finished in 0.083s (avg.: 0.087s). Training for 1.2s.\n","\tLR: 0.08775\n","Epoch 14 finished in 0.090s (avg.: 0.088s). Training for 1.3s.\n","\tLR: 0.08687\n","Epoch 15 finished in 0.080s (avg.: 0.087s). Training for 1.4s.\n","\tLR: 0.08601\n","Epoch 16 finished in 0.091s (avg.: 0.087s). Training for 1.5s.\n","\tLR: 0.08515\n","Epoch 17 finished in 0.090s (avg.: 0.087s). Training for 1.6s.\n","\tLR: 0.08429\n","Epoch 18 finished in 0.085s (avg.: 0.087s). Training for 1.7s.\n","\tLR: 0.08345\n","Epoch 19 finished in 0.095s (avg.: 0.088s). Training for 1.8s.\n","\tLR: 0.08262\n","53113 60000\n","\ttrain loss: 0.362480, accuracy: 0.8852\n","8921 10000\n","\ttest  loss: 0.340373, accuracy: 0.8921\n","Epoch 20 finished in 0.086s (avg.: 0.088s). Training for 3.5s.\n","\tLR: 0.08179\n","Epoch 21 finished in 0.087s (avg.: 0.088s). Training for 3.6s.\n","\tLR: 0.08097\n","Epoch 22 finished in 0.087s (avg.: 0.088s). Training for 3.7s.\n","\tLR: 0.08016\n","Epoch 23 finished in 0.088s (avg.: 0.088s). Training for 3.8s.\n","\tLR: 0.07936\n","Epoch 24 finished in 0.100s (avg.: 0.088s). Training for 3.9s.\n","\tLR: 0.07857\n","Epoch 25 finished in 0.106s (avg.: 0.089s). Training for 4.0s.\n","\tLR: 0.07778\n","Epoch 26 finished in 0.276s (avg.: 0.096s). Training for 4.3s.\n","\tLR: 0.077\n","Epoch 27 finished in 0.124s (avg.: 0.097s). Training for 4.4s.\n","\tLR: 0.07623\n","Epoch 28 finished in 0.091s (avg.: 0.097s). Training for 4.5s.\n","\tLR: 0.07547\n","Epoch 29 finished in 0.089s (avg.: 0.096s). Training for 4.6s.\n","\tLR: 0.07472\n","Epoch 30 finished in 0.084s (avg.: 0.096s). Training for 4.6s.\n","\tLR: 0.07397\n","Epoch 31 finished in 0.083s (avg.: 0.096s). Training for 4.7s.\n","\tLR: 0.07323\n","Epoch 32 finished in 0.087s (avg.: 0.095s). Training for 4.8s.\n","\tLR: 0.0725\n","Epoch 33 finished in 0.096s (avg.: 0.095s). Training for 4.9s.\n","\tLR: 0.07177\n","Epoch 34 finished in 0.090s (avg.: 0.095s). Training for 5.0s.\n","\tLR: 0.07106\n","Epoch 35 finished in 0.112s (avg.: 0.096s). Training for 5.1s.\n","\tLR: 0.07034\n","Epoch 36 finished in 0.117s (avg.: 0.096s). Training for 5.2s.\n","\tLR: 0.06964\n","Epoch 37 finished in 0.145s (avg.: 0.097s). Training for 5.4s.\n","\tLR: 0.06894\n","Epoch 38 finished in 0.133s (avg.: 0.098s). Training for 5.5s.\n","\tLR: 0.06826\n","Epoch 39 finished in 0.134s (avg.: 0.099s). Training for 5.7s.\n","\tLR: 0.06757\n","57010 60000\n","\ttrain loss: 0.166636, accuracy: 0.9502\n","9548 10000\n","\ttest  loss: 0.150211, accuracy: 0.9548\n","Epoch 40 finished in 0.108s (avg.: 0.099s). Training for 8.1s.\n","\tLR: 0.0669\n","Epoch 41 finished in 0.091s (avg.: 0.099s). Training for 8.1s.\n","\tLR: 0.06623\n","Epoch 42 finished in 0.082s (avg.: 0.099s). Training for 8.2s.\n","\tLR: 0.06557\n","Epoch 43 finished in 0.091s (avg.: 0.099s). Training for 8.3s.\n","\tLR: 0.06491\n","Epoch 44 finished in 0.082s (avg.: 0.098s). Training for 8.4s.\n","\tLR: 0.06426\n","Epoch 45 finished in 0.099s (avg.: 0.098s). Training for 8.5s.\n","\tLR: 0.06362\n","Epoch 46 finished in 0.081s (avg.: 0.098s). Training for 8.6s.\n","\tLR: 0.06298\n","Epoch 47 finished in 0.086s (avg.: 0.098s). Training for 8.7s.\n","\tLR: 0.06235\n","Epoch 48 finished in 0.094s (avg.: 0.098s). Training for 8.8s.\n","\tLR: 0.06173\n","Epoch 49 finished in 0.084s (avg.: 0.097s). Training for 8.8s.\n","\tLR: 0.06111\n","Epoch 50 finished in 0.089s (avg.: 0.097s). Training for 8.9s.\n","\tLR: 0.0605\n","Epoch 51 finished in 0.101s (avg.: 0.097s). Training for 9.0s.\n","\tLR: 0.0599\n","Epoch 52 finished in 0.092s (avg.: 0.097s). Training for 9.1s.\n","\tLR: 0.0593\n","Epoch 53 finished in 0.082s (avg.: 0.097s). Training for 9.2s.\n","\tLR: 0.0587\n","Epoch 54 finished in 0.083s (avg.: 0.097s). Training for 9.3s.\n","\tLR: 0.05812\n","Epoch 55 finished in 0.092s (avg.: 0.097s). Training for 9.4s.\n","\tLR: 0.05754\n","Epoch 56 finished in 0.087s (avg.: 0.096s). Training for 9.5s.\n","\tLR: 0.05696\n","Epoch 57 finished in 0.089s (avg.: 0.096s). Training for 9.6s.\n","\tLR: 0.05639\n","Epoch 58 finished in 0.091s (avg.: 0.096s). Training for 9.7s.\n","\tLR: 0.05583\n","Epoch 59 finished in 0.086s (avg.: 0.096s). Training for 9.7s.\n","\tLR: 0.05527\n","57838 60000\n","\ttrain loss: 0.126237, accuracy: 0.9640\n","9673 10000\n","\ttest  loss: 0.114711, accuracy: 0.9673\n","Epoch 60 finished in 0.089s (avg.: 0.096s). Training for 11.3s.\n","\tLR: 0.05472\n","Epoch 61 finished in 0.086s (avg.: 0.096s). Training for 11.4s.\n","\tLR: 0.05417\n","Epoch 62 finished in 0.095s (avg.: 0.096s). Training for 11.5s.\n","\tLR: 0.05363\n","Epoch 63 finished in 0.086s (avg.: 0.096s). Training for 11.6s.\n","\tLR: 0.05309\n","Epoch 64 finished in 0.093s (avg.: 0.096s). Training for 11.7s.\n","\tLR: 0.05256\n","Epoch 65 finished in 0.087s (avg.: 0.095s). Training for 11.8s.\n","\tLR: 0.05203\n","Epoch 66 finished in 0.086s (avg.: 0.095s). Training for 11.9s.\n","\tLR: 0.05151\n","Epoch 67 finished in 0.088s (avg.: 0.095s). Training for 12.0s.\n","\tLR: 0.051\n","Epoch 68 finished in 0.083s (avg.: 0.095s). Training for 12.0s.\n","\tLR: 0.05049\n","Epoch 69 finished in 0.102s (avg.: 0.095s). Training for 12.1s.\n","\tLR: 0.04998\n","Epoch 70 finished in 0.088s (avg.: 0.095s). Training for 12.2s.\n","\tLR: 0.04948\n","Epoch 71 finished in 0.086s (avg.: 0.095s). Training for 12.3s.\n","\tLR: 0.04899\n","Epoch 72 finished in 0.086s (avg.: 0.095s). Training for 12.4s.\n","\tLR: 0.0485\n","Epoch 73 finished in 0.094s (avg.: 0.095s). Training for 12.5s.\n","\tLR: 0.04801\n","Epoch 74 finished in 0.086s (avg.: 0.095s). Training for 12.6s.\n","\tLR: 0.04753\n","Epoch 75 finished in 0.094s (avg.: 0.095s). Training for 12.7s.\n","\tLR: 0.04706\n","Epoch 76 finished in 0.098s (avg.: 0.095s). Training for 12.8s.\n","\tLR: 0.04659\n","Epoch 77 finished in 0.088s (avg.: 0.095s). Training for 12.9s.\n","\tLR: 0.04612\n","Epoch 78 finished in 0.089s (avg.: 0.095s). Training for 13.0s.\n","\tLR: 0.04566\n","Epoch 79 finished in 0.090s (avg.: 0.094s). Training for 13.0s.\n","\tLR: 0.0452\n","58176 60000\n","\ttrain loss: 0.103551, accuracy: 0.9696\n","9728 10000\n","\ttest  loss: 0.092375, accuracy: 0.9728\n","Epoch 80 finished in 0.088s (avg.: 0.094s). Training for 14.6s.\n","\tLR: 0.04475\n","Epoch 81 finished in 0.093s (avg.: 0.094s). Training for 14.7s.\n","\tLR: 0.0443\n","Epoch 82 finished in 0.082s (avg.: 0.094s). Training for 14.8s.\n","\tLR: 0.04386\n","Epoch 83 finished in 0.091s (avg.: 0.094s). Training for 14.8s.\n","\tLR: 0.04342\n","Epoch 84 finished in 0.093s (avg.: 0.094s). Training for 14.9s.\n","\tLR: 0.04299\n","Epoch 85 finished in 0.099s (avg.: 0.094s). Training for 15.0s.\n","\tLR: 0.04256\n","Epoch 86 finished in 0.093s (avg.: 0.094s). Training for 15.1s.\n","\tLR: 0.04213\n","Epoch 87 finished in 0.095s (avg.: 0.094s). Training for 15.2s.\n","\tLR: 0.04171\n","Epoch 88 finished in 0.076s (avg.: 0.094s). Training for 15.3s.\n","\tLR: 0.04129\n","Epoch 89 finished in 0.092s (avg.: 0.094s). Training for 15.4s.\n","\tLR: 0.04088\n","Epoch 90 finished in 0.088s (avg.: 0.094s). Training for 15.5s.\n","\tLR: 0.04047\n","Epoch 91 finished in 0.082s (avg.: 0.094s). Training for 15.6s.\n","\tLR: 0.04007\n","Epoch 92 finished in 0.086s (avg.: 0.094s). Training for 15.7s.\n","\tLR: 0.03967\n","Epoch 93 finished in 0.091s (avg.: 0.094s). Training for 15.7s.\n","\tLR: 0.03927\n","Epoch 94 finished in 0.078s (avg.: 0.094s). Training for 15.8s.\n","\tLR: 0.03888\n","Epoch 95 finished in 0.091s (avg.: 0.094s). Training for 15.9s.\n","\tLR: 0.03849\n","Epoch 96 finished in 0.091s (avg.: 0.093s). Training for 16.0s.\n","\tLR: 0.0381\n","Epoch 97 finished in 0.094s (avg.: 0.093s). Training for 16.1s.\n","\tLR: 0.03772\n","Epoch 98 finished in 0.097s (avg.: 0.094s). Training for 16.2s.\n","\tLR: 0.03735\n","Epoch 99 finished in 0.081s (avg.: 0.093s). Training for 16.3s.\n","\tLR: 0.03697\n","58409 60000\n","\ttrain loss: 0.087775, accuracy: 0.9735\n","9749 10000\n","\ttest  loss: 0.080811, accuracy: 0.9749\n"]}]},{"cell_type":"markdown","source":["## Rank Selection"],"metadata":{"id":"-FMj3wsai6NK"}},{"cell_type":"code","source":["selected_rank1=[10, 10, 15, 7]\n","selected_rank2=[5, 5, 7, 5]\n","selected_rank3=[3, 3, 5, 3]"],"metadata":{"id":"--beybIo53uM","executionInfo":{"status":"ok","timestamp":1693984398353,"user_tz":-540,"elapsed":289,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Define the functions for low-rank compression"],"metadata":{"id":"4gcxAgt3jCuw"}},{"cell_type":"code","source":["def linear_layer_reparametrizer(sub_module, conv_scheme='scheme_1'):\n","    W = sub_module.weight.data.cpu().numpy()\n","\n","    init_shape = None\n","    n,m,d1,d2 = None, None, None, None\n","    if isinstance(sub_module, nn.Conv2d):\n","        if conv_scheme == 'scheme_1':\n","            ####################\n","            # implement here\n","            init_shape = W.shape\n","            reshaped = W.reshape([init_shape[0], -1])\n","            W = reshaped\n","            ####################\n","        elif conv_scheme == 'scheme_2':\n","            raise NotImplementedError(\"We did not implement scheme-2 in this pratice.\")\n","\n","    u, s, v = svd(W, full_matrices=False)\n","    from numpy.linalg import matrix_rank\n","\n","    r = sub_module.rank_ if hasattr(sub_module, 'rank_') else sub_module.selected_rank_ if hasattr(sub_module, 'selected_rank_') else int(matrix_rank(W))\n","\n","    if r < np.min(W.shape):\n","        diag = np.diag(s[:r] ** 0.5)\n","        U = u[:, :r] @ diag\n","        V = diag @ v[:r, :]\n","        new_W = U @ V\n","\n","\n","        from numpy.linalg import norm\n","        m,n = W.shape\n","        if r > np.floor(m*n/(m+n)):\n","            raise RankNotEfficientException(\"Selected rank doesn't contribute to any savings\")\n","        bias = sub_module.bias is not None\n","        if isinstance(sub_module, nn.Linear): # (out_c, in_c) : matrix / (out_c, r) & (r, in_c)\n","            ####################\n","            # implement here\n","            #l1 = nn.Linear(in_features = sub_module.in_features, out_feature = r, bias = False) # (r, in_c)\n","            #l2 = nn.Linear(in_features = r, out_feature = sub_module.out_feature, bias = bias) # (out_c, r)\n","\n","            l1 = nn.Linear(in_features=sub_module.in_features, out_features=r, bias=False)\n","            l2 = nn.Linear(in_features=r, out_features=sub_module.out_features, bias=bias)\n","            ####################\n","            l1.weight.data = torch.from_numpy(V)\n","            l2.weight.data = torch.from_numpy(U)\n","            if bias:\n","                l2.bias.data = sub_module.bias.data\n","            return l1, l2\n","        else:\n","            if conv_scheme == 'scheme_1':\n","                ####################\n","                # implement here # (out_c, in_c x k_size x k_size) --> (out_c, r) & (r, in_c x k_size x k_size)\n","                # l1 = nn.Conv2d(in_channels = sub_module.in_channels, # --> (out_c, r x 1 x 1)\n","                #                out_channels = 4,\n","                #                kernel_size = sub_module.kernel_size,\n","                #                stride = sub_module.stide,\n","                #                padding = sub_module.padding,\n","                #                dilation = sub_module.dilation,\n","                #                groups = sub_module.groups,\n","                #                bias = False) # (r, in_c x k_size x k_size)\n","                # l2 = nn.Conv2d(in_channels = r, out_channels = sub_module.out_channels, kernel_size = 1, bias = bias)\n","\n","                l1 = nn.Conv2d(in_channels=sub_module.in_channels,\n","                               out_channels=r,\n","                               kernel_size=sub_module.kernel_size,\n","                               stride=sub_module.stride,\n","                               padding=sub_module.padding,\n","                               dilation=sub_module.dilation,\n","                               groups=sub_module.groups,\n","                               bias=False)\n","\n","                l2 = nn.Conv2d(in_channels=r, out_channels=sub_module.out_channels,\n","                               kernel_size=1,\n","                               bias=bias)\n","                ####################\n","                l1.weight.data = torch.from_numpy(V.reshape([-1, *init_shape[1:]]))\n","                l2.weight.data = torch.from_numpy(U[:, :, None, None])\n","\n","                if bias:\n","                    l2.bias.data = sub_module.bias.data\n","\n","                return l1, l2\n","            elif conv_scheme == 'scheme_2':\n","                raise NotImplementedError(\"We did not implement scheme-2 in this pratice.\")\n","\n","\n","def reparametrization_helper(list_of_modules, conv_scheme, old_weight_decay=True):\n","    new_sequence = []\n","    items = list_of_modules.items()\n","    decayed_values_repar = []\n","    decayed_valued_old = []\n","    for i, (name, sub_module) in enumerate(items):\n","        if isinstance(sub_module, nn.Sequential):\n","            dv_repar_sub, dv_old_sub, nseq_sub = reparametrization_helper(sub_module._modules, conv_scheme=conv_scheme,old_weight_decay=old_weight_decay)\n","            new_sequence.append((name, nn.Sequential(OrderedDict(nseq_sub))))\n","            decayed_values_repar.extend(dv_repar_sub)\n","            decayed_valued_old.extend(dv_old_sub)\n","        elif isinstance(sub_module, nn.Linear) or isinstance(sub_module, nn.Conv2d):\n","            try:\n","                l1, l2 = linear_layer_reparametrizer(sub_module, conv_scheme=conv_scheme)\n","                new_sequence.append((name + '_V', l1))\n","                new_sequence.append((name + '_U', l2))\n","                decayed_values_repar.append((l1, l2))\n","\n","            except Exception as e:\n","                new_sequence.append((name, sub_module))\n","                decayed_valued_old.append(sub_module.weight)\n","        else:\n","            new_sequence.append((name, sub_module))\n","            if old_weight_decay and hasattr(sub_module, 'weight'):\n","                decayed_valued_old.append(sub_module.weight)\n","    return decayed_values_repar, decayed_valued_old, new_sequence\n","\n","\n","def reparametrize_low_rank(model, old_weight_decay=True):\n","    decayed_values_repar, decayed_valued_old, new_sequence = reparametrization_helper(model.output._modules, conv_scheme='scheme_1', old_weight_decay=old_weight_decay)\n","    model.output = nn.Sequential(OrderedDict(new_sequence))\n","\n","    def weight_decay():\n","        sum_ = torch.autograd.Variable(torch.FloatTensor([0.0]).cuda())\n","        for x in decayed_valued_old:\n","            sum_ += torch.sum(x**2)\n","        for v,u in decayed_values_repar:\n","            v = v.weight\n","            u = u.weight\n","            u_ = u.view(u.size()[0], -1)\n","            v_ = v.view(u_.size()[1], -1)\n","            sum_ += torch.sum(torch.matmul(u_,v_)**2)\n","        return sum_\n","    model.weight_decay = weight_decay\n","    return nn.Sequential(OrderedDict(new_sequence))"],"metadata":{"id":"OYEwODYk535Z","executionInfo":{"status":"ok","timestamp":1693984899624,"user_tz":-540,"elapsed":277,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Compress the model(i.e., compressed_model1) using first ranks"],"metadata":{"id":"m8M_VZIujTIU"}},{"cell_type":"code","source":["compressed_model1 = copy.deepcopy(model)\n","for i, module in enumerate([x for x in compressed_model1.modules() if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)]):\n","      module.selected_rank_ = selected_rank1[i]\n","      print(module.selected_rank_)\n","reparametrize_low_rank(compressed_model1)\n","compressed_model1.cuda()\n","compressed_model1.eval()\n","accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model1)\n","print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))"],"metadata":{"id":"MU3y6lZ67zSd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693984902184,"user_tz":-540,"elapsed":807,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"f007346e-f3c5-4318-c919-ec0542f932ee"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n","10\n","15\n","7\n","9204 10000\n","\ttest  loss: 0.261443, accuracy: 0.9204\n"]}]},{"cell_type":"markdown","source":["## Compress the model(i.e., compressed_model1) using second ranks"],"metadata":{"id":"cr9GJdAcjgYz"}},{"cell_type":"code","source":["compressed_model2 = copy.deepcopy(model)\n","for i, module in enumerate([x for x in compressed_model2.modules() if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)]):\n","      module.selected_rank_ = selected_rank2[i]\n","      print(module.selected_rank_)\n","reparametrize_low_rank(compressed_model2)\n","compressed_model2.cuda()\n","compressed_model2.eval()\n","accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model2)\n","print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))"],"metadata":{"id":"V1VQSxq88KaP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693984905330,"user_tz":-540,"elapsed":459,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"305f4a5f-cf3a-435c-f9de-6d425e29a4c2"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","5\n","7\n","5\n","7363 10000\n","\ttest  loss: 0.765986, accuracy: 0.7363\n"]}]},{"cell_type":"markdown","source":["## Compress the model(i.e., compressed_model1) using third ranks"],"metadata":{"id":"dnH-C3THjnB1"}},{"cell_type":"code","source":["compressed_model3 = copy.deepcopy(model)\n","for i, module in enumerate([x for x in compressed_model3.modules() if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)]):\n","      module.selected_rank_ = selected_rank3[i]\n","      print(module.selected_rank_)\n","reparametrize_low_rank(compressed_model3)\n","compressed_model3.cuda()\n","compressed_model3.eval()\n","accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model3)\n","print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))"],"metadata":{"id":"OqZbga6Pgs5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693984908073,"user_tz":-540,"elapsed":541,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"3cd6b4fc-4167-4981-d59a-11652c67f833"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","3\n","5\n","3\n","3167 10000\n","\ttest  loss: 1.675865, accuracy: 0.3167\n"]}]},{"cell_type":"markdown","source":["## Fine-tune the compressed_model1"],"metadata":{"id":"TTRbvHtzjp4p"}},{"cell_type":"code","source":["# fine-tuning\n","\n","batch_size = 256\n","lr = 0.02\n","lr_decay = 0.99\n","momentum = 0.9\n","epochs = 100\n","dataset = 'MNIST'\n","n_workers = 2\n","start_epoch = 0\n","print_freq = 20\n","checkpoint = 20\n","\n","\n","training_time = 0\n","epoch = 0\n","all_start_time = time.time()\n","epoch_time = AverageMeter()\n","\n","optimizer = torch.optim.SGD(compressed_model1.parameters(), lr, momentum=momentum, nesterov=True)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n","\n","for epoch in range(start_epoch, epochs):\n","    start_time = time.time()\n","    compressed_model1.train()\n","    for batch_idx, (x, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        x, target = x.cuda(), target.cuda()\n","        out = compressed_model1.forward(x)\n","        loss = torch.nn.functional.cross_entropy(out, target)\n","        loss.backward()\n","        optimizer.step()\n","        break\n","    end_time = time.time()\n","    epoch_time.update(end_time - start_time)\n","    training_time = end_time - all_start_time\n","    compressed_model1.eval()\n","    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n","    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n","    if (epoch+1) % print_freq == 0:\n","        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, compressed_model1)\n","        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n","        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model1)\n","        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n","    scheduler.step()\n","\n","    if checkpoint and (epoch+1) % checkpoint == 0:\n","        # create and save checkpoint here\n","        to_save = {}\n","        to_save['model_state'] = compressed_model1.state_dict()\n","        to_save['optimizer_state'] = optimizer.state_dict()\n","        to_save['lr'] = scheduler.get_last_lr()\n","        to_save['epoch'] = epoch + 1\n","        torch.save(to_save, './compressed_lenet5_v1_checkpoint.pth.tar')"],"metadata":{"id":"653_v79d8StE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693984926915,"user_tz":-540,"elapsed":16950,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"0170e4c3-c4bf-4e3a-9f08-092ab7d55329"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 finished in 0.105s (avg.: 0.105s). Training for 0.1s.\n","\tLR: 0.02\n","Epoch 1 finished in 0.096s (avg.: 0.100s). Training for 0.2s.\n","\tLR: 0.0198\n","Epoch 2 finished in 0.085s (avg.: 0.095s). Training for 0.3s.\n","\tLR: 0.0196\n","Epoch 3 finished in 0.091s (avg.: 0.094s). Training for 0.4s.\n","\tLR: 0.01941\n","Epoch 4 finished in 0.087s (avg.: 0.093s). Training for 0.5s.\n","\tLR: 0.01921\n","Epoch 5 finished in 0.085s (avg.: 0.091s). Training for 0.6s.\n","\tLR: 0.01902\n","Epoch 6 finished in 0.092s (avg.: 0.091s). Training for 0.6s.\n","\tLR: 0.01883\n","Epoch 7 finished in 0.090s (avg.: 0.091s). Training for 0.7s.\n","\tLR: 0.01864\n","Epoch 8 finished in 0.088s (avg.: 0.091s). Training for 0.8s.\n","\tLR: 0.01845\n","Epoch 9 finished in 0.092s (avg.: 0.091s). Training for 0.9s.\n","\tLR: 0.01827\n","Epoch 10 finished in 0.084s (avg.: 0.090s). Training for 1.0s.\n","\tLR: 0.01809\n","Epoch 11 finished in 0.087s (avg.: 0.090s). Training for 1.1s.\n","\tLR: 0.01791\n","Epoch 12 finished in 0.085s (avg.: 0.090s). Training for 1.2s.\n","\tLR: 0.01773\n","Epoch 13 finished in 0.088s (avg.: 0.090s). Training for 1.3s.\n","\tLR: 0.01755\n","Epoch 14 finished in 0.092s (avg.: 0.090s). Training for 1.4s.\n","\tLR: 0.01737\n","Epoch 15 finished in 0.094s (avg.: 0.090s). Training for 1.5s.\n","\tLR: 0.0172\n","Epoch 16 finished in 0.091s (avg.: 0.090s). Training for 1.5s.\n","\tLR: 0.01703\n","Epoch 17 finished in 0.086s (avg.: 0.090s). Training for 1.6s.\n","\tLR: 0.01686\n","Epoch 18 finished in 0.098s (avg.: 0.090s). Training for 1.7s.\n","\tLR: 0.01669\n","Epoch 19 finished in 0.090s (avg.: 0.090s). Training for 1.8s.\n","\tLR: 0.01652\n","57141 60000\n","\ttrain loss: 0.156604, accuracy: 0.9524\n","9556 10000\n","\ttest  loss: 0.138592, accuracy: 0.9556\n","Epoch 20 finished in 0.093s (avg.: 0.090s). Training for 3.3s.\n","\tLR: 0.01636\n","Epoch 21 finished in 0.094s (avg.: 0.091s). Training for 3.4s.\n","\tLR: 0.01619\n","Epoch 22 finished in 0.087s (avg.: 0.090s). Training for 3.5s.\n","\tLR: 0.01603\n","Epoch 23 finished in 0.087s (avg.: 0.090s). Training for 3.6s.\n","\tLR: 0.01587\n","Epoch 24 finished in 0.088s (avg.: 0.090s). Training for 3.7s.\n","\tLR: 0.01571\n","Epoch 25 finished in 0.094s (avg.: 0.090s). Training for 3.8s.\n","\tLR: 0.01556\n","Epoch 26 finished in 0.085s (avg.: 0.090s). Training for 3.8s.\n","\tLR: 0.0154\n","Epoch 27 finished in 0.097s (avg.: 0.090s). Training for 3.9s.\n","\tLR: 0.01525\n","Epoch 28 finished in 0.084s (avg.: 0.090s). Training for 4.0s.\n","\tLR: 0.01509\n","Epoch 29 finished in 0.086s (avg.: 0.090s). Training for 4.1s.\n","\tLR: 0.01494\n","Epoch 30 finished in 0.097s (avg.: 0.090s). Training for 4.2s.\n","\tLR: 0.01479\n","Epoch 31 finished in 0.087s (avg.: 0.090s). Training for 4.3s.\n","\tLR: 0.01465\n","Epoch 32 finished in 0.087s (avg.: 0.090s). Training for 4.4s.\n","\tLR: 0.0145\n","Epoch 33 finished in 0.088s (avg.: 0.090s). Training for 4.5s.\n","\tLR: 0.01435\n","Epoch 34 finished in 0.074s (avg.: 0.090s). Training for 4.5s.\n","\tLR: 0.01421\n","Epoch 35 finished in 0.088s (avg.: 0.090s). Training for 4.6s.\n","\tLR: 0.01407\n","Epoch 36 finished in 0.078s (avg.: 0.089s). Training for 4.7s.\n","\tLR: 0.01393\n","Epoch 37 finished in 0.092s (avg.: 0.089s). Training for 4.8s.\n","\tLR: 0.01379\n","Epoch 38 finished in 0.086s (avg.: 0.089s). Training for 4.9s.\n","\tLR: 0.01365\n","Epoch 39 finished in 0.085s (avg.: 0.089s). Training for 5.0s.\n","\tLR: 0.01351\n","57667 60000\n","\ttrain loss: 0.132656, accuracy: 0.9611\n","9651 10000\n","\ttest  loss: 0.116595, accuracy: 0.9651\n","Epoch 40 finished in 0.085s (avg.: 0.089s). Training for 6.4s.\n","\tLR: 0.01338\n","Epoch 41 finished in 0.086s (avg.: 0.089s). Training for 6.5s.\n","\tLR: 0.01325\n","Epoch 42 finished in 0.088s (avg.: 0.089s). Training for 6.6s.\n","\tLR: 0.01311\n","Epoch 43 finished in 0.085s (avg.: 0.089s). Training for 6.7s.\n","\tLR: 0.01298\n","Epoch 44 finished in 0.083s (avg.: 0.089s). Training for 6.8s.\n","\tLR: 0.01285\n","Epoch 45 finished in 0.091s (avg.: 0.089s). Training for 6.9s.\n","\tLR: 0.01272\n","Epoch 46 finished in 0.130s (avg.: 0.090s). Training for 7.0s.\n","\tLR: 0.0126\n","Epoch 47 finished in 0.116s (avg.: 0.090s). Training for 7.1s.\n","\tLR: 0.01247\n","Epoch 48 finished in 0.137s (avg.: 0.091s). Training for 7.2s.\n","\tLR: 0.01235\n","Epoch 49 finished in 0.144s (avg.: 0.092s). Training for 7.4s.\n","\tLR: 0.01222\n","Epoch 50 finished in 0.138s (avg.: 0.093s). Training for 7.5s.\n","\tLR: 0.0121\n","Epoch 51 finished in 0.126s (avg.: 0.094s). Training for 7.7s.\n","\tLR: 0.01198\n","Epoch 52 finished in 0.137s (avg.: 0.095s). Training for 7.8s.\n","\tLR: 0.01186\n","Epoch 53 finished in 0.131s (avg.: 0.095s). Training for 7.9s.\n","\tLR: 0.01174\n","Epoch 54 finished in 0.115s (avg.: 0.096s). Training for 8.0s.\n","\tLR: 0.01162\n","Epoch 55 finished in 0.146s (avg.: 0.096s). Training for 8.2s.\n","\tLR: 0.01151\n","Epoch 56 finished in 0.129s (avg.: 0.097s). Training for 8.3s.\n","\tLR: 0.01139\n","Epoch 57 finished in 0.141s (avg.: 0.098s). Training for 8.5s.\n","\tLR: 0.01128\n","Epoch 58 finished in 0.132s (avg.: 0.098s). Training for 8.6s.\n","\tLR: 0.01117\n","Epoch 59 finished in 0.139s (avg.: 0.099s). Training for 8.7s.\n","\tLR: 0.01105\n","57803 60000\n","\ttrain loss: 0.122654, accuracy: 0.9634\n","9670 10000\n","\ttest  loss: 0.109186, accuracy: 0.9670\n","Epoch 60 finished in 0.083s (avg.: 0.099s). Training for 10.4s.\n","\tLR: 0.01094\n","Epoch 61 finished in 0.086s (avg.: 0.099s). Training for 10.5s.\n","\tLR: 0.01083\n","Epoch 62 finished in 0.106s (avg.: 0.099s). Training for 10.6s.\n","\tLR: 0.01073\n","Epoch 63 finished in 0.081s (avg.: 0.098s). Training for 10.7s.\n","\tLR: 0.01062\n","Epoch 64 finished in 0.089s (avg.: 0.098s). Training for 10.8s.\n","\tLR: 0.01051\n","Epoch 65 finished in 0.084s (avg.: 0.098s). Training for 10.8s.\n","\tLR: 0.01041\n","Epoch 66 finished in 0.111s (avg.: 0.098s). Training for 10.9s.\n","\tLR: 0.0103\n","Epoch 67 finished in 0.082s (avg.: 0.098s). Training for 11.0s.\n","\tLR: 0.0102\n","Epoch 68 finished in 0.087s (avg.: 0.098s). Training for 11.1s.\n","\tLR: 0.0101\n","Epoch 69 finished in 0.096s (avg.: 0.098s). Training for 11.2s.\n","\tLR: 0.009997\n","Epoch 70 finished in 0.086s (avg.: 0.098s). Training for 11.3s.\n","\tLR: 0.009897\n","Epoch 71 finished in 0.090s (avg.: 0.098s). Training for 11.4s.\n","\tLR: 0.009798\n","Epoch 72 finished in 0.090s (avg.: 0.097s). Training for 11.5s.\n","\tLR: 0.0097\n","Epoch 73 finished in 0.087s (avg.: 0.097s). Training for 11.6s.\n","\tLR: 0.009603\n","Epoch 74 finished in 0.094s (avg.: 0.097s). Training for 11.7s.\n","\tLR: 0.009507\n","Epoch 75 finished in 0.084s (avg.: 0.097s). Training for 11.7s.\n","\tLR: 0.009412\n","Epoch 76 finished in 0.086s (avg.: 0.097s). Training for 11.8s.\n","\tLR: 0.009318\n","Epoch 77 finished in 0.088s (avg.: 0.097s). Training for 11.9s.\n","\tLR: 0.009224\n","Epoch 78 finished in 0.089s (avg.: 0.097s). Training for 12.0s.\n","\tLR: 0.009132\n","Epoch 79 finished in 0.085s (avg.: 0.097s). Training for 12.1s.\n","\tLR: 0.009041\n","57927 60000\n","\ttrain loss: 0.114098, accuracy: 0.9655\n","9697 10000\n","\ttest  loss: 0.100459, accuracy: 0.9697\n","Epoch 80 finished in 0.090s (avg.: 0.097s). Training for 13.6s.\n","\tLR: 0.00895\n","Epoch 81 finished in 0.091s (avg.: 0.096s). Training for 13.7s.\n","\tLR: 0.008861\n","Epoch 82 finished in 0.085s (avg.: 0.096s). Training for 13.8s.\n","\tLR: 0.008772\n","Epoch 83 finished in 0.083s (avg.: 0.096s). Training for 13.9s.\n","\tLR: 0.008685\n","Epoch 84 finished in 0.087s (avg.: 0.096s). Training for 13.9s.\n","\tLR: 0.008598\n","Epoch 85 finished in 0.101s (avg.: 0.096s). Training for 14.0s.\n","\tLR: 0.008512\n","Epoch 86 finished in 0.089s (avg.: 0.096s). Training for 14.1s.\n","\tLR: 0.008427\n","Epoch 87 finished in 0.087s (avg.: 0.096s). Training for 14.2s.\n","\tLR: 0.008342\n","Epoch 88 finished in 0.086s (avg.: 0.096s). Training for 14.3s.\n","\tLR: 0.008259\n","Epoch 89 finished in 0.097s (avg.: 0.096s). Training for 14.4s.\n","\tLR: 0.008176\n","Epoch 90 finished in 0.090s (avg.: 0.096s). Training for 14.5s.\n","\tLR: 0.008095\n","Epoch 91 finished in 0.090s (avg.: 0.096s). Training for 14.6s.\n","\tLR: 0.008014\n","Epoch 92 finished in 0.081s (avg.: 0.096s). Training for 14.7s.\n","\tLR: 0.007934\n","Epoch 93 finished in 0.090s (avg.: 0.096s). Training for 14.8s.\n","\tLR: 0.007854\n","Epoch 94 finished in 0.086s (avg.: 0.095s). Training for 14.8s.\n","\tLR: 0.007776\n","Epoch 95 finished in 0.098s (avg.: 0.095s). Training for 14.9s.\n","\tLR: 0.007698\n","Epoch 96 finished in 0.093s (avg.: 0.095s). Training for 15.0s.\n","\tLR: 0.007621\n","Epoch 97 finished in 0.083s (avg.: 0.095s). Training for 15.1s.\n","\tLR: 0.007545\n","Epoch 98 finished in 0.085s (avg.: 0.095s). Training for 15.2s.\n","\tLR: 0.007469\n","Epoch 99 finished in 0.085s (avg.: 0.095s). Training for 15.3s.\n","\tLR: 0.007395\n","58071 60000\n","\ttrain loss: 0.108368, accuracy: 0.9678\n","9698 10000\n","\ttest  loss: 0.096425, accuracy: 0.9698\n"]}]},{"cell_type":"markdown","source":["## Fine-tune the compressed_model2"],"metadata":{"id":"rnu_I8dSjxhv"}},{"cell_type":"code","source":["training_time = 0\n","epoch = 0\n","all_start_time = time.time()\n","epoch_time = AverageMeter()\n","\n","optimizer = torch.optim.SGD(compressed_model2.parameters(), lr, momentum=momentum, nesterov=True)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n","\n","for epoch in range(start_epoch, epochs):\n","    start_time = time.time()\n","    compressed_model2.train()\n","    for batch_idx, (x, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        x, target = x.cuda(), target.cuda()\n","        out = compressed_model2.forward(x)\n","        loss = torch.nn.functional.cross_entropy(out, target)\n","        loss.backward()\n","        optimizer.step()\n","        break\n","    end_time = time.time()\n","    epoch_time.update(end_time - start_time)\n","    training_time = end_time - all_start_time\n","    compressed_model2.eval()\n","    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n","    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n","    if (epoch+1) % print_freq == 0:\n","        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, compressed_model2)\n","        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n","        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model2)\n","        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n","    scheduler.step()\n","\n","    if checkpoint and (epoch+1) % checkpoint == 0:\n","        # create and save checkpoint here\n","        to_save = {}\n","        to_save['model_state'] = compressed_model2.state_dict()\n","        to_save['optimizer_state'] = optimizer.state_dict()\n","        to_save['lr'] = scheduler.get_last_lr()\n","        to_save['epoch'] = epoch + 1\n","        torch.save(to_save, './compressed_lenet5_v2_checkpoint.pth.tar')"],"metadata":{"id":"soUrac4zGBAz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693984985065,"user_tz":-540,"elapsed":17060,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"a38737a3-eaf9-4b81-e6db-3278c8bb90eb"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 finished in 0.090s (avg.: 0.090s). Training for 0.1s.\n","\tLR: 0.1\n","Epoch 1 finished in 0.087s (avg.: 0.089s). Training for 0.2s.\n","\tLR: 0.099\n","Epoch 2 finished in 0.081s (avg.: 0.086s). Training for 0.3s.\n","\tLR: 0.09801\n","Epoch 3 finished in 0.085s (avg.: 0.086s). Training for 0.3s.\n","\tLR: 0.09703\n","Epoch 4 finished in 0.083s (avg.: 0.085s). Training for 0.4s.\n","\tLR: 0.09606\n","Epoch 5 finished in 0.083s (avg.: 0.085s). Training for 0.5s.\n","\tLR: 0.0951\n","Epoch 6 finished in 0.095s (avg.: 0.086s). Training for 0.6s.\n","\tLR: 0.09415\n","Epoch 7 finished in 0.084s (avg.: 0.086s). Training for 0.7s.\n","\tLR: 0.09321\n","Epoch 8 finished in 0.104s (avg.: 0.088s). Training for 0.8s.\n","\tLR: 0.09227\n","Epoch 9 finished in 0.093s (avg.: 0.088s). Training for 0.9s.\n","\tLR: 0.09135\n","Epoch 10 finished in 0.083s (avg.: 0.088s). Training for 1.0s.\n","\tLR: 0.09044\n","Epoch 11 finished in 0.094s (avg.: 0.088s). Training for 1.1s.\n","\tLR: 0.08953\n","Epoch 12 finished in 0.086s (avg.: 0.088s). Training for 1.2s.\n","\tLR: 0.08864\n","Epoch 13 finished in 0.094s (avg.: 0.089s). Training for 1.2s.\n","\tLR: 0.08775\n","Epoch 14 finished in 0.089s (avg.: 0.089s). Training for 1.3s.\n","\tLR: 0.08687\n","Epoch 15 finished in 0.082s (avg.: 0.088s). Training for 1.4s.\n","\tLR: 0.08601\n","Epoch 16 finished in 0.093s (avg.: 0.089s). Training for 1.5s.\n","\tLR: 0.08515\n","Epoch 17 finished in 0.096s (avg.: 0.089s). Training for 1.6s.\n","\tLR: 0.08429\n","Epoch 18 finished in 0.098s (avg.: 0.089s). Training for 1.7s.\n","\tLR: 0.08345\n","Epoch 19 finished in 0.093s (avg.: 0.090s). Training for 1.8s.\n","\tLR: 0.08262\n","38680 60000\n","\ttrain loss: 1.026245, accuracy: 0.6447\n","6486 10000\n","\ttest  loss: 1.011981, accuracy: 0.6486\n","Epoch 20 finished in 0.090s (avg.: 0.090s). Training for 3.3s.\n","\tLR: 0.08179\n","Epoch 21 finished in 0.090s (avg.: 0.090s). Training for 3.4s.\n","\tLR: 0.08097\n","Epoch 22 finished in 0.088s (avg.: 0.090s). Training for 3.5s.\n","\tLR: 0.08016\n","Epoch 23 finished in 0.088s (avg.: 0.090s). Training for 3.6s.\n","\tLR: 0.07936\n","Epoch 24 finished in 0.091s (avg.: 0.090s). Training for 3.7s.\n","\tLR: 0.07857\n","Epoch 25 finished in 0.093s (avg.: 0.090s). Training for 3.8s.\n","\tLR: 0.07778\n","Epoch 26 finished in 0.101s (avg.: 0.090s). Training for 3.9s.\n","\tLR: 0.077\n","Epoch 27 finished in 0.090s (avg.: 0.090s). Training for 4.0s.\n","\tLR: 0.07623\n","Epoch 28 finished in 0.091s (avg.: 0.090s). Training for 4.0s.\n","\tLR: 0.07547\n","Epoch 29 finished in 0.086s (avg.: 0.090s). Training for 4.1s.\n","\tLR: 0.07472\n","Epoch 30 finished in 0.091s (avg.: 0.090s). Training for 4.2s.\n","\tLR: 0.07397\n","Epoch 31 finished in 0.090s (avg.: 0.090s). Training for 4.3s.\n","\tLR: 0.07323\n","Epoch 32 finished in 0.083s (avg.: 0.090s). Training for 4.4s.\n","\tLR: 0.0725\n","Epoch 33 finished in 0.079s (avg.: 0.090s). Training for 4.5s.\n","\tLR: 0.07177\n","Epoch 34 finished in 0.103s (avg.: 0.090s). Training for 4.6s.\n","\tLR: 0.07106\n","Epoch 35 finished in 0.086s (avg.: 0.090s). Training for 4.7s.\n","\tLR: 0.07034\n","Epoch 36 finished in 0.082s (avg.: 0.090s). Training for 4.7s.\n","\tLR: 0.06964\n","Epoch 37 finished in 0.081s (avg.: 0.089s). Training for 4.8s.\n","\tLR: 0.06894\n","Epoch 38 finished in 0.095s (avg.: 0.090s). Training for 4.9s.\n","\tLR: 0.06826\n","Epoch 39 finished in 0.091s (avg.: 0.090s). Training for 5.0s.\n","\tLR: 0.06757\n","52432 60000\n","\ttrain loss: 0.447156, accuracy: 0.8739\n","8786 10000\n","\ttest  loss: 0.418368, accuracy: 0.8786\n","Epoch 40 finished in 0.092s (avg.: 0.090s). Training for 6.5s.\n","\tLR: 0.0669\n","Epoch 41 finished in 0.084s (avg.: 0.089s). Training for 6.6s.\n","\tLR: 0.06623\n","Epoch 42 finished in 0.081s (avg.: 0.089s). Training for 6.6s.\n","\tLR: 0.06557\n","Epoch 43 finished in 0.091s (avg.: 0.089s). Training for 6.7s.\n","\tLR: 0.06491\n","Epoch 44 finished in 0.090s (avg.: 0.089s). Training for 6.8s.\n","\tLR: 0.06426\n","Epoch 45 finished in 0.088s (avg.: 0.089s). Training for 6.9s.\n","\tLR: 0.06362\n","Epoch 46 finished in 0.095s (avg.: 0.089s). Training for 7.0s.\n","\tLR: 0.06298\n","Epoch 47 finished in 0.080s (avg.: 0.089s). Training for 7.1s.\n","\tLR: 0.06235\n","Epoch 48 finished in 0.076s (avg.: 0.089s). Training for 7.2s.\n","\tLR: 0.06173\n","Epoch 49 finished in 0.081s (avg.: 0.089s). Training for 7.2s.\n","\tLR: 0.06111\n","Epoch 50 finished in 0.093s (avg.: 0.089s). Training for 7.3s.\n","\tLR: 0.0605\n","Epoch 51 finished in 0.094s (avg.: 0.089s). Training for 7.4s.\n","\tLR: 0.0599\n","Epoch 52 finished in 0.085s (avg.: 0.089s). Training for 7.5s.\n","\tLR: 0.0593\n","Epoch 53 finished in 0.090s (avg.: 0.089s). Training for 7.6s.\n","\tLR: 0.0587\n","Epoch 54 finished in 0.088s (avg.: 0.089s). Training for 7.7s.\n","\tLR: 0.05812\n","Epoch 55 finished in 0.117s (avg.: 0.089s). Training for 7.8s.\n","\tLR: 0.05754\n","Epoch 56 finished in 0.140s (avg.: 0.090s). Training for 8.0s.\n","\tLR: 0.05696\n","Epoch 57 finished in 0.143s (avg.: 0.091s). Training for 8.1s.\n","\tLR: 0.05639\n","Epoch 58 finished in 0.123s (avg.: 0.092s). Training for 8.2s.\n","\tLR: 0.05583\n","Epoch 59 finished in 0.142s (avg.: 0.093s). Training for 8.4s.\n","\tLR: 0.05527\n","55123 60000\n","\ttrain loss: 0.289710, accuracy: 0.9187\n","9230 10000\n","\ttest  loss: 0.264705, accuracy: 0.9230\n","Epoch 60 finished in 0.077s (avg.: 0.092s). Training for 10.5s.\n","\tLR: 0.05472\n","Epoch 61 finished in 0.093s (avg.: 0.092s). Training for 10.6s.\n","\tLR: 0.05417\n","Epoch 62 finished in 0.084s (avg.: 0.092s). Training for 10.7s.\n","\tLR: 0.05363\n","Epoch 63 finished in 0.094s (avg.: 0.092s). Training for 10.8s.\n","\tLR: 0.05309\n","Epoch 64 finished in 0.086s (avg.: 0.092s). Training for 10.9s.\n","\tLR: 0.05256\n","Epoch 65 finished in 0.094s (avg.: 0.092s). Training for 11.0s.\n","\tLR: 0.05203\n","Epoch 66 finished in 0.096s (avg.: 0.092s). Training for 11.1s.\n","\tLR: 0.05151\n","Epoch 67 finished in 0.086s (avg.: 0.092s). Training for 11.2s.\n","\tLR: 0.051\n","Epoch 68 finished in 0.091s (avg.: 0.092s). Training for 11.2s.\n","\tLR: 0.05049\n","Epoch 69 finished in 0.087s (avg.: 0.092s). Training for 11.3s.\n","\tLR: 0.04998\n","Epoch 70 finished in 0.088s (avg.: 0.092s). Training for 11.4s.\n","\tLR: 0.04948\n","Epoch 71 finished in 0.098s (avg.: 0.092s). Training for 11.5s.\n","\tLR: 0.04899\n","Epoch 72 finished in 0.085s (avg.: 0.092s). Training for 11.6s.\n","\tLR: 0.0485\n","Epoch 73 finished in 0.085s (avg.: 0.092s). Training for 11.7s.\n","\tLR: 0.04801\n","Epoch 74 finished in 0.082s (avg.: 0.092s). Training for 11.8s.\n","\tLR: 0.04753\n","Epoch 75 finished in 0.095s (avg.: 0.092s). Training for 11.9s.\n","\tLR: 0.04706\n","Epoch 76 finished in 0.088s (avg.: 0.092s). Training for 12.0s.\n","\tLR: 0.04659\n","Epoch 77 finished in 0.092s (avg.: 0.092s). Training for 12.1s.\n","\tLR: 0.04612\n","Epoch 78 finished in 0.093s (avg.: 0.092s). Training for 12.1s.\n","\tLR: 0.04566\n","Epoch 79 finished in 0.087s (avg.: 0.092s). Training for 12.2s.\n","\tLR: 0.0452\n","56079 60000\n","\ttrain loss: 0.220044, accuracy: 0.9346\n","9388 10000\n","\ttest  loss: 0.196222, accuracy: 0.9388\n","Epoch 80 finished in 0.084s (avg.: 0.092s). Training for 13.7s.\n","\tLR: 0.04475\n","Epoch 81 finished in 0.096s (avg.: 0.092s). Training for 13.8s.\n","\tLR: 0.0443\n","Epoch 82 finished in 0.087s (avg.: 0.092s). Training for 13.9s.\n","\tLR: 0.04386\n","Epoch 83 finished in 0.093s (avg.: 0.092s). Training for 14.0s.\n","\tLR: 0.04342\n","Epoch 84 finished in 0.091s (avg.: 0.092s). Training for 14.1s.\n","\tLR: 0.04299\n","Epoch 85 finished in 0.085s (avg.: 0.092s). Training for 14.2s.\n","\tLR: 0.04256\n","Epoch 86 finished in 0.093s (avg.: 0.092s). Training for 14.3s.\n","\tLR: 0.04213\n","Epoch 87 finished in 0.085s (avg.: 0.091s). Training for 14.4s.\n","\tLR: 0.04171\n","Epoch 88 finished in 0.085s (avg.: 0.091s). Training for 14.5s.\n","\tLR: 0.04129\n","Epoch 89 finished in 0.087s (avg.: 0.091s). Training for 14.6s.\n","\tLR: 0.04088\n","Epoch 90 finished in 0.090s (avg.: 0.091s). Training for 14.6s.\n","\tLR: 0.04047\n","Epoch 91 finished in 0.083s (avg.: 0.091s). Training for 14.7s.\n","\tLR: 0.04007\n","Epoch 92 finished in 0.103s (avg.: 0.091s). Training for 14.8s.\n","\tLR: 0.03967\n","Epoch 93 finished in 0.087s (avg.: 0.091s). Training for 14.9s.\n","\tLR: 0.03927\n","Epoch 94 finished in 0.088s (avg.: 0.091s). Training for 15.0s.\n","\tLR: 0.03888\n","Epoch 95 finished in 0.089s (avg.: 0.091s). Training for 15.1s.\n","\tLR: 0.03849\n","Epoch 96 finished in 0.092s (avg.: 0.091s). Training for 15.2s.\n","\tLR: 0.0381\n","Epoch 97 finished in 0.088s (avg.: 0.091s). Training for 15.3s.\n","\tLR: 0.03772\n","Epoch 98 finished in 0.095s (avg.: 0.091s). Training for 15.4s.\n","\tLR: 0.03735\n","Epoch 99 finished in 0.089s (avg.: 0.091s). Training for 15.5s.\n","\tLR: 0.03697\n","56569 60000\n","\ttrain loss: 0.195686, accuracy: 0.9428\n","9443 10000\n","\ttest  loss: 0.175795, accuracy: 0.9443\n"]}]},{"cell_type":"markdown","source":["## Fine-tune the compressed_model3"],"metadata":{"id":"uHBCHKlwjzBl"}},{"cell_type":"code","source":["training_time = 0\n","epoch = 0\n","all_start_time = time.time()\n","epoch_time = AverageMeter()\n","\n","optimizer = torch.optim.SGD(compressed_model3.parameters(), lr, momentum=momentum, nesterov=True)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n","\n","for epoch in range(start_epoch, epochs):\n","    start_time = time.time()\n","    compressed_model3.train()\n","    for batch_idx, (x, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        x, target = x.cuda(), target.cuda()\n","        out = compressed_model3.forward(x)\n","        loss = torch.nn.functional.cross_entropy(out, target)\n","        loss.backward()\n","        optimizer.step()\n","        break\n","    end_time = time.time()\n","    epoch_time.update(end_time - start_time)\n","    training_time = end_time - all_start_time\n","    compressed_model3.eval()\n","    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n","    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n","    if (epoch+1) % print_freq == 0:\n","        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, compressed_model3)\n","        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n","        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model3)\n","        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n","    scheduler.step()\n","\n","    if checkpoint and (epoch+1) % checkpoint == 0:\n","        # create and save checkpoint here\n","        to_save = {}\n","        to_save['model_state'] = compressed_model3.state_dict()\n","        to_save['optimizer_state'] = optimizer.state_dict()\n","        to_save['lr'] = scheduler.get_last_lr()\n","        to_save['epoch'] = epoch + 1\n","        torch.save(to_save, './compressed_lenet5_v3_checkpoint.pth.tar')"],"metadata":{"id":"opTbHeSFhSSs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693985008899,"user_tz":-540,"elapsed":17907,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"4de9a5a1-6ad1-4892-aad0-3c3db4974c5e"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 finished in 0.164s (avg.: 0.164s). Training for 0.2s.\n","\tLR: 0.1\n","Epoch 1 finished in 0.147s (avg.: 0.155s). Training for 0.3s.\n","\tLR: 0.099\n","Epoch 2 finished in 0.180s (avg.: 0.164s). Training for 0.5s.\n","\tLR: 0.09801\n","Epoch 3 finished in 0.201s (avg.: 0.173s). Training for 0.7s.\n","\tLR: 0.09703\n","Epoch 4 finished in 0.148s (avg.: 0.168s). Training for 0.8s.\n","\tLR: 0.09606\n","Epoch 5 finished in 0.133s (avg.: 0.162s). Training for 1.0s.\n","\tLR: 0.0951\n","Epoch 6 finished in 0.162s (avg.: 0.162s). Training for 1.1s.\n","\tLR: 0.09415\n","Epoch 7 finished in 0.174s (avg.: 0.164s). Training for 1.3s.\n","\tLR: 0.09321\n","Epoch 8 finished in 0.183s (avg.: 0.166s). Training for 1.5s.\n","\tLR: 0.09227\n","Epoch 9 finished in 0.188s (avg.: 0.168s). Training for 1.7s.\n","\tLR: 0.09135\n","Epoch 10 finished in 0.160s (avg.: 0.167s). Training for 1.8s.\n","\tLR: 0.09044\n","Epoch 11 finished in 0.164s (avg.: 0.167s). Training for 2.0s.\n","\tLR: 0.08953\n","Epoch 12 finished in 0.110s (avg.: 0.163s). Training for 2.1s.\n","\tLR: 0.08864\n","Epoch 13 finished in 0.087s (avg.: 0.157s). Training for 2.2s.\n","\tLR: 0.08775\n","Epoch 14 finished in 0.102s (avg.: 0.154s). Training for 2.3s.\n","\tLR: 0.08687\n","Epoch 15 finished in 0.096s (avg.: 0.150s). Training for 2.4s.\n","\tLR: 0.08601\n","Epoch 16 finished in 0.091s (avg.: 0.146s). Training for 2.5s.\n","\tLR: 0.08515\n","Epoch 17 finished in 0.086s (avg.: 0.143s). Training for 2.6s.\n","\tLR: 0.08429\n","Epoch 18 finished in 0.090s (avg.: 0.140s). Training for 2.7s.\n","\tLR: 0.08345\n","Epoch 19 finished in 0.089s (avg.: 0.138s). Training for 2.8s.\n","\tLR: 0.08262\n","36629 60000\n","\ttrain loss: 1.063103, accuracy: 0.6105\n","6226 10000\n","\ttest  loss: 1.049302, accuracy: 0.6226\n","Epoch 20 finished in 0.087s (avg.: 0.135s). Training for 4.2s.\n","\tLR: 0.08179\n","Epoch 21 finished in 0.083s (avg.: 0.133s). Training for 4.3s.\n","\tLR: 0.08097\n","Epoch 22 finished in 0.091s (avg.: 0.131s). Training for 4.3s.\n","\tLR: 0.08016\n","Epoch 23 finished in 0.095s (avg.: 0.130s). Training for 4.4s.\n","\tLR: 0.07936\n","Epoch 24 finished in 0.085s (avg.: 0.128s). Training for 4.5s.\n","\tLR: 0.07857\n","Epoch 25 finished in 0.094s (avg.: 0.127s). Training for 4.6s.\n","\tLR: 0.07778\n","Epoch 26 finished in 0.088s (avg.: 0.125s). Training for 4.7s.\n","\tLR: 0.077\n","Epoch 27 finished in 0.088s (avg.: 0.124s). Training for 4.8s.\n","\tLR: 0.07623\n","Epoch 28 finished in 0.089s (avg.: 0.123s). Training for 4.9s.\n","\tLR: 0.07547\n","Epoch 29 finished in 0.095s (avg.: 0.122s). Training for 5.0s.\n","\tLR: 0.07472\n","Epoch 30 finished in 0.084s (avg.: 0.120s). Training for 5.1s.\n","\tLR: 0.07397\n","Epoch 31 finished in 0.088s (avg.: 0.119s). Training for 5.2s.\n","\tLR: 0.07323\n","Epoch 32 finished in 0.085s (avg.: 0.118s). Training for 5.2s.\n","\tLR: 0.0725\n","Epoch 33 finished in 0.084s (avg.: 0.117s). Training for 5.3s.\n","\tLR: 0.07177\n","Epoch 34 finished in 0.098s (avg.: 0.117s). Training for 5.4s.\n","\tLR: 0.07106\n","Epoch 35 finished in 0.086s (avg.: 0.116s). Training for 5.5s.\n","\tLR: 0.07034\n","Epoch 36 finished in 0.079s (avg.: 0.115s). Training for 5.6s.\n","\tLR: 0.06964\n","Epoch 37 finished in 0.085s (avg.: 0.114s). Training for 5.7s.\n","\tLR: 0.06894\n","Epoch 38 finished in 0.091s (avg.: 0.114s). Training for 5.8s.\n","\tLR: 0.06826\n","Epoch 39 finished in 0.097s (avg.: 0.113s). Training for 5.9s.\n","\tLR: 0.06757\n","49038 60000\n","\ttrain loss: 0.577741, accuracy: 0.8173\n","8224 10000\n","\ttest  loss: 0.556744, accuracy: 0.8224\n","Epoch 40 finished in 0.083s (avg.: 0.112s). Training for 7.2s.\n","\tLR: 0.0669\n","Epoch 41 finished in 0.091s (avg.: 0.112s). Training for 7.3s.\n","\tLR: 0.06623\n","Epoch 42 finished in 0.082s (avg.: 0.111s). Training for 7.4s.\n","\tLR: 0.06557\n","Epoch 43 finished in 0.098s (avg.: 0.111s). Training for 7.5s.\n","\tLR: 0.06491\n","Epoch 44 finished in 0.087s (avg.: 0.110s). Training for 7.6s.\n","\tLR: 0.06426\n","Epoch 45 finished in 0.085s (avg.: 0.110s). Training for 7.7s.\n","\tLR: 0.06362\n","Epoch 46 finished in 0.085s (avg.: 0.109s). Training for 7.8s.\n","\tLR: 0.06298\n","Epoch 47 finished in 0.084s (avg.: 0.109s). Training for 7.9s.\n","\tLR: 0.06235\n","Epoch 48 finished in 0.082s (avg.: 0.108s). Training for 7.9s.\n","\tLR: 0.06173\n","Epoch 49 finished in 0.086s (avg.: 0.108s). Training for 8.0s.\n","\tLR: 0.06111\n","Epoch 50 finished in 0.091s (avg.: 0.107s). Training for 8.1s.\n","\tLR: 0.0605\n","Epoch 51 finished in 0.089s (avg.: 0.107s). Training for 8.2s.\n","\tLR: 0.0599\n","Epoch 52 finished in 0.085s (avg.: 0.107s). Training for 8.3s.\n","\tLR: 0.0593\n","Epoch 53 finished in 0.127s (avg.: 0.107s). Training for 8.4s.\n","\tLR: 0.0587\n","Epoch 54 finished in 0.152s (avg.: 0.108s). Training for 8.6s.\n","\tLR: 0.05812\n","Epoch 55 finished in 0.133s (avg.: 0.108s). Training for 8.7s.\n","\tLR: 0.05754\n","Epoch 56 finished in 0.184s (avg.: 0.110s). Training for 8.9s.\n","\tLR: 0.05696\n","Epoch 57 finished in 0.120s (avg.: 0.110s). Training for 9.0s.\n","\tLR: 0.05639\n","Epoch 58 finished in 0.137s (avg.: 0.110s). Training for 9.2s.\n","\tLR: 0.05583\n","Epoch 59 finished in 0.142s (avg.: 0.111s). Training for 9.3s.\n","\tLR: 0.05527\n","51968 60000\n","\ttrain loss: 0.438999, accuracy: 0.8661\n","8744 10000\n","\ttest  loss: 0.406322, accuracy: 0.8744\n","Epoch 60 finished in 0.100s (avg.: 0.111s). Training for 11.3s.\n","\tLR: 0.05472\n","Epoch 61 finished in 0.087s (avg.: 0.110s). Training for 11.4s.\n","\tLR: 0.05417\n","Epoch 62 finished in 0.081s (avg.: 0.110s). Training for 11.5s.\n","\tLR: 0.05363\n","Epoch 63 finished in 0.094s (avg.: 0.110s). Training for 11.6s.\n","\tLR: 0.05309\n","Epoch 64 finished in 0.095s (avg.: 0.109s). Training for 11.7s.\n","\tLR: 0.05256\n","Epoch 65 finished in 0.091s (avg.: 0.109s). Training for 11.8s.\n","\tLR: 0.05203\n","Epoch 66 finished in 0.088s (avg.: 0.109s). Training for 11.9s.\n","\tLR: 0.05151\n","Epoch 67 finished in 0.093s (avg.: 0.108s). Training for 11.9s.\n","\tLR: 0.051\n","Epoch 68 finished in 0.086s (avg.: 0.108s). Training for 12.0s.\n","\tLR: 0.05049\n","Epoch 69 finished in 0.088s (avg.: 0.108s). Training for 12.1s.\n","\tLR: 0.04998\n","Epoch 70 finished in 0.089s (avg.: 0.108s). Training for 12.2s.\n","\tLR: 0.04948\n","Epoch 71 finished in 0.093s (avg.: 0.107s). Training for 12.3s.\n","\tLR: 0.04899\n","Epoch 72 finished in 0.092s (avg.: 0.107s). Training for 12.4s.\n","\tLR: 0.0485\n","Epoch 73 finished in 0.092s (avg.: 0.107s). Training for 12.5s.\n","\tLR: 0.04801\n","Epoch 74 finished in 0.094s (avg.: 0.107s). Training for 12.6s.\n","\tLR: 0.04753\n","Epoch 75 finished in 0.110s (avg.: 0.107s). Training for 12.7s.\n","\tLR: 0.04706\n","Epoch 76 finished in 0.098s (avg.: 0.107s). Training for 12.8s.\n","\tLR: 0.04659\n","Epoch 77 finished in 0.089s (avg.: 0.106s). Training for 12.9s.\n","\tLR: 0.04612\n","Epoch 78 finished in 0.086s (avg.: 0.106s). Training for 13.0s.\n","\tLR: 0.04566\n","Epoch 79 finished in 0.084s (avg.: 0.106s). Training for 13.0s.\n","\tLR: 0.0452\n","54489 60000\n","\ttrain loss: 0.315178, accuracy: 0.9082\n","9145 10000\n","\ttest  loss: 0.298721, accuracy: 0.9145\n","Epoch 80 finished in 0.086s (avg.: 0.106s). Training for 14.4s.\n","\tLR: 0.04475\n","Epoch 81 finished in 0.094s (avg.: 0.106s). Training for 14.5s.\n","\tLR: 0.0443\n","Epoch 82 finished in 0.091s (avg.: 0.105s). Training for 14.6s.\n","\tLR: 0.04386\n","Epoch 83 finished in 0.098s (avg.: 0.105s). Training for 14.7s.\n","\tLR: 0.04342\n","Epoch 84 finished in 0.086s (avg.: 0.105s). Training for 14.8s.\n","\tLR: 0.04299\n","Epoch 85 finished in 0.094s (avg.: 0.105s). Training for 14.9s.\n","\tLR: 0.04256\n","Epoch 86 finished in 0.086s (avg.: 0.105s). Training for 15.0s.\n","\tLR: 0.04213\n","Epoch 87 finished in 0.086s (avg.: 0.105s). Training for 15.1s.\n","\tLR: 0.04171\n","Epoch 88 finished in 0.098s (avg.: 0.104s). Training for 15.2s.\n","\tLR: 0.04129\n","Epoch 89 finished in 0.088s (avg.: 0.104s). Training for 15.3s.\n","\tLR: 0.04088\n","Epoch 90 finished in 0.093s (avg.: 0.104s). Training for 15.3s.\n","\tLR: 0.04047\n","Epoch 91 finished in 0.086s (avg.: 0.104s). Training for 15.4s.\n","\tLR: 0.04007\n","Epoch 92 finished in 0.097s (avg.: 0.104s). Training for 15.5s.\n","\tLR: 0.03967\n","Epoch 93 finished in 0.091s (avg.: 0.104s). Training for 15.6s.\n","\tLR: 0.03927\n","Epoch 94 finished in 0.091s (avg.: 0.104s). Training for 15.7s.\n","\tLR: 0.03888\n","Epoch 95 finished in 0.094s (avg.: 0.103s). Training for 15.8s.\n","\tLR: 0.03849\n","Epoch 96 finished in 0.092s (avg.: 0.103s). Training for 15.9s.\n","\tLR: 0.0381\n","Epoch 97 finished in 0.091s (avg.: 0.103s). Training for 16.0s.\n","\tLR: 0.03772\n","Epoch 98 finished in 0.091s (avg.: 0.103s). Training for 16.1s.\n","\tLR: 0.03735\n","Epoch 99 finished in 0.090s (avg.: 0.103s). Training for 16.2s.\n","\tLR: 0.03697\n","55146 60000\n","\ttrain loss: 0.285163, accuracy: 0.9191\n","9213 10000\n","\ttest  loss: 0.276601, accuracy: 0.9213\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cAZ9X1IjX6Qa"},"execution_count":null,"outputs":[]}]}