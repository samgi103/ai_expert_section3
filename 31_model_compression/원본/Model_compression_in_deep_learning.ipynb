{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practice: Model compression in Deep Learning"
      ],
      "metadata": {
        "id": "VWgdS661S03g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typical process of the low-rank compression\n",
        "#### Normal Training ⇒ Rank Selection *(excluded from this practice)* ⇒ Low-rank compression ⇒ Fine-tuning\n",
        "\n",
        "## Question\n",
        "1. 코드에서 빈 부분을 채우세요.\n",
        "2. 3가지의 rank setting에 대해서 성능 비교를 수행하세요.\n",
        " - R=[20, 100, 200, 8]\n",
        " - R=[15, 50, 100, 6]\n",
        " - R=[10, 10, 50, 3]\n",
        "3. 3개의 compressed model에 대해 Fine-tuning을 수행한 뒤 성능 비교를 수행하세요."
      ],
      "metadata": {
        "id": "R9yuhWKpS7EB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library import"
      ],
      "metadata": {
        "id": "5jR7stYXV1CX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "umb9o8VBa4Ap"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import OrderedDict\n",
        "from scipy.linalg import svd\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a model"
      ],
      "metadata": {
        "id": "bYhCoXQ-V_h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear):\n",
        "        xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, dropout, nonlinearity):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.special = True\n",
        "        filters = [(20, 5), (50, 5)]\n",
        "        layers = [(800, 500), (500, 10)]\n",
        "\n",
        "        cfg = []\n",
        "        cfg.append(['init_reshape', LambdaLayer(lambda x: x.view(x.size(0), 1,28,28))])\n",
        "        for i, f in enumerate(filters):\n",
        "            prev = 1 if i==0 else filters[i-1][0]\n",
        "            cfg.append(('compressible_' + str(i), nn.Conv2d(prev, f[0], f[1])))\n",
        "            cfg.append(('nonlineairy_'+str(i), nonlinearity()))\n",
        "            cfg.append(('maxpool_'+str(i), nn.MaxPool2d(kernel_size=(2,2), stride=2)))\n",
        "\n",
        "\n",
        "        cfg.append(['reshape', LambdaLayer(lambda x: x.view(x.size(0),-1))])\n",
        "        for i, l in enumerate(layers):\n",
        "            cfg.append(('compressible_' + str(i+len(filters)), nn.Linear(*l)))\n",
        "            if i != len(layers)-1:\n",
        "                # only non terminal layers have nonlinearity and (possible) dropouts\n",
        "                cfg.append(('nonlinearity_' + str(i+len(filters)), nonlinearity()))\n",
        "                if dropout:\n",
        "                    cfg.append(('drop_'+str(i+len(filters)), nn.Dropout()))\n",
        "\n",
        "        self.output = nn.Sequential(OrderedDict(cfg))\n",
        "        self.apply(_weights_init)\n",
        "    def forward(self, input):\n",
        "        h = self.output(input)\n",
        "        return h\n",
        "\n",
        "def lenet5_classic():\n",
        "    return LeNet5(dropout=False, nonlinearity=lambda: nn.ReLU(True))"
      ],
      "metadata": {
        "id": "19Q4sWh80Kxu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define datasets"
      ],
      "metadata": {
        "id": "n-y5CX01Wesp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'MNIST'\n",
        "batch_size = 256\n",
        "n_workers = 2\n",
        "\n",
        "def mnist_data():\n",
        "    mnist_train = torchvision.datasets.MNIST(root='./datasets/', train=True, download=True)\n",
        "    mnist_test = torchvision.datasets.MNIST(root='./datasets/', train=False, download=True)\n",
        "\n",
        "    train_data = mnist_train.data.to(torch.float) / 255.\n",
        "    test_data = mnist_test.data.to(torch.float) / 255.\n",
        "    mean_image = torch.mean(train_data, dim=0)\n",
        "\n",
        "    train_data -= mean_image\n",
        "    test_data -= mean_image\n",
        "\n",
        "    train_labels = mnist_train.targets\n",
        "    test_labels = mnist_test.targets\n",
        "\n",
        "    our_mnist = {\n",
        "        'train_data': train_data, 'test_data': test_data,\n",
        "        'train_labels': train_labels, 'test_labels': test_labels\n",
        "    }\n",
        "    return our_mnist\n",
        "\n",
        "data = mnist_data()\n",
        "train_data = TensorDataset(data['train_data'], data['train_labels'])\n",
        "test_data = TensorDataset(data['test_data'], data['test_labels'])\n",
        "\n",
        "train_loader = DataLoader(train_data, num_workers=n_workers, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
        "test_loader = DataLoader(test_data, num_workers=n_workers, batch_size=batch_size, shuffle=False, pin_memory=False)\n"
      ],
      "metadata": {
        "id": "okdiL5xRWePh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Training"
      ],
      "metadata": {
        "id": "ymYXA20XWIMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a model\n",
        "model = lenet5_classic()\n",
        "model.cuda()\n",
        "print(model)\n",
        "\n",
        "# Hyper-parameters for training\n",
        "lr = 0.1\n",
        "lr_decay = 0.99\n",
        "momentum = 0.9\n",
        "epochs = 100\n",
        "start_epoch = 0\n",
        "print_freq = 20\n",
        "checkpoint = 20\n",
        "\n",
        "# Define an optimizer and a scheduler\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n",
        "\n",
        "# Define the funcions required to training\n",
        "def my_eval(x, target, model):\n",
        "    out_ = model.forward(x)\n",
        "    return out_, torch.nn.functional.cross_entropy(out_, target)\n",
        "\n",
        "def format_time(seconds):\n",
        "    if seconds < 60:\n",
        "        return '{:.1f}s.'.format(seconds)\n",
        "    if seconds < 3600:\n",
        "        return '{:d}m. {}'.format(int(seconds//60), format_time(seconds%60))\n",
        "    if seconds < 3600*24:\n",
        "        return '{:d}h. {}'.format(int(seconds//3600), format_time(seconds%3600))\n",
        "    return '{:d}d. {}'.format(int(seconds//(3600*24)), format_time(seconds%(3600*24)))\n",
        "\n",
        "def compute_acc_loss(forward_func, data_loader, model):\n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    for batch_idx, (x, target) in enumerate(data_loader):\n",
        "        with torch.no_grad():\n",
        "            target = target.cuda()\n",
        "            score, loss = forward_func(x.cuda(), target, model)\n",
        "            _, pred_label = torch.max(score.data, 1)\n",
        "            correct_cnt += (pred_label == target.data).sum().item()\n",
        "            ave_loss += loss.data.item() * len(x)\n",
        "    accuracy = correct_cnt * 1.0 / len(data_loader.dataset)\n",
        "    print(correct_cnt, len(data_loader.dataset))\n",
        "    ave_loss /= len(data_loader.dataset)\n",
        "    return accuracy, ave_loss\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0.0\n",
        "        self.avg = 0.0\n",
        "        self.sum = 0.0\n",
        "        self.count = 0.0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "# Training\n",
        "training_time = 0\n",
        "epoch = 0\n",
        "all_start_time = time.time()\n",
        "epoch_time = AverageMeter()\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = model.forward(x)\n",
        "        loss = torch.nn.functional.cross_entropy(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "    end_time = time.time()\n",
        "    epoch_time.update(end_time - start_time)\n",
        "    training_time = end_time - all_start_time\n",
        "    model.eval()\n",
        "    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n",
        "    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n",
        "    if (epoch+1) % print_freq == 0:\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, model)\n",
        "        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, model)\n",
        "        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "    scheduler.step()\n",
        "\n",
        "    if checkpoint and (epoch+1) % checkpoint == 0:\n",
        "        # create and save checkpoint here\n",
        "        to_save = {}\n",
        "        to_save['model_state'] = model.state_dict()\n",
        "        to_save['optimizer_state'] = optimizer.state_dict()\n",
        "        to_save['lr'] = scheduler.get_last_lr()\n",
        "        to_save['epoch'] = epoch + 1\n",
        "        torch.save(to_save, './lenet5_checkpoint.pth.tar')"
      ],
      "metadata": {
        "id": "CqBzq36B0Yy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rank Selection"
      ],
      "metadata": {
        "id": "-FMj3wsai6NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_rank1=[10, 10, 15, 7]\n",
        "selected_rank2=[5, 5, 7, 5]\n",
        "selected_rank3=[3, 3, 5, 3]"
      ],
      "metadata": {
        "id": "--beybIo53uM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the functions for low-rank compression"
      ],
      "metadata": {
        "id": "4gcxAgt3jCuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_layer_reparametrizer(sub_module, conv_scheme='scheme_1'):\n",
        "    W = sub_module.weight.data.cpu().numpy()\n",
        "\n",
        "    init_shape = None\n",
        "    n,m,d1,d2 = None, None, None, None\n",
        "    if isinstance(sub_module, nn.Conv2d):\n",
        "        if conv_scheme == 'scheme_1':\n",
        "            ####################\n",
        "            # implement here\n",
        "            ####################\n",
        "        elif conv_scheme == 'scheme_2':\n",
        "            raise NotImplementedError(\"We did not implement scheme-2 in this pratice.\")\n",
        "\n",
        "    u, s, v = svd(W, full_matrices=False)\n",
        "    from numpy.linalg import matrix_rank\n",
        "\n",
        "    r = sub_module.rank_ if hasattr(sub_module, 'rank_') else sub_module.selected_rank_ if hasattr(sub_module, 'selected_rank_') else int(matrix_rank(W))\n",
        "\n",
        "    if r < np.min(W.shape):\n",
        "        diag = np.diag(s[:r] ** 0.5)\n",
        "        U = u[:, :r] @ diag\n",
        "        V = diag @ v[:r, :]\n",
        "        new_W = U @ V\n",
        "\n",
        "\n",
        "        from numpy.linalg import norm\n",
        "        m,n = W.shape\n",
        "        if r > np.floor(m*n/(m+n)):\n",
        "            raise RankNotEfficientException(\"Selected rank doesn't contribute to any savings\")\n",
        "        bias = sub_module.bias is not None\n",
        "        if isinstance(sub_module, nn.Linear):\n",
        "            ####################\n",
        "            # implement here\n",
        "            ####################\n",
        "            l1.weight.data = torch.from_numpy(V)\n",
        "            l2.weight.data = torch.from_numpy(U)\n",
        "            if bias:\n",
        "                l2.bias.data = sub_module.bias.data\n",
        "            return l1, l2\n",
        "        else:\n",
        "            if conv_scheme == 'scheme_1':\n",
        "                ####################\n",
        "                # implement here\n",
        "                ####################\n",
        "                l1.weight.data = torch.from_numpy(V.reshape([-1, *init_shape[1:]]))\n",
        "                l2.weight.data = torch.from_numpy(U[:, :, None, None])\n",
        "\n",
        "                if bias:\n",
        "                    l2.bias.data = sub_module.bias.data\n",
        "\n",
        "                return l1, l2\n",
        "            elif conv_scheme == 'scheme_2':\n",
        "                raise NotImplementedError(\"We did not implement scheme-2 in this pratice.\")\n",
        "\n",
        "\n",
        "def reparametrization_helper(list_of_modules, conv_scheme, old_weight_decay=True):\n",
        "    new_sequence = []\n",
        "    items = list_of_modules.items()\n",
        "    decayed_values_repar = []\n",
        "    decayed_valued_old = []\n",
        "    for i, (name, sub_module) in enumerate(items):\n",
        "        if isinstance(sub_module, nn.Sequential):\n",
        "            dv_repar_sub, dv_old_sub, nseq_sub = reparametrization_helper(sub_module._modules, conv_scheme=conv_scheme,old_weight_decay=old_weight_decay)\n",
        "            new_sequence.append((name, nn.Sequential(OrderedDict(nseq_sub))))\n",
        "            decayed_values_repar.extend(dv_repar_sub)\n",
        "            decayed_valued_old.extend(dv_old_sub)\n",
        "        elif isinstance(sub_module, nn.Linear) or isinstance(sub_module, nn.Conv2d):\n",
        "            try:\n",
        "                l1, l2 = linear_layer_reparametrizer(sub_module, conv_scheme=conv_scheme)\n",
        "                new_sequence.append((name + '_V', l1))\n",
        "                new_sequence.append((name + '_U', l2))\n",
        "                decayed_values_repar.append((l1, l2))\n",
        "\n",
        "            except Exception as e:\n",
        "                new_sequence.append((name, sub_module))\n",
        "                decayed_valued_old.append(sub_module.weight)\n",
        "        else:\n",
        "            new_sequence.append((name, sub_module))\n",
        "            if old_weight_decay and hasattr(sub_module, 'weight'):\n",
        "                decayed_valued_old.append(sub_module.weight)\n",
        "    return decayed_values_repar, decayed_valued_old, new_sequence\n",
        "\n",
        "\n",
        "def reparametrize_low_rank(model, old_weight_decay=True):\n",
        "    decayed_values_repar, decayed_valued_old, new_sequence = reparametrization_helper(model.output._modules, conv_scheme='scheme_1', old_weight_decay=old_weight_decay)\n",
        "    model.output = nn.Sequential(OrderedDict(new_sequence))\n",
        "\n",
        "    def weight_decay():\n",
        "        sum_ = torch.autograd.Variable(torch.FloatTensor([0.0]).cuda())\n",
        "        for x in decayed_valued_old:\n",
        "            sum_ += torch.sum(x**2)\n",
        "        for v,u in decayed_values_repar:\n",
        "            v = v.weight\n",
        "            u = u.weight\n",
        "            u_ = u.view(u.size()[0], -1)\n",
        "            v_ = v.view(u_.size()[1], -1)\n",
        "            sum_ += torch.sum(torch.matmul(u_,v_)**2)\n",
        "        return sum_\n",
        "    model.weight_decay = weight_decay\n",
        "    return nn.Sequential(OrderedDict(new_sequence))"
      ],
      "metadata": {
        "id": "OYEwODYk535Z"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compress the model(i.e., compressed_model1) using first ranks"
      ],
      "metadata": {
        "id": "m8M_VZIujTIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_model1 = copy.deepcopy(model)\n",
        "for i, module in enumerate([x for x in compressed_model1.modules() if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)]):\n",
        "      module.selected_rank_ = selected_rank1[i]\n",
        "      print(module.selected_rank_)\n",
        "reparametrize_low_rank(compressed_model1)\n",
        "compressed_model1.cuda()\n",
        "compressed_model1.eval()\n",
        "accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model1)\n",
        "print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))"
      ],
      "metadata": {
        "id": "MU3y6lZ67zSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compress the model(i.e., compressed_model1) using second ranks"
      ],
      "metadata": {
        "id": "cr9GJdAcjgYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_model2 = copy.deepcopy(model)\n",
        "for i, module in enumerate([x for x in compressed_model2.modules() if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)]):\n",
        "      module.selected_rank_ = selected_rank2[i]\n",
        "      print(module.selected_rank_)\n",
        "reparametrize_low_rank(compressed_model2)\n",
        "compressed_model2.cuda()\n",
        "compressed_model2.eval()\n",
        "accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model2)\n",
        "print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))"
      ],
      "metadata": {
        "id": "V1VQSxq88KaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compress the model(i.e., compressed_model1) using third ranks"
      ],
      "metadata": {
        "id": "dnH-C3THjnB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_model3 = copy.deepcopy(model)\n",
        "for i, module in enumerate([x for x in compressed_model3.modules() if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)]):\n",
        "      module.selected_rank_ = selected_rank3[i]\n",
        "      print(module.selected_rank_)\n",
        "reparametrize_low_rank(compressed_model3)\n",
        "compressed_model3.cuda()\n",
        "compressed_model3.eval()\n",
        "accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model3)\n",
        "print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))"
      ],
      "metadata": {
        "id": "OqZbga6Pgs5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune the compressed_model1"
      ],
      "metadata": {
        "id": "TTRbvHtzjp4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuning\n",
        "\n",
        "batch_size = 256\n",
        "lr = 0.02\n",
        "lr_decay = 0.99\n",
        "momentum = 0.9\n",
        "epochs = 100\n",
        "dataset = 'MNIST'\n",
        "n_workers = 2\n",
        "start_epoch = 0\n",
        "print_freq = 20\n",
        "checkpoint = 20\n",
        "\n",
        "\n",
        "training_time = 0\n",
        "epoch = 0\n",
        "all_start_time = time.time()\n",
        "epoch_time = AverageMeter()\n",
        "\n",
        "optimizer = torch.optim.SGD(compressed_model1.parameters(), lr, momentum=momentum, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start_time = time.time()\n",
        "    compressed_model1.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = compressed_model1.forward(x)\n",
        "        loss = torch.nn.functional.cross_entropy(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "    end_time = time.time()\n",
        "    epoch_time.update(end_time - start_time)\n",
        "    training_time = end_time - all_start_time\n",
        "    compressed_model1.eval()\n",
        "    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n",
        "    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n",
        "    if (epoch+1) % print_freq == 0:\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, compressed_model1)\n",
        "        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model1)\n",
        "        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "    scheduler.step()\n",
        "\n",
        "    if checkpoint and (epoch+1) % checkpoint == 0:\n",
        "        # create and save checkpoint here\n",
        "        to_save = {}\n",
        "        to_save['model_state'] = compressed_model1.state_dict()\n",
        "        to_save['optimizer_state'] = optimizer.state_dict()\n",
        "        to_save['lr'] = scheduler.get_last_lr()\n",
        "        to_save['epoch'] = epoch + 1\n",
        "        torch.save(to_save, './compressed_lenet5_v1_checkpoint.pth.tar')"
      ],
      "metadata": {
        "id": "653_v79d8StE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune the compressed_model2"
      ],
      "metadata": {
        "id": "rnu_I8dSjxhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = 0\n",
        "epoch = 0\n",
        "all_start_time = time.time()\n",
        "epoch_time = AverageMeter()\n",
        "\n",
        "optimizer = torch.optim.SGD(compressed_model2.parameters(), lr, momentum=momentum, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start_time = time.time()\n",
        "    compressed_model2.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = compressed_model2.forward(x)\n",
        "        loss = torch.nn.functional.cross_entropy(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "    end_time = time.time()\n",
        "    epoch_time.update(end_time - start_time)\n",
        "    training_time = end_time - all_start_time\n",
        "    compressed_model2.eval()\n",
        "    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n",
        "    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n",
        "    if (epoch+1) % print_freq == 0:\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, compressed_model2)\n",
        "        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model2)\n",
        "        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "    scheduler.step()\n",
        "\n",
        "    if checkpoint and (epoch+1) % checkpoint == 0:\n",
        "        # create and save checkpoint here\n",
        "        to_save = {}\n",
        "        to_save['model_state'] = compressed_model2.state_dict()\n",
        "        to_save['optimizer_state'] = optimizer.state_dict()\n",
        "        to_save['lr'] = scheduler.get_last_lr()\n",
        "        to_save['epoch'] = epoch + 1\n",
        "        torch.save(to_save, './compressed_lenet5_v2_checkpoint.pth.tar')"
      ],
      "metadata": {
        "id": "soUrac4zGBAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune the compressed_model3"
      ],
      "metadata": {
        "id": "uHBCHKlwjzBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = 0\n",
        "epoch = 0\n",
        "all_start_time = time.time()\n",
        "epoch_time = AverageMeter()\n",
        "\n",
        "optimizer = torch.optim.SGD(compressed_model3.parameters(), lr, momentum=momentum, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start_time = time.time()\n",
        "    compressed_model3.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = compressed_model3.forward(x)\n",
        "        loss = torch.nn.functional.cross_entropy(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "    end_time = time.time()\n",
        "    epoch_time.update(end_time - start_time)\n",
        "    training_time = end_time - all_start_time\n",
        "    compressed_model3.eval()\n",
        "    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n",
        "    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n",
        "    if (epoch+1) % print_freq == 0:\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, compressed_model3)\n",
        "        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model3)\n",
        "        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "    scheduler.step()\n",
        "\n",
        "    if checkpoint and (epoch+1) % checkpoint == 0:\n",
        "        # create and save checkpoint here\n",
        "        to_save = {}\n",
        "        to_save['model_state'] = compressed_model3.state_dict()\n",
        "        to_save['optimizer_state'] = optimizer.state_dict()\n",
        "        to_save['lr'] = scheduler.get_last_lr()\n",
        "        to_save['epoch'] = epoch + 1\n",
        "        torch.save(to_save, './compressed_lenet5_v3_checkpoint.pth.tar')"
      ],
      "metadata": {
        "id": "opTbHeSFhSSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}