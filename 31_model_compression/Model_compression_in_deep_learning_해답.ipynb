{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practice: Model compression in Deep Learning"
      ],
      "metadata": {
        "id": "VWgdS661S03g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typical process of the low-rank compression\n",
        "#### Normal Training ⇒ Rank Selection *(excluded from this practice)* ⇒ Low-rank compression ⇒ Fine-tuning\n",
        "\n",
        "## Question\n",
        "1. 코드에서 빈 부분을 채우세요.\n",
        "2. 3가지의 rank setting에 대해서 성능 비교를 수행하세요.\n",
        " - R=[20, 100, 200, 8]\n",
        " - R=[15, 50, 100, 6]\n",
        " - R=[10, 10, 50, 3]\n",
        "3. 3개의 compressed model에 대해 Fine-tuning을 수행한 뒤 성능 비교를 수행하세요."
      ],
      "metadata": {
        "id": "R9yuhWKpS7EB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library import"
      ],
      "metadata": {
        "id": "5jR7stYXV1CX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "umb9o8VBa4Ap"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import OrderedDict\n",
        "from scipy.linalg import svd\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a model"
      ],
      "metadata": {
        "id": "bYhCoXQ-V_h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear):\n",
        "        xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, dropout, nonlinearity):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.special = True\n",
        "        filters = [(20, 5), (50, 5)]\n",
        "        layers = [(800, 500), (500, 10)]\n",
        "\n",
        "        cfg = []\n",
        "        cfg.append(['init_reshape', LambdaLayer(lambda x: x.view(x.size(0), 1,28,28))])\n",
        "        for i, f in enumerate(filters):\n",
        "            prev = 1 if i==0 else filters[i-1][0]\n",
        "            cfg.append(('compressible_' + str(i), nn.Conv2d(prev, f[0], f[1])))\n",
        "            cfg.append(('nonlineairy_'+str(i), nonlinearity()))\n",
        "            cfg.append(('maxpool_'+str(i), nn.MaxPool2d(kernel_size=(2,2), stride=2)))\n",
        "\n",
        "\n",
        "        cfg.append(['reshape', LambdaLayer(lambda x: x.view(x.size(0),-1))])\n",
        "        for i, l in enumerate(layers):\n",
        "            cfg.append(('compressible_' + str(i+len(filters)), nn.Linear(*l)))\n",
        "            if i != len(layers)-1:\n",
        "                # only non terminal layers have nonlinearity and (possible) dropouts\n",
        "                cfg.append(('nonlinearity_' + str(i+len(filters)), nonlinearity()))\n",
        "                if dropout:\n",
        "                    cfg.append(('drop_'+str(i+len(filters)), nn.Dropout()))\n",
        "\n",
        "        self.output = nn.Sequential(OrderedDict(cfg))\n",
        "        self.apply(_weights_init)\n",
        "    def forward(self, input):\n",
        "        h = self.output(input)\n",
        "        return h\n",
        "\n",
        "def lenet5_classic():\n",
        "    return LeNet5(dropout=False, nonlinearity=lambda: nn.ReLU(True))"
      ],
      "metadata": {
        "id": "19Q4sWh80Kxu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define datasets"
      ],
      "metadata": {
        "id": "n-y5CX01Wesp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'MNIST'\n",
        "batch_size = 256\n",
        "n_workers = 2\n",
        "\n",
        "def mnist_data():\n",
        "    mnist_train = torchvision.datasets.MNIST(root='./datasets/', train=True, download=True)\n",
        "    mnist_test = torchvision.datasets.MNIST(root='./datasets/', train=False, download=True)\n",
        "\n",
        "    train_data = mnist_train.data.to(torch.float) / 255.\n",
        "    test_data = mnist_test.data.to(torch.float) / 255.\n",
        "    mean_image = torch.mean(train_data, dim=0)\n",
        "\n",
        "    train_data -= mean_image\n",
        "    test_data -= mean_image\n",
        "\n",
        "    train_labels = mnist_train.targets\n",
        "    test_labels = mnist_test.targets\n",
        "\n",
        "    our_mnist = {\n",
        "        'train_data': train_data, 'test_data': test_data,\n",
        "        'train_labels': train_labels, 'test_labels': test_labels\n",
        "    }\n",
        "    return our_mnist\n",
        "\n",
        "data = mnist_data()\n",
        "train_data = TensorDataset(data['train_data'], data['train_labels'])\n",
        "test_data = TensorDataset(data['test_data'], data['test_labels'])\n",
        "\n",
        "train_loader = DataLoader(train_data, num_workers=n_workers, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
        "test_loader = DataLoader(test_data, num_workers=n_workers, batch_size=batch_size, shuffle=False, pin_memory=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okdiL5xRWePh",
        "outputId": "9d5a1747-dcf2-423a-f18f-d395c0ec0457"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 113892640.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 4806590.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 42655667.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16609005.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Training"
      ],
      "metadata": {
        "id": "ymYXA20XWIMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a model\n",
        "model = lenet5_classic()\n",
        "model.cuda()\n",
        "print(model)\n",
        "\n",
        "# Hyper-parameters for training\n",
        "lr = 0.1\n",
        "lr_decay = 0.99\n",
        "momentum = 0.9\n",
        "epochs = 100\n",
        "start_epoch = 0\n",
        "print_freq = 20\n",
        "checkpoint = 20\n",
        "\n",
        "# Define an optimizer and a scheduler\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n",
        "\n",
        "# Define the funcions required to training\n",
        "def my_eval(x, target, model):\n",
        "    out_ = model.forward(x)\n",
        "    return out_, torch.nn.functional.cross_entropy(out_, target)\n",
        "\n",
        "def format_time(seconds):\n",
        "    if seconds < 60:\n",
        "        return '{:.1f}s.'.format(seconds)\n",
        "    if seconds < 3600:\n",
        "        return '{:d}m. {}'.format(int(seconds//60), format_time(seconds%60))\n",
        "    if seconds < 3600*24:\n",
        "        return '{:d}h. {}'.format(int(seconds//3600), format_time(seconds%3600))\n",
        "    return '{:d}d. {}'.format(int(seconds//(3600*24)), format_time(seconds%(3600*24)))\n",
        "\n",
        "def compute_acc_loss(forward_func, data_loader, model):\n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    for batch_idx, (x, target) in enumerate(data_loader):\n",
        "        with torch.no_grad():\n",
        "            target = target.cuda()\n",
        "            score, loss = forward_func(x.cuda(), target, model)\n",
        "            _, pred_label = torch.max(score.data, 1)\n",
        "            correct_cnt += (pred_label == target.data).sum().item()\n",
        "            ave_loss += loss.data.item() * len(x)\n",
        "    accuracy = correct_cnt * 1.0 / len(data_loader.dataset)\n",
        "    print(correct_cnt, len(data_loader.dataset))\n",
        "    ave_loss /= len(data_loader.dataset)\n",
        "    return accuracy, ave_loss\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0.0\n",
        "        self.avg = 0.0\n",
        "        self.sum = 0.0\n",
        "        self.count = 0.0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "# Training\n",
        "training_time = 0\n",
        "epoch = 0\n",
        "all_start_time = time.time()\n",
        "epoch_time = AverageMeter()\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = model.forward(x)\n",
        "        loss = torch.nn.functional.cross_entropy(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "    end_time = time.time()\n",
        "    epoch_time.update(end_time - start_time)\n",
        "    training_time = end_time - all_start_time\n",
        "    model.eval()\n",
        "    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n",
        "    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n",
        "    if (epoch+1) % print_freq == 0:\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, model)\n",
        "        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, model)\n",
        "        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "    scheduler.step()\n",
        "\n",
        "    if checkpoint and (epoch+1) % checkpoint == 0:\n",
        "        # create and save checkpoint here\n",
        "        to_save = {}\n",
        "        to_save['model_state'] = model.state_dict()\n",
        "        to_save['optimizer_state'] = optimizer.state_dict()\n",
        "        to_save['lr'] = scheduler.get_last_lr()\n",
        "        to_save['epoch'] = epoch + 1\n",
        "        torch.save(to_save, './lenet5_checkpoint.pth.tar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqBzq36B0Yy1",
        "outputId": "b4c20680-93cf-4838-afa4-65aefa21cf7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeNet5(\n",
            "  (output): Sequential(\n",
            "    (init_reshape): LambdaLayer()\n",
            "    (compressible_0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (nonlineairy_0): ReLU(inplace=True)\n",
            "    (maxpool_0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (compressible_1): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (nonlineairy_1): ReLU(inplace=True)\n",
            "    (maxpool_1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (reshape): LambdaLayer()\n",
            "    (compressible_2): Linear(in_features=800, out_features=500, bias=True)\n",
            "    (nonlinearity_2): ReLU(inplace=True)\n",
            "    (compressible_3): Linear(in_features=500, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 0 finished in 1.513s (avg.: 1.513s). Training for 1.5s.\n",
            "\tLR: 0.1\n",
            "Epoch 1 finished in 0.085s (avg.: 0.799s). Training for 1.6s.\n",
            "\tLR: 0.099\n",
            "Epoch 2 finished in 0.078s (avg.: 0.558s). Training for 1.7s.\n",
            "\tLR: 0.09801\n",
            "Epoch 3 finished in 0.082s (avg.: 0.439s). Training for 1.8s.\n",
            "\tLR: 0.09703\n",
            "Epoch 4 finished in 0.095s (avg.: 0.371s). Training for 1.9s.\n",
            "\tLR: 0.09606\n",
            "Epoch 5 finished in 0.075s (avg.: 0.321s). Training for 1.9s.\n",
            "\tLR: 0.0951\n",
            "Epoch 6 finished in 0.077s (avg.: 0.286s). Training for 2.0s.\n",
            "\tLR: 0.09415\n",
            "Epoch 7 finished in 0.074s (avg.: 0.260s). Training for 2.1s.\n",
            "\tLR: 0.09321\n",
            "Epoch 8 finished in 0.089s (avg.: 0.241s). Training for 2.2s.\n",
            "\tLR: 0.09227\n",
            "Epoch 9 finished in 0.078s (avg.: 0.225s). Training for 2.3s.\n",
            "\tLR: 0.09135\n",
            "Epoch 10 finished in 0.078s (avg.: 0.211s). Training for 2.3s.\n",
            "\tLR: 0.09044\n",
            "Epoch 11 finished in 0.073s (avg.: 0.200s). Training for 2.4s.\n",
            "\tLR: 0.08953\n",
            "Epoch 12 finished in 0.085s (avg.: 0.191s). Training for 2.5s.\n",
            "\tLR: 0.08864\n",
            "Epoch 13 finished in 0.074s (avg.: 0.183s). Training for 2.6s.\n",
            "\tLR: 0.08775\n",
            "Epoch 14 finished in 0.082s (avg.: 0.176s). Training for 2.6s.\n",
            "\tLR: 0.08687\n",
            "Epoch 15 finished in 0.091s (avg.: 0.171s). Training for 2.7s.\n",
            "\tLR: 0.08601\n",
            "Epoch 16 finished in 0.075s (avg.: 0.165s). Training for 2.8s.\n",
            "\tLR: 0.08515\n",
            "Epoch 17 finished in 0.087s (avg.: 0.161s). Training for 2.9s.\n",
            "\tLR: 0.08429\n",
            "Epoch 18 finished in 0.091s (avg.: 0.157s). Training for 3.0s.\n",
            "\tLR: 0.08345\n",
            "Epoch 19 finished in 0.081s (avg.: 0.153s). Training for 3.1s.\n",
            "\tLR: 0.08262\n",
            "54067 60000\n",
            "\ttrain loss: 0.318492, accuracy: 0.9011\n",
            "9081 10000\n",
            "\ttest  loss: 0.295336, accuracy: 0.9081\n",
            "Epoch 20 finished in 0.084s (avg.: 0.150s). Training for 4.7s.\n",
            "\tLR: 0.08179\n",
            "Epoch 21 finished in 0.074s (avg.: 0.146s). Training for 4.8s.\n",
            "\tLR: 0.08097\n",
            "Epoch 22 finished in 0.076s (avg.: 0.143s). Training for 4.8s.\n",
            "\tLR: 0.08016\n",
            "Epoch 23 finished in 0.080s (avg.: 0.141s). Training for 4.9s.\n",
            "\tLR: 0.07936\n",
            "Epoch 24 finished in 0.071s (avg.: 0.138s). Training for 5.0s.\n",
            "\tLR: 0.07857\n",
            "Epoch 25 finished in 0.080s (avg.: 0.136s). Training for 5.1s.\n",
            "\tLR: 0.07778\n",
            "Epoch 26 finished in 0.078s (avg.: 0.133s). Training for 5.1s.\n",
            "\tLR: 0.077\n",
            "Epoch 27 finished in 0.080s (avg.: 0.132s). Training for 5.2s.\n",
            "\tLR: 0.07623\n",
            "Epoch 28 finished in 0.077s (avg.: 0.130s). Training for 5.3s.\n",
            "\tLR: 0.07547\n",
            "Epoch 29 finished in 0.082s (avg.: 0.128s). Training for 5.4s.\n",
            "\tLR: 0.07472\n",
            "Epoch 30 finished in 0.073s (avg.: 0.126s). Training for 5.5s.\n",
            "\tLR: 0.07397\n",
            "Epoch 31 finished in 0.071s (avg.: 0.125s). Training for 5.5s.\n",
            "\tLR: 0.07323\n",
            "Epoch 32 finished in 0.074s (avg.: 0.123s). Training for 5.6s.\n",
            "\tLR: 0.0725\n",
            "Epoch 33 finished in 0.081s (avg.: 0.122s). Training for 5.7s.\n",
            "\tLR: 0.07177\n",
            "Epoch 34 finished in 0.076s (avg.: 0.121s). Training for 5.8s.\n",
            "\tLR: 0.07106\n",
            "Epoch 35 finished in 0.077s (avg.: 0.119s). Training for 5.8s.\n",
            "\tLR: 0.07034\n",
            "Epoch 36 finished in 0.074s (avg.: 0.118s). Training for 5.9s.\n",
            "\tLR: 0.06964\n",
            "Epoch 37 finished in 0.090s (avg.: 0.117s). Training for 6.0s.\n",
            "\tLR: 0.06894\n",
            "Epoch 38 finished in 0.077s (avg.: 0.116s). Training for 6.1s.\n",
            "\tLR: 0.06826\n",
            "Epoch 39 finished in 0.080s (avg.: 0.115s). Training for 6.2s.\n",
            "\tLR: 0.06757\n",
            "57125 60000\n",
            "\ttrain loss: 0.161667, accuracy: 0.9521\n",
            "9563 10000\n",
            "\ttest  loss: 0.146920, accuracy: 0.9563\n",
            "Epoch 40 finished in 0.085s (avg.: 0.115s). Training for 7.6s.\n",
            "\tLR: 0.0669\n",
            "Epoch 41 finished in 0.078s (avg.: 0.114s). Training for 7.7s.\n",
            "\tLR: 0.06623\n",
            "Epoch 42 finished in 0.104s (avg.: 0.114s). Training for 7.8s.\n",
            "\tLR: 0.06557\n",
            "Epoch 43 finished in 0.133s (avg.: 0.114s). Training for 7.9s.\n",
            "\tLR: 0.06491\n",
            "Epoch 44 finished in 0.139s (avg.: 0.115s). Training for 8.0s.\n",
            "\tLR: 0.06426\n",
            "Epoch 45 finished in 0.121s (avg.: 0.115s). Training for 8.2s.\n",
            "\tLR: 0.06362\n",
            "Epoch 46 finished in 0.125s (avg.: 0.115s). Training for 8.3s.\n",
            "\tLR: 0.06298\n",
            "Epoch 47 finished in 0.123s (avg.: 0.115s). Training for 8.4s.\n",
            "\tLR: 0.06235\n",
            "Epoch 48 finished in 0.111s (avg.: 0.115s). Training for 8.5s.\n",
            "\tLR: 0.06173\n",
            "Epoch 49 finished in 0.122s (avg.: 0.115s). Training for 8.6s.\n",
            "\tLR: 0.06111\n",
            "Epoch 50 finished in 0.112s (avg.: 0.115s). Training for 8.8s.\n",
            "\tLR: 0.0605\n",
            "Epoch 51 finished in 0.109s (avg.: 0.115s). Training for 8.9s.\n",
            "\tLR: 0.0599\n",
            "Epoch 52 finished in 0.113s (avg.: 0.115s). Training for 9.0s.\n",
            "\tLR: 0.0593\n",
            "Epoch 53 finished in 0.132s (avg.: 0.115s). Training for 9.1s.\n",
            "\tLR: 0.0587\n",
            "Epoch 54 finished in 0.124s (avg.: 0.115s). Training for 9.2s.\n",
            "\tLR: 0.05812\n",
            "Epoch 55 finished in 0.125s (avg.: 0.116s). Training for 9.4s.\n",
            "\tLR: 0.05754\n",
            "Epoch 56 finished in 0.118s (avg.: 0.116s). Training for 9.5s.\n",
            "\tLR: 0.05696\n",
            "Epoch 57 finished in 0.107s (avg.: 0.115s). Training for 9.6s.\n",
            "\tLR: 0.05639\n",
            "Epoch 58 finished in 0.108s (avg.: 0.115s). Training for 9.7s.\n",
            "\tLR: 0.05583\n",
            "Epoch 59 finished in 0.097s (avg.: 0.115s). Training for 9.8s.\n",
            "\tLR: 0.05527\n",
            "58074 60000\n",
            "\ttrain loss: 0.107966, accuracy: 0.9679\n",
            "9713 10000\n",
            "\ttest  loss: 0.095565, accuracy: 0.9713\n",
            "Epoch 60 finished in 0.081s (avg.: 0.114s). Training for 11.3s.\n",
            "\tLR: 0.05472\n",
            "Epoch 61 finished in 0.077s (avg.: 0.114s). Training for 11.4s.\n",
            "\tLR: 0.05417\n",
            "Epoch 62 finished in 0.074s (avg.: 0.113s). Training for 11.5s.\n",
            "\tLR: 0.05363\n",
            "Epoch 63 finished in 0.076s (avg.: 0.113s). Training for 11.6s.\n",
            "\tLR: 0.05309\n",
            "Epoch 64 finished in 0.079s (avg.: 0.112s). Training for 11.6s.\n",
            "\tLR: 0.05256\n",
            "Epoch 65 finished in 0.079s (avg.: 0.112s). Training for 11.7s.\n",
            "\tLR: 0.05203\n",
            "Epoch 66 finished in 0.077s (avg.: 0.111s). Training for 11.8s.\n",
            "\tLR: 0.05151\n",
            "Epoch 67 finished in 0.082s (avg.: 0.111s). Training for 11.9s.\n",
            "\tLR: 0.051\n",
            "Epoch 68 finished in 0.084s (avg.: 0.110s). Training for 12.0s.\n",
            "\tLR: 0.05049\n",
            "Epoch 69 finished in 0.073s (avg.: 0.110s). Training for 12.0s.\n",
            "\tLR: 0.04998\n",
            "Epoch 70 finished in 0.078s (avg.: 0.109s). Training for 12.1s.\n",
            "\tLR: 0.04948\n",
            "Epoch 71 finished in 0.082s (avg.: 0.109s). Training for 12.2s.\n",
            "\tLR: 0.04899\n",
            "Epoch 72 finished in 0.082s (avg.: 0.109s). Training for 12.3s.\n",
            "\tLR: 0.0485\n",
            "Epoch 73 finished in 0.082s (avg.: 0.108s). Training for 12.4s.\n",
            "\tLR: 0.04801\n",
            "Epoch 74 finished in 0.074s (avg.: 0.108s). Training for 12.4s.\n",
            "\tLR: 0.04753\n",
            "Epoch 75 finished in 0.077s (avg.: 0.107s). Training for 12.5s.\n",
            "\tLR: 0.04706\n",
            "Epoch 76 finished in 0.076s (avg.: 0.107s). Training for 12.6s.\n",
            "\tLR: 0.04659\n",
            "Epoch 77 finished in 0.082s (avg.: 0.107s). Training for 12.7s.\n",
            "\tLR: 0.04612\n",
            "Epoch 78 finished in 0.078s (avg.: 0.106s). Training for 12.7s.\n",
            "\tLR: 0.04566\n",
            "Epoch 79 finished in 0.079s (avg.: 0.106s). Training for 12.8s.\n",
            "\tLR: 0.0452\n",
            "58358 60000\n",
            "\ttrain loss: 0.091141, accuracy: 0.9726\n",
            "9734 10000\n",
            "\ttest  loss: 0.081370, accuracy: 0.9734\n",
            "Epoch 80 finished in 0.081s (avg.: 0.106s). Training for 14.3s.\n",
            "\tLR: 0.04475\n",
            "Epoch 81 finished in 0.093s (avg.: 0.105s). Training for 14.4s.\n",
            "\tLR: 0.0443\n",
            "Epoch 82 finished in 0.083s (avg.: 0.105s). Training for 14.5s.\n",
            "\tLR: 0.04386\n",
            "Epoch 83 finished in 0.079s (avg.: 0.105s). Training for 14.5s.\n",
            "\tLR: 0.04342\n",
            "Epoch 84 finished in 0.086s (avg.: 0.105s). Training for 14.6s.\n",
            "\tLR: 0.04299\n",
            "Epoch 85 finished in 0.075s (avg.: 0.104s). Training for 14.7s.\n",
            "\tLR: 0.04256\n",
            "Epoch 86 finished in 0.079s (avg.: 0.104s). Training for 14.8s.\n",
            "\tLR: 0.04213\n",
            "Epoch 87 finished in 0.083s (avg.: 0.104s). Training for 14.9s.\n",
            "\tLR: 0.04171\n",
            "Epoch 88 finished in 0.079s (avg.: 0.103s). Training for 15.0s.\n",
            "\tLR: 0.04129\n",
            "Epoch 89 finished in 0.079s (avg.: 0.103s). Training for 15.0s.\n",
            "\tLR: 0.04088\n",
            "Epoch 90 finished in 0.084s (avg.: 0.103s). Training for 15.1s.\n",
            "\tLR: 0.04047\n",
            "Epoch 91 finished in 0.077s (avg.: 0.103s). Training for 15.2s.\n",
            "\tLR: 0.04007\n",
            "Epoch 92 finished in 0.082s (avg.: 0.102s). Training for 15.3s.\n",
            "\tLR: 0.03967\n",
            "Epoch 93 finished in 0.077s (avg.: 0.102s). Training for 15.4s.\n",
            "\tLR: 0.03927\n",
            "Epoch 94 finished in 0.093s (avg.: 0.102s). Training for 15.4s.\n",
            "\tLR: 0.03888\n",
            "Epoch 95 finished in 0.077s (avg.: 0.102s). Training for 15.5s.\n",
            "\tLR: 0.03849\n",
            "Epoch 96 finished in 0.077s (avg.: 0.102s). Training for 15.6s.\n",
            "\tLR: 0.0381\n",
            "Epoch 97 finished in 0.072s (avg.: 0.101s). Training for 15.7s.\n",
            "\tLR: 0.03772\n",
            "Epoch 98 finished in 0.080s (avg.: 0.101s). Training for 15.8s.\n",
            "\tLR: 0.03735\n",
            "Epoch 99 finished in 0.075s (avg.: 0.101s). Training for 15.8s.\n",
            "\tLR: 0.03697\n",
            "58554 60000\n",
            "\ttrain loss: 0.081422, accuracy: 0.9759\n",
            "9762 10000\n",
            "\ttest  loss: 0.075974, accuracy: 0.9762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rank Selection"
      ],
      "metadata": {
        "id": "-FMj3wsai6NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_rank1=[10, 10, 15, 7]\n",
        "selected_rank2=[5, 5, 7, 5]\n",
        "selected_rank3=[3, 3, 5, 3]"
      ],
      "metadata": {
        "id": "--beybIo53uM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the functions for low-rank compression"
      ],
      "metadata": {
        "id": "4gcxAgt3jCuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_layer_reparametrizer(sub_module, conv_scheme='scheme_1'):\n",
        "    W = sub_module.weight.data.cpu().numpy()\n",
        "\n",
        "    init_shape = None\n",
        "    n,m,d1,d2 = None, None, None, None\n",
        "    if isinstance(sub_module, nn.Conv2d):\n",
        "        if conv_scheme == 'scheme_1':\n",
        "            init_shape = W.shape\n",
        "            reshaped = W.reshape([init_shape[0], -1])\n",
        "            W = reshaped\n",
        "        elif conv_scheme == 'scheme_2':\n",
        "            raise NotImplementedError(\"We did not implement scheme-2 in this pratice.\")\n",
        "\n",
        "    u, s, v = svd(W, full_matrices=False)\n",
        "    from numpy.linalg import matrix_rank\n",
        "\n",
        "    r = sub_module.rank_ if hasattr(sub_module, 'rank_') else sub_module.selected_rank_ if hasattr(sub_module, 'selected_rank_') else int(matrix_rank(W))\n",
        "\n",
        "    if r < np.min(W.shape):\n",
        "        diag = np.diag(s[:r] ** 0.5)\n",
        "        U = u[:, :r] @ diag\n",
        "        V = diag @ v[:r, :]\n",
        "        new_W = U @ V\n",
        "\n",
        "\n",
        "        from numpy.linalg import norm\n",
        "        m,n = W.shape\n",
        "        if r > np.floor(m*n/(m+n)):\n",
        "            raise RankNotEfficientException(\"Selected rank doesn't contribute to any savings\")\n",
        "        bias = sub_module.bias is not None\n",
        "        if isinstance(sub_module, nn.Linear):\n",
        "            l1 = nn.Linear(in_features=sub_module.in_features, out_features=r, bias=False)\n",
        "            l2 = nn.Linear(in_features=r, out_features=sub_module.out_features, bias=bias)\n",
        "            l1.weight.data = torch.from_numpy(V)\n",
        "            l2.weight.data = torch.from_numpy(U)\n",
        "            if bias:\n",
        "                l2.bias.data = sub_module.bias.data\n",
        "            return l1, l2\n",
        "        else:\n",
        "            if conv_scheme == 'scheme_1':\n",
        "                l1 = nn.Conv2d(in_channels=sub_module.in_channels,\n",
        "                               out_channels=r,\n",
        "                               kernel_size=sub_module.kernel_size,\n",
        "                               stride=sub_module.stride,\n",
        "                               padding=sub_module.padding,\n",
        "                               dilation=sub_module.dilation,\n",
        "                               groups=sub_module.groups,\n",
        "                               bias=False\n",
        "                               )\n",
        "                l2 = nn.Conv2d(in_channels=r, out_channels=sub_module.out_channels,\n",
        "                               kernel_size=1,\n",
        "                               bias=bias)\n",
        "                l1.weight.data = torch.from_numpy(V.reshape([-1, *init_shape[1:]]))\n",
        "                l2.weight.data = torch.from_numpy(U[:, :, None, None])\n",
        "\n",
        "                if bias:\n",
        "                    l2.bias.data = sub_module.bias.data\n",
        "\n",
        "                return l1, l2\n",
        "            elif conv_scheme == 'scheme_2':\n",
        "                raise NotImplementedError(\"We did not implement scheme-2 in this pratice.\")\n",
        "\n",
        "\n",
        "def reparametrization_helper(list_of_modules, conv_scheme, old_weight_decay=True):\n",
        "    new_sequence = []\n",
        "    items = list_of_modules.items()\n",
        "    decayed_values_repar = []\n",
        "    decayed_valued_old = []\n",
        "    for i, (name, sub_module) in enumerate(items):\n",
        "        if isinstance(sub_module, nn.Sequential):\n",
        "            dv_repar_sub, dv_old_sub, nseq_sub = reparametrization_helper(sub_module._modules, conv_scheme=conv_scheme,old_weight_decay=old_weight_decay)\n",
        "            new_sequence.append((name, nn.Sequential(OrderedDict(nseq_sub))))\n",
        "            decayed_values_repar.extend(dv_repar_sub)\n",
        "            decayed_valued_old.extend(dv_old_sub)\n",
        "        elif isinstance(sub_module, nn.Linear) or isinstance(sub_module, nn.Conv2d):\n",
        "            try:\n",
        "                l1, l2 = linear_layer_reparametrizer(sub_module, conv_scheme=conv_scheme)\n",
        "                new_sequence.append((name + '_V', l1))\n",
        "                new_sequence.append((name + '_U', l2))\n",
        "                decayed_values_repar.append((l1, l2))\n",
        "\n",
        "            except Exception as e:\n",
        "                new_sequence.append((name, sub_module))\n",
        "                decayed_valued_old.append(sub_module.weight)\n",
        "        else:\n",
        "            new_sequence.append((name, sub_module))\n",
        "            if old_weight_decay and hasattr(sub_module, 'weight'):\n",
        "                decayed_valued_old.append(sub_module.weight)\n",
        "    return decayed_values_repar, decayed_valued_old, new_sequence\n",
        "\n",
        "\n",
        "def reparametrize_low_rank(model, old_weight_decay=True):\n",
        "    decayed_values_repar, decayed_valued_old, new_sequence = reparametrization_helper(model.output._modules, conv_scheme='scheme_1', old_weight_decay=old_weight_decay)\n",
        "    model.output = nn.Sequential(OrderedDict(new_sequence))\n",
        "\n",
        "    def weight_decay():\n",
        "        sum_ = torch.autograd.Variable(torch.FloatTensor([0.0]).cuda())\n",
        "        for x in decayed_valued_old:\n",
        "            sum_ += torch.sum(x**2)\n",
        "        for v,u in decayed_values_repar:\n",
        "            v = v.weight\n",
        "            u = u.weight\n",
        "            u_ = u.view(u.size()[0], -1)\n",
        "            v_ = v.view(u_.size()[1], -1)\n",
        "            sum_ += torch.sum(torch.matmul(u_,v_)**2)\n",
        "        return sum_\n",
        "    model.weight_decay = weight_decay\n",
        "    return nn.Sequential(OrderedDict(new_sequence))"
      ],
      "metadata": {
        "id": "OYEwODYk535Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compress the model(i.e., compressed_model1) using first ranks"
      ],
      "metadata": {
        "id": "m8M_VZIujTIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_model1 = copy.deepcopy(model)\n",
        "for i, module in enumerate([x for x in compressed_model1.modules() if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)]):\n",
        "      module.selected_rank_ = selected_rank1[i]\n",
        "      print(module.selected_rank_)\n",
        "reparametrize_low_rank(compressed_model1)\n",
        "print(compressed_model1)\n",
        "compressed_model1.cuda()\n",
        "compressed_model1.eval()\n",
        "accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model1)\n",
        "print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU3y6lZ67zSd",
        "outputId": "2d62686c-e431-4717-d707-7488eac61ebd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "10\n",
            "15\n",
            "7\n",
            "LeNet5(\n",
            "  (output): Sequential(\n",
            "    (init_reshape): LambdaLayer()\n",
            "    (compressible_0_V): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "    (compressible_0_U): Conv2d(10, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (nonlineairy_0): ReLU(inplace=True)\n",
            "    (maxpool_0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (compressible_1_V): Conv2d(20, 10, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "    (compressible_1_U): Conv2d(10, 50, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (nonlineairy_1): ReLU(inplace=True)\n",
            "    (maxpool_1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (reshape): LambdaLayer()\n",
            "    (compressible_2_V): Linear(in_features=800, out_features=15, bias=False)\n",
            "    (compressible_2_U): Linear(in_features=15, out_features=500, bias=True)\n",
            "    (nonlinearity_2): ReLU(inplace=True)\n",
            "    (compressible_3_V): Linear(in_features=500, out_features=7, bias=False)\n",
            "    (compressible_3_U): Linear(in_features=7, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "9139 10000\n",
            "\ttest  loss: 0.262218, accuracy: 0.9139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compress the model(i.e., compressed_model1) using second ranks"
      ],
      "metadata": {
        "id": "cr9GJdAcjgYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_model2 = copy.deepcopy(model)\n",
        "for i, module in enumerate([x for x in compressed_model2.modules() if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)]):\n",
        "      module.selected_rank_ = selected_rank2[i]\n",
        "      print(module.selected_rank_)\n",
        "reparametrize_low_rank(compressed_model2)\n",
        "compressed_model2.cuda()\n",
        "compressed_model2.eval()\n",
        "accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model2)\n",
        "print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1VQSxq88KaP",
        "outputId": "d771473d-2627-478b-b738-e82f606fca6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "5\n",
            "7\n",
            "5\n",
            "7987 10000\n",
            "\ttest  loss: 0.652572, accuracy: 0.7987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compress the model(i.e., compressed_model1) using third ranks"
      ],
      "metadata": {
        "id": "dnH-C3THjnB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_model3 = copy.deepcopy(model)\n",
        "for i, module in enumerate([x for x in compressed_model3.modules() if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)]):\n",
        "      module.selected_rank_ = selected_rank3[i]\n",
        "      print(module.selected_rank_)\n",
        "reparametrize_low_rank(compressed_model3)\n",
        "compressed_model3.cuda()\n",
        "compressed_model3.eval()\n",
        "accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model3)\n",
        "print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqZbga6Pgs5V",
        "outputId": "896c5e0b-0286-41ff-86f6-a77a8954ab14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "3\n",
            "5\n",
            "3\n",
            "3438 10000\n",
            "\ttest  loss: 1.754112, accuracy: 0.3438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune the compressed_model1"
      ],
      "metadata": {
        "id": "TTRbvHtzjp4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuning\n",
        "\n",
        "batch_size = 256\n",
        "lr = 0.02\n",
        "lr_decay = 0.99\n",
        "momentum = 0.9\n",
        "epochs = 100\n",
        "dataset = 'MNIST'\n",
        "n_workers = 2\n",
        "start_epoch = 0\n",
        "print_freq = 20\n",
        "checkpoint = 20\n",
        "\n",
        "\n",
        "training_time = 0\n",
        "epoch = 0\n",
        "all_start_time = time.time()\n",
        "epoch_time = AverageMeter()\n",
        "\n",
        "optimizer = torch.optim.SGD(compressed_model1.parameters(), lr, momentum=momentum, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start_time = time.time()\n",
        "    compressed_model1.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = compressed_model1.forward(x)\n",
        "        loss = torch.nn.functional.cross_entropy(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "    end_time = time.time()\n",
        "    epoch_time.update(end_time - start_time)\n",
        "    training_time = end_time - all_start_time\n",
        "    compressed_model1.eval()\n",
        "    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n",
        "    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n",
        "    if (epoch+1) % print_freq == 0:\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, compressed_model1)\n",
        "        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model1)\n",
        "        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "    scheduler.step()\n",
        "\n",
        "    if checkpoint and (epoch+1) % checkpoint == 0:\n",
        "        # create and save checkpoint here\n",
        "        to_save = {}\n",
        "        to_save['model_state'] = compressed_model1.state_dict()\n",
        "        to_save['optimizer_state'] = optimizer.state_dict()\n",
        "        to_save['lr'] = scheduler.get_last_lr()\n",
        "        to_save['epoch'] = epoch + 1\n",
        "        torch.save(to_save, './compressed_lenet5_v1_checkpoint.pth.tar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "653_v79d8StE",
        "outputId": "f5faf1e0-383d-4881-f41d-62cb479df5dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 finished in 0.094s (avg.: 0.094s). Training for 0.1s.\n",
            "\tLR: 0.02\n",
            "Epoch 1 finished in 0.075s (avg.: 0.084s). Training for 0.2s.\n",
            "\tLR: 0.0198\n",
            "Epoch 2 finished in 0.079s (avg.: 0.083s). Training for 0.2s.\n",
            "\tLR: 0.0196\n",
            "Epoch 3 finished in 0.082s (avg.: 0.083s). Training for 0.3s.\n",
            "\tLR: 0.01941\n",
            "Epoch 4 finished in 0.089s (avg.: 0.084s). Training for 0.4s.\n",
            "\tLR: 0.01921\n",
            "Epoch 5 finished in 0.081s (avg.: 0.083s). Training for 0.5s.\n",
            "\tLR: 0.01902\n",
            "Epoch 6 finished in 0.094s (avg.: 0.085s). Training for 0.6s.\n",
            "\tLR: 0.01883\n",
            "Epoch 7 finished in 0.088s (avg.: 0.085s). Training for 0.7s.\n",
            "\tLR: 0.01864\n",
            "Epoch 8 finished in 0.083s (avg.: 0.085s). Training for 0.8s.\n",
            "\tLR: 0.01845\n",
            "Epoch 9 finished in 0.081s (avg.: 0.085s). Training for 0.9s.\n",
            "\tLR: 0.01827\n",
            "Epoch 10 finished in 0.081s (avg.: 0.084s). Training for 0.9s.\n",
            "\tLR: 0.01809\n",
            "Epoch 11 finished in 0.084s (avg.: 0.084s). Training for 1.0s.\n",
            "\tLR: 0.01791\n",
            "Epoch 12 finished in 0.082s (avg.: 0.084s). Training for 1.1s.\n",
            "\tLR: 0.01773\n",
            "Epoch 13 finished in 0.089s (avg.: 0.084s). Training for 1.2s.\n",
            "\tLR: 0.01755\n",
            "Epoch 14 finished in 0.082s (avg.: 0.084s). Training for 1.3s.\n",
            "\tLR: 0.01737\n",
            "Epoch 15 finished in 0.080s (avg.: 0.084s). Training for 1.4s.\n",
            "\tLR: 0.0172\n",
            "Epoch 16 finished in 0.071s (avg.: 0.083s). Training for 1.4s.\n",
            "\tLR: 0.01703\n",
            "Epoch 17 finished in 0.119s (avg.: 0.085s). Training for 1.5s.\n",
            "\tLR: 0.01686\n",
            "Epoch 18 finished in 0.129s (avg.: 0.088s). Training for 1.7s.\n",
            "\tLR: 0.01669\n",
            "Epoch 19 finished in 0.113s (avg.: 0.089s). Training for 1.8s.\n",
            "\tLR: 0.01652\n",
            "57507 60000\n",
            "\ttrain loss: 0.139953, accuracy: 0.9585\n",
            "9613 10000\n",
            "\ttest  loss: 0.126347, accuracy: 0.9613\n",
            "Epoch 20 finished in 0.080s (avg.: 0.088s). Training for 4.1s.\n",
            "\tLR: 0.01636\n",
            "Epoch 21 finished in 0.078s (avg.: 0.088s). Training for 4.1s.\n",
            "\tLR: 0.01619\n",
            "Epoch 22 finished in 0.084s (avg.: 0.088s). Training for 4.2s.\n",
            "\tLR: 0.01603\n",
            "Epoch 23 finished in 0.088s (avg.: 0.088s). Training for 4.3s.\n",
            "\tLR: 0.01587\n",
            "Epoch 24 finished in 0.084s (avg.: 0.088s). Training for 4.4s.\n",
            "\tLR: 0.01571\n",
            "Epoch 25 finished in 0.083s (avg.: 0.087s). Training for 4.5s.\n",
            "\tLR: 0.01556\n",
            "Epoch 26 finished in 0.084s (avg.: 0.087s). Training for 4.6s.\n",
            "\tLR: 0.0154\n",
            "Epoch 27 finished in 0.092s (avg.: 0.087s). Training for 4.7s.\n",
            "\tLR: 0.01525\n",
            "Epoch 28 finished in 0.089s (avg.: 0.088s). Training for 4.7s.\n",
            "\tLR: 0.01509\n",
            "Epoch 29 finished in 0.075s (avg.: 0.087s). Training for 4.8s.\n",
            "\tLR: 0.01494\n",
            "Epoch 30 finished in 0.090s (avg.: 0.087s). Training for 4.9s.\n",
            "\tLR: 0.01479\n",
            "Epoch 31 finished in 0.079s (avg.: 0.087s). Training for 5.0s.\n",
            "\tLR: 0.01465\n",
            "Epoch 32 finished in 0.079s (avg.: 0.087s). Training for 5.1s.\n",
            "\tLR: 0.0145\n",
            "Epoch 33 finished in 0.084s (avg.: 0.087s). Training for 5.2s.\n",
            "\tLR: 0.01435\n",
            "Epoch 34 finished in 0.077s (avg.: 0.086s). Training for 5.2s.\n",
            "\tLR: 0.01421\n",
            "Epoch 35 finished in 0.082s (avg.: 0.086s). Training for 5.3s.\n",
            "\tLR: 0.01407\n",
            "Epoch 36 finished in 0.081s (avg.: 0.086s). Training for 5.4s.\n",
            "\tLR: 0.01393\n",
            "Epoch 37 finished in 0.080s (avg.: 0.086s). Training for 5.5s.\n",
            "\tLR: 0.01379\n",
            "Epoch 38 finished in 0.085s (avg.: 0.086s). Training for 5.6s.\n",
            "\tLR: 0.01365\n",
            "Epoch 39 finished in 0.078s (avg.: 0.086s). Training for 5.6s.\n",
            "\tLR: 0.01351\n",
            "57774 60000\n",
            "\ttrain loss: 0.122824, accuracy: 0.9629\n",
            "9659 10000\n",
            "\ttest  loss: 0.106790, accuracy: 0.9659\n",
            "Epoch 40 finished in 0.079s (avg.: 0.086s). Training for 7.1s.\n",
            "\tLR: 0.01338\n",
            "Epoch 41 finished in 0.076s (avg.: 0.085s). Training for 7.2s.\n",
            "\tLR: 0.01325\n",
            "Epoch 42 finished in 0.074s (avg.: 0.085s). Training for 7.2s.\n",
            "\tLR: 0.01311\n",
            "Epoch 43 finished in 0.078s (avg.: 0.085s). Training for 7.3s.\n",
            "\tLR: 0.01298\n",
            "Epoch 44 finished in 0.078s (avg.: 0.085s). Training for 7.4s.\n",
            "\tLR: 0.01285\n",
            "Epoch 45 finished in 0.078s (avg.: 0.085s). Training for 7.5s.\n",
            "\tLR: 0.01272\n",
            "Epoch 46 finished in 0.082s (avg.: 0.085s). Training for 7.6s.\n",
            "\tLR: 0.0126\n",
            "Epoch 47 finished in 0.082s (avg.: 0.085s). Training for 7.6s.\n",
            "\tLR: 0.01247\n",
            "Epoch 48 finished in 0.084s (avg.: 0.085s). Training for 7.7s.\n",
            "\tLR: 0.01235\n",
            "Epoch 49 finished in 0.076s (avg.: 0.084s). Training for 7.8s.\n",
            "\tLR: 0.01222\n",
            "Epoch 50 finished in 0.079s (avg.: 0.084s). Training for 7.9s.\n",
            "\tLR: 0.0121\n",
            "Epoch 51 finished in 0.079s (avg.: 0.084s). Training for 8.0s.\n",
            "\tLR: 0.01198\n",
            "Epoch 52 finished in 0.095s (avg.: 0.084s). Training for 8.1s.\n",
            "\tLR: 0.01186\n",
            "Epoch 53 finished in 0.087s (avg.: 0.084s). Training for 8.1s.\n",
            "\tLR: 0.01174\n",
            "Epoch 54 finished in 0.077s (avg.: 0.084s). Training for 8.2s.\n",
            "\tLR: 0.01162\n",
            "Epoch 55 finished in 0.079s (avg.: 0.084s). Training for 8.3s.\n",
            "\tLR: 0.01151\n",
            "Epoch 56 finished in 0.079s (avg.: 0.084s). Training for 8.4s.\n",
            "\tLR: 0.01139\n",
            "Epoch 57 finished in 0.088s (avg.: 0.084s). Training for 8.5s.\n",
            "\tLR: 0.01128\n",
            "Epoch 58 finished in 0.080s (avg.: 0.084s). Training for 8.5s.\n",
            "\tLR: 0.01117\n",
            "Epoch 59 finished in 0.078s (avg.: 0.084s). Training for 8.6s.\n",
            "\tLR: 0.01105\n",
            "57967 60000\n",
            "\ttrain loss: 0.113341, accuracy: 0.9661\n",
            "9684 10000\n",
            "\ttest  loss: 0.102714, accuracy: 0.9684\n",
            "Epoch 60 finished in 0.083s (avg.: 0.084s). Training for 10.1s.\n",
            "\tLR: 0.01094\n",
            "Epoch 61 finished in 0.086s (avg.: 0.084s). Training for 10.1s.\n",
            "\tLR: 0.01083\n",
            "Epoch 62 finished in 0.099s (avg.: 0.084s). Training for 10.2s.\n",
            "\tLR: 0.01073\n",
            "Epoch 63 finished in 0.077s (avg.: 0.084s). Training for 10.3s.\n",
            "\tLR: 0.01062\n",
            "Epoch 64 finished in 0.083s (avg.: 0.084s). Training for 10.4s.\n",
            "\tLR: 0.01051\n",
            "Epoch 65 finished in 0.090s (avg.: 0.084s). Training for 10.5s.\n",
            "\tLR: 0.01041\n",
            "Epoch 66 finished in 0.074s (avg.: 0.084s). Training for 10.6s.\n",
            "\tLR: 0.0103\n",
            "Epoch 67 finished in 0.082s (avg.: 0.084s). Training for 10.7s.\n",
            "\tLR: 0.0102\n",
            "Epoch 68 finished in 0.076s (avg.: 0.084s). Training for 10.7s.\n",
            "\tLR: 0.0101\n",
            "Epoch 69 finished in 0.083s (avg.: 0.084s). Training for 10.8s.\n",
            "\tLR: 0.009997\n",
            "Epoch 70 finished in 0.078s (avg.: 0.084s). Training for 10.9s.\n",
            "\tLR: 0.009897\n",
            "Epoch 71 finished in 0.078s (avg.: 0.084s). Training for 11.0s.\n",
            "\tLR: 0.009798\n",
            "Epoch 72 finished in 0.083s (avg.: 0.084s). Training for 11.1s.\n",
            "\tLR: 0.0097\n",
            "Epoch 73 finished in 0.076s (avg.: 0.084s). Training for 11.1s.\n",
            "\tLR: 0.009603\n",
            "Epoch 74 finished in 0.086s (avg.: 0.084s). Training for 11.2s.\n",
            "\tLR: 0.009507\n",
            "Epoch 75 finished in 0.074s (avg.: 0.084s). Training for 11.3s.\n",
            "\tLR: 0.009412\n",
            "Epoch 76 finished in 0.076s (avg.: 0.083s). Training for 11.4s.\n",
            "\tLR: 0.009318\n",
            "Epoch 77 finished in 0.082s (avg.: 0.083s). Training for 11.5s.\n",
            "\tLR: 0.009224\n",
            "Epoch 78 finished in 0.082s (avg.: 0.083s). Training for 11.5s.\n",
            "\tLR: 0.009132\n",
            "Epoch 79 finished in 0.072s (avg.: 0.083s). Training for 11.6s.\n",
            "\tLR: 0.009041\n",
            "58023 60000\n",
            "\ttrain loss: 0.107308, accuracy: 0.9670\n",
            "9687 10000\n",
            "\ttest  loss: 0.096371, accuracy: 0.9687\n",
            "Epoch 80 finished in 0.077s (avg.: 0.083s). Training for 13.0s.\n",
            "\tLR: 0.00895\n",
            "Epoch 81 finished in 0.095s (avg.: 0.083s). Training for 13.1s.\n",
            "\tLR: 0.008861\n",
            "Epoch 82 finished in 0.075s (avg.: 0.083s). Training for 13.2s.\n",
            "\tLR: 0.008772\n",
            "Epoch 83 finished in 0.080s (avg.: 0.083s). Training for 13.3s.\n",
            "\tLR: 0.008685\n",
            "Epoch 84 finished in 0.079s (avg.: 0.083s). Training for 13.4s.\n",
            "\tLR: 0.008598\n",
            "Epoch 85 finished in 0.074s (avg.: 0.083s). Training for 13.4s.\n",
            "\tLR: 0.008512\n",
            "Epoch 86 finished in 0.081s (avg.: 0.083s). Training for 13.5s.\n",
            "\tLR: 0.008427\n",
            "Epoch 87 finished in 0.076s (avg.: 0.083s). Training for 13.6s.\n",
            "\tLR: 0.008342\n",
            "Epoch 88 finished in 0.085s (avg.: 0.083s). Training for 13.7s.\n",
            "\tLR: 0.008259\n",
            "Epoch 89 finished in 0.135s (avg.: 0.084s). Training for 13.8s.\n",
            "\tLR: 0.008176\n",
            "Epoch 90 finished in 0.112s (avg.: 0.084s). Training for 13.9s.\n",
            "\tLR: 0.008095\n",
            "Epoch 91 finished in 0.109s (avg.: 0.084s). Training for 14.0s.\n",
            "\tLR: 0.008014\n",
            "Epoch 92 finished in 0.123s (avg.: 0.085s). Training for 14.2s.\n",
            "\tLR: 0.007934\n",
            "Epoch 93 finished in 0.115s (avg.: 0.085s). Training for 14.3s.\n",
            "\tLR: 0.007854\n",
            "Epoch 94 finished in 0.132s (avg.: 0.085s). Training for 14.4s.\n",
            "\tLR: 0.007776\n",
            "Epoch 95 finished in 0.119s (avg.: 0.086s). Training for 14.5s.\n",
            "\tLR: 0.007698\n",
            "Epoch 96 finished in 0.133s (avg.: 0.086s). Training for 14.7s.\n",
            "\tLR: 0.007621\n",
            "Epoch 97 finished in 0.118s (avg.: 0.086s). Training for 14.8s.\n",
            "\tLR: 0.007545\n",
            "Epoch 98 finished in 0.113s (avg.: 0.087s). Training for 14.9s.\n",
            "\tLR: 0.007469\n",
            "Epoch 99 finished in 0.132s (avg.: 0.087s). Training for 15.0s.\n",
            "\tLR: 0.007395\n",
            "58167 60000\n",
            "\ttrain loss: 0.099715, accuracy: 0.9695\n",
            "9723 10000\n",
            "\ttest  loss: 0.087307, accuracy: 0.9723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune the compressed_model2"
      ],
      "metadata": {
        "id": "rnu_I8dSjxhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = 0\n",
        "epoch = 0\n",
        "all_start_time = time.time()\n",
        "epoch_time = AverageMeter()\n",
        "\n",
        "optimizer = torch.optim.SGD(compressed_model2.parameters(), lr, momentum=momentum, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start_time = time.time()\n",
        "    compressed_model2.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = compressed_model2.forward(x)\n",
        "        loss = torch.nn.functional.cross_entropy(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "    end_time = time.time()\n",
        "    epoch_time.update(end_time - start_time)\n",
        "    training_time = end_time - all_start_time\n",
        "    compressed_model2.eval()\n",
        "    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n",
        "    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n",
        "    if (epoch+1) % print_freq == 0:\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, compressed_model2)\n",
        "        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model2)\n",
        "        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "    scheduler.step()\n",
        "\n",
        "    if checkpoint and (epoch+1) % checkpoint == 0:\n",
        "        # create and save checkpoint here\n",
        "        to_save = {}\n",
        "        to_save['model_state'] = compressed_model2.state_dict()\n",
        "        to_save['optimizer_state'] = optimizer.state_dict()\n",
        "        to_save['lr'] = scheduler.get_last_lr()\n",
        "        to_save['epoch'] = epoch + 1\n",
        "        torch.save(to_save, './compressed_lenet5_v2_checkpoint.pth.tar')"
      ],
      "metadata": {
        "id": "soUrac4zGBAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5fded9-49c0-482f-d8e3-9f08ade0307a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 finished in 0.085s (avg.: 0.085s). Training for 0.1s.\n",
            "\tLR: 0.02\n",
            "Epoch 1 finished in 0.085s (avg.: 0.085s). Training for 0.2s.\n",
            "\tLR: 0.0198\n",
            "Epoch 2 finished in 0.074s (avg.: 0.082s). Training for 0.2s.\n",
            "\tLR: 0.0196\n",
            "Epoch 3 finished in 0.081s (avg.: 0.081s). Training for 0.3s.\n",
            "\tLR: 0.01941\n",
            "Epoch 4 finished in 0.092s (avg.: 0.084s). Training for 0.4s.\n",
            "\tLR: 0.01921\n",
            "Epoch 5 finished in 0.078s (avg.: 0.083s). Training for 0.5s.\n",
            "\tLR: 0.01902\n",
            "Epoch 6 finished in 0.084s (avg.: 0.083s). Training for 0.6s.\n",
            "\tLR: 0.01883\n",
            "Epoch 7 finished in 0.077s (avg.: 0.082s). Training for 0.7s.\n",
            "\tLR: 0.01864\n",
            "Epoch 8 finished in 0.080s (avg.: 0.082s). Training for 0.7s.\n",
            "\tLR: 0.01845\n",
            "Epoch 9 finished in 0.095s (avg.: 0.083s). Training for 0.8s.\n",
            "\tLR: 0.01827\n",
            "Epoch 10 finished in 0.088s (avg.: 0.084s). Training for 0.9s.\n",
            "\tLR: 0.01809\n",
            "Epoch 11 finished in 0.088s (avg.: 0.084s). Training for 1.0s.\n",
            "\tLR: 0.01791\n",
            "Epoch 12 finished in 0.076s (avg.: 0.083s). Training for 1.1s.\n",
            "\tLR: 0.01773\n",
            "Epoch 13 finished in 0.083s (avg.: 0.083s). Training for 1.2s.\n",
            "\tLR: 0.01755\n",
            "Epoch 14 finished in 0.080s (avg.: 0.083s). Training for 1.3s.\n",
            "\tLR: 0.01737\n",
            "Epoch 15 finished in 0.085s (avg.: 0.083s). Training for 1.3s.\n",
            "\tLR: 0.0172\n",
            "Epoch 16 finished in 0.097s (avg.: 0.084s). Training for 1.4s.\n",
            "\tLR: 0.01703\n",
            "Epoch 17 finished in 0.081s (avg.: 0.084s). Training for 1.5s.\n",
            "\tLR: 0.01686\n",
            "Epoch 18 finished in 0.088s (avg.: 0.084s). Training for 1.6s.\n",
            "\tLR: 0.01669\n",
            "Epoch 19 finished in 0.077s (avg.: 0.084s). Training for 1.7s.\n",
            "\tLR: 0.01652\n",
            "55920 60000\n",
            "\ttrain loss: 0.218990, accuracy: 0.9320\n",
            "9380 10000\n",
            "\ttest  loss: 0.204543, accuracy: 0.9380\n",
            "Epoch 20 finished in 0.084s (avg.: 0.084s). Training for 3.1s.\n",
            "\tLR: 0.01636\n",
            "Epoch 21 finished in 0.084s (avg.: 0.084s). Training for 3.2s.\n",
            "\tLR: 0.01619\n",
            "Epoch 22 finished in 0.079s (avg.: 0.084s). Training for 3.3s.\n",
            "\tLR: 0.01603\n",
            "Epoch 23 finished in 0.081s (avg.: 0.083s). Training for 3.4s.\n",
            "\tLR: 0.01587\n",
            "Epoch 24 finished in 0.094s (avg.: 0.084s). Training for 3.5s.\n",
            "\tLR: 0.01571\n",
            "Epoch 25 finished in 0.089s (avg.: 0.084s). Training for 3.6s.\n",
            "\tLR: 0.01556\n",
            "Epoch 26 finished in 0.086s (avg.: 0.084s). Training for 3.6s.\n",
            "\tLR: 0.0154\n",
            "Epoch 27 finished in 0.082s (avg.: 0.084s). Training for 3.7s.\n",
            "\tLR: 0.01525\n",
            "Epoch 28 finished in 0.081s (avg.: 0.084s). Training for 3.8s.\n",
            "\tLR: 0.01509\n",
            "Epoch 29 finished in 0.081s (avg.: 0.084s). Training for 3.9s.\n",
            "\tLR: 0.01494\n",
            "Epoch 30 finished in 0.075s (avg.: 0.084s). Training for 4.0s.\n",
            "\tLR: 0.01479\n",
            "Epoch 31 finished in 0.078s (avg.: 0.083s). Training for 4.0s.\n",
            "\tLR: 0.01465\n",
            "Epoch 32 finished in 0.083s (avg.: 0.083s). Training for 4.1s.\n",
            "\tLR: 0.0145\n",
            "Epoch 33 finished in 0.085s (avg.: 0.083s). Training for 4.2s.\n",
            "\tLR: 0.01435\n",
            "Epoch 34 finished in 0.079s (avg.: 0.083s). Training for 4.3s.\n",
            "\tLR: 0.01421\n",
            "Epoch 35 finished in 0.077s (avg.: 0.083s). Training for 4.4s.\n",
            "\tLR: 0.01407\n",
            "Epoch 36 finished in 0.076s (avg.: 0.083s). Training for 4.4s.\n",
            "\tLR: 0.01393\n",
            "Epoch 37 finished in 0.093s (avg.: 0.083s). Training for 4.5s.\n",
            "\tLR: 0.01379\n",
            "Epoch 38 finished in 0.080s (avg.: 0.083s). Training for 4.6s.\n",
            "\tLR: 0.01365\n",
            "Epoch 39 finished in 0.080s (avg.: 0.083s). Training for 4.7s.\n",
            "\tLR: 0.01351\n",
            "56622 60000\n",
            "\ttrain loss: 0.182009, accuracy: 0.9437\n",
            "9483 10000\n",
            "\ttest  loss: 0.166750, accuracy: 0.9483\n",
            "Epoch 40 finished in 0.084s (avg.: 0.083s). Training for 6.1s.\n",
            "\tLR: 0.01338\n",
            "Epoch 41 finished in 0.085s (avg.: 0.083s). Training for 6.2s.\n",
            "\tLR: 0.01325\n",
            "Epoch 42 finished in 0.082s (avg.: 0.083s). Training for 6.3s.\n",
            "\tLR: 0.01311\n",
            "Epoch 43 finished in 0.090s (avg.: 0.083s). Training for 6.4s.\n",
            "\tLR: 0.01298\n",
            "Epoch 44 finished in 0.089s (avg.: 0.083s). Training for 6.5s.\n",
            "\tLR: 0.01285\n",
            "Epoch 45 finished in 0.093s (avg.: 0.084s). Training for 6.6s.\n",
            "\tLR: 0.01272\n",
            "Epoch 46 finished in 0.087s (avg.: 0.084s). Training for 6.7s.\n",
            "\tLR: 0.0126\n",
            "Epoch 47 finished in 0.090s (avg.: 0.084s). Training for 6.8s.\n",
            "\tLR: 0.01247\n",
            "Epoch 48 finished in 0.084s (avg.: 0.084s). Training for 6.8s.\n",
            "\tLR: 0.01235\n",
            "Epoch 49 finished in 0.081s (avg.: 0.084s). Training for 6.9s.\n",
            "\tLR: 0.01222\n",
            "Epoch 50 finished in 0.093s (avg.: 0.084s). Training for 7.0s.\n",
            "\tLR: 0.0121\n",
            "Epoch 51 finished in 0.083s (avg.: 0.084s). Training for 7.1s.\n",
            "\tLR: 0.01198\n",
            "Epoch 52 finished in 0.089s (avg.: 0.084s). Training for 7.2s.\n",
            "\tLR: 0.01186\n",
            "Epoch 53 finished in 0.079s (avg.: 0.084s). Training for 7.3s.\n",
            "\tLR: 0.01174\n",
            "Epoch 54 finished in 0.085s (avg.: 0.084s). Training for 7.4s.\n",
            "\tLR: 0.01162\n",
            "Epoch 55 finished in 0.080s (avg.: 0.084s). Training for 7.4s.\n",
            "\tLR: 0.01151\n",
            "Epoch 56 finished in 0.077s (avg.: 0.084s). Training for 7.5s.\n",
            "\tLR: 0.01139\n",
            "Epoch 57 finished in 0.087s (avg.: 0.084s). Training for 7.6s.\n",
            "\tLR: 0.01128\n",
            "Epoch 58 finished in 0.092s (avg.: 0.084s). Training for 7.7s.\n",
            "\tLR: 0.01117\n",
            "Epoch 59 finished in 0.085s (avg.: 0.084s). Training for 7.8s.\n",
            "\tLR: 0.01105\n",
            "57067 60000\n",
            "\ttrain loss: 0.162730, accuracy: 0.9511\n",
            "9545 10000\n",
            "\ttest  loss: 0.152350, accuracy: 0.9545\n",
            "Epoch 60 finished in 0.114s (avg.: 0.084s). Training for 9.2s.\n",
            "\tLR: 0.01094\n",
            "Epoch 61 finished in 0.130s (avg.: 0.085s). Training for 9.4s.\n",
            "\tLR: 0.01083\n",
            "Epoch 62 finished in 0.138s (avg.: 0.086s). Training for 9.5s.\n",
            "\tLR: 0.01073\n",
            "Epoch 63 finished in 0.162s (avg.: 0.087s). Training for 9.7s.\n",
            "\tLR: 0.01062\n",
            "Epoch 64 finished in 0.135s (avg.: 0.088s). Training for 9.8s.\n",
            "\tLR: 0.01051\n",
            "Epoch 65 finished in 0.135s (avg.: 0.089s). Training for 9.9s.\n",
            "\tLR: 0.01041\n",
            "Epoch 66 finished in 0.138s (avg.: 0.089s). Training for 10.1s.\n",
            "\tLR: 0.0103\n",
            "Epoch 67 finished in 0.120s (avg.: 0.090s). Training for 10.2s.\n",
            "\tLR: 0.0102\n",
            "Epoch 68 finished in 0.132s (avg.: 0.090s). Training for 10.3s.\n",
            "\tLR: 0.0101\n",
            "Epoch 69 finished in 0.129s (avg.: 0.091s). Training for 10.5s.\n",
            "\tLR: 0.009997\n",
            "Epoch 70 finished in 0.114s (avg.: 0.091s). Training for 10.6s.\n",
            "\tLR: 0.009897\n",
            "Epoch 71 finished in 0.142s (avg.: 0.092s). Training for 10.7s.\n",
            "\tLR: 0.009798\n",
            "Epoch 72 finished in 0.132s (avg.: 0.093s). Training for 10.9s.\n",
            "\tLR: 0.0097\n",
            "Epoch 73 finished in 0.140s (avg.: 0.093s). Training for 11.0s.\n",
            "\tLR: 0.009603\n",
            "Epoch 74 finished in 0.134s (avg.: 0.094s). Training for 11.1s.\n",
            "\tLR: 0.009507\n",
            "Epoch 75 finished in 0.126s (avg.: 0.094s). Training for 11.3s.\n",
            "\tLR: 0.009412\n",
            "Epoch 76 finished in 0.132s (avg.: 0.095s). Training for 11.4s.\n",
            "\tLR: 0.009318\n",
            "Epoch 77 finished in 0.109s (avg.: 0.095s). Training for 11.5s.\n",
            "\tLR: 0.009224\n",
            "Epoch 78 finished in 0.082s (avg.: 0.095s). Training for 11.6s.\n",
            "\tLR: 0.009132\n",
            "Epoch 79 finished in 0.086s (avg.: 0.095s). Training for 11.7s.\n",
            "\tLR: 0.009041\n",
            "57219 60000\n",
            "\ttrain loss: 0.152966, accuracy: 0.9536\n",
            "9561 10000\n",
            "\ttest  loss: 0.143904, accuracy: 0.9561\n",
            "Epoch 80 finished in 0.078s (avg.: 0.094s). Training for 13.1s.\n",
            "\tLR: 0.00895\n",
            "Epoch 81 finished in 0.076s (avg.: 0.094s). Training for 13.2s.\n",
            "\tLR: 0.008861\n",
            "Epoch 82 finished in 0.084s (avg.: 0.094s). Training for 13.3s.\n",
            "\tLR: 0.008772\n",
            "Epoch 83 finished in 0.081s (avg.: 0.094s). Training for 13.3s.\n",
            "\tLR: 0.008685\n",
            "Epoch 84 finished in 0.074s (avg.: 0.094s). Training for 13.4s.\n",
            "\tLR: 0.008598\n",
            "Epoch 85 finished in 0.087s (avg.: 0.094s). Training for 13.5s.\n",
            "\tLR: 0.008512\n",
            "Epoch 86 finished in 0.084s (avg.: 0.093s). Training for 13.6s.\n",
            "\tLR: 0.008427\n",
            "Epoch 87 finished in 0.083s (avg.: 0.093s). Training for 13.7s.\n",
            "\tLR: 0.008342\n",
            "Epoch 88 finished in 0.096s (avg.: 0.093s). Training for 13.8s.\n",
            "\tLR: 0.008259\n",
            "Epoch 89 finished in 0.093s (avg.: 0.093s). Training for 13.9s.\n",
            "\tLR: 0.008176\n",
            "Epoch 90 finished in 0.082s (avg.: 0.093s). Training for 13.9s.\n",
            "\tLR: 0.008095\n",
            "Epoch 91 finished in 0.083s (avg.: 0.093s). Training for 14.0s.\n",
            "\tLR: 0.008014\n",
            "Epoch 92 finished in 0.091s (avg.: 0.093s). Training for 14.1s.\n",
            "\tLR: 0.007934\n",
            "Epoch 93 finished in 0.080s (avg.: 0.093s). Training for 14.2s.\n",
            "\tLR: 0.007854\n",
            "Epoch 94 finished in 0.083s (avg.: 0.093s). Training for 14.3s.\n",
            "\tLR: 0.007776\n",
            "Epoch 95 finished in 0.083s (avg.: 0.093s). Training for 14.4s.\n",
            "\tLR: 0.007698\n",
            "Epoch 96 finished in 0.088s (avg.: 0.093s). Training for 14.4s.\n",
            "\tLR: 0.007621\n",
            "Epoch 97 finished in 0.074s (avg.: 0.093s). Training for 14.5s.\n",
            "\tLR: 0.007545\n",
            "Epoch 98 finished in 0.079s (avg.: 0.092s). Training for 14.6s.\n",
            "\tLR: 0.007469\n",
            "Epoch 99 finished in 0.084s (avg.: 0.092s). Training for 14.7s.\n",
            "\tLR: 0.007395\n",
            "57255 60000\n",
            "\ttrain loss: 0.148936, accuracy: 0.9543\n",
            "9569 10000\n",
            "\ttest  loss: 0.137864, accuracy: 0.9569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune the compressed_model3"
      ],
      "metadata": {
        "id": "uHBCHKlwjzBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = 0\n",
        "epoch = 0\n",
        "all_start_time = time.time()\n",
        "epoch_time = AverageMeter()\n",
        "\n",
        "optimizer = torch.optim.SGD(compressed_model3.parameters(), lr, momentum=momentum, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=start_epoch - 1)\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    start_time = time.time()\n",
        "    compressed_model3.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "        out = compressed_model3.forward(x)\n",
        "        loss = torch.nn.functional.cross_entropy(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "    end_time = time.time()\n",
        "    epoch_time.update(end_time - start_time)\n",
        "    training_time = end_time - all_start_time\n",
        "    compressed_model3.eval()\n",
        "    print('Epoch {0} finished in {et.val:.3f}s (avg.: {et.avg:.3f}s). Training for {1}'.format(epoch, format_time(training_time), et=epoch_time))\n",
        "    print('\\tLR: {:.4}'.format(scheduler.get_last_lr()[0]))\n",
        "    if (epoch+1) % print_freq == 0:\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, train_loader, compressed_model3)\n",
        "        print('\\ttrain loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "        accuracy, ave_loss = compute_acc_loss(my_eval, test_loader, compressed_model3)\n",
        "        print('\\ttest  loss: {:.6f}, accuracy: {:.4f}'.format(ave_loss, accuracy))\n",
        "    scheduler.step()\n",
        "\n",
        "    if checkpoint and (epoch+1) % checkpoint == 0:\n",
        "        # create and save checkpoint here\n",
        "        to_save = {}\n",
        "        to_save['model_state'] = compressed_model3.state_dict()\n",
        "        to_save['optimizer_state'] = optimizer.state_dict()\n",
        "        to_save['lr'] = scheduler.get_last_lr()\n",
        "        to_save['epoch'] = epoch + 1\n",
        "        torch.save(to_save, './compressed_lenet5_v1_checkpoint.pth.tar')"
      ],
      "metadata": {
        "id": "opTbHeSFhSSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a487805-e0ab-4d4f-bd9a-33caff86e465"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 finished in 0.087s (avg.: 0.087s). Training for 0.1s.\n",
            "\tLR: 0.02\n",
            "Epoch 1 finished in 0.093s (avg.: 0.090s). Training for 0.2s.\n",
            "\tLR: 0.0198\n",
            "Epoch 2 finished in 0.081s (avg.: 0.087s). Training for 0.3s.\n",
            "\tLR: 0.0196\n",
            "Epoch 3 finished in 0.089s (avg.: 0.088s). Training for 0.4s.\n",
            "\tLR: 0.01941\n",
            "Epoch 4 finished in 0.088s (avg.: 0.088s). Training for 0.4s.\n",
            "\tLR: 0.01921\n",
            "Epoch 5 finished in 0.087s (avg.: 0.088s). Training for 0.5s.\n",
            "\tLR: 0.01902\n",
            "Epoch 6 finished in 0.084s (avg.: 0.087s). Training for 0.6s.\n",
            "\tLR: 0.01883\n",
            "Epoch 7 finished in 0.078s (avg.: 0.086s). Training for 0.7s.\n",
            "\tLR: 0.01864\n",
            "Epoch 8 finished in 0.102s (avg.: 0.088s). Training for 0.8s.\n",
            "\tLR: 0.01845\n",
            "Epoch 9 finished in 0.084s (avg.: 0.087s). Training for 0.9s.\n",
            "\tLR: 0.01827\n",
            "Epoch 10 finished in 0.097s (avg.: 0.088s). Training for 1.0s.\n",
            "\tLR: 0.01809\n",
            "Epoch 11 finished in 0.076s (avg.: 0.087s). Training for 1.1s.\n",
            "\tLR: 0.01791\n",
            "Epoch 12 finished in 0.080s (avg.: 0.087s). Training for 1.1s.\n",
            "\tLR: 0.01773\n",
            "Epoch 13 finished in 0.078s (avg.: 0.086s). Training for 1.2s.\n",
            "\tLR: 0.01755\n",
            "Epoch 14 finished in 0.085s (avg.: 0.086s). Training for 1.3s.\n",
            "\tLR: 0.01737\n",
            "Epoch 15 finished in 0.080s (avg.: 0.086s). Training for 1.4s.\n",
            "\tLR: 0.0172\n",
            "Epoch 16 finished in 0.086s (avg.: 0.086s). Training for 1.5s.\n",
            "\tLR: 0.01703\n",
            "Epoch 17 finished in 0.084s (avg.: 0.085s). Training for 1.5s.\n",
            "\tLR: 0.01686\n",
            "Epoch 18 finished in 0.092s (avg.: 0.086s). Training for 1.6s.\n",
            "\tLR: 0.01669\n",
            "Epoch 19 finished in 0.084s (avg.: 0.086s). Training for 1.7s.\n",
            "\tLR: 0.01652\n",
            "48347 60000\n",
            "\ttrain loss: 0.576292, accuracy: 0.8058\n",
            "8083 10000\n",
            "\ttest  loss: 0.560897, accuracy: 0.8083\n",
            "Epoch 20 finished in 0.085s (avg.: 0.086s). Training for 3.1s.\n",
            "\tLR: 0.01636\n",
            "Epoch 21 finished in 0.080s (avg.: 0.085s). Training for 3.2s.\n",
            "\tLR: 0.01619\n",
            "Epoch 22 finished in 0.070s (avg.: 0.085s). Training for 3.2s.\n",
            "\tLR: 0.01603\n",
            "Epoch 23 finished in 0.079s (avg.: 0.085s). Training for 3.3s.\n",
            "\tLR: 0.01587\n",
            "Epoch 24 finished in 0.088s (avg.: 0.085s). Training for 3.4s.\n",
            "\tLR: 0.01571\n",
            "Epoch 25 finished in 0.088s (avg.: 0.085s). Training for 3.5s.\n",
            "\tLR: 0.01556\n",
            "Epoch 26 finished in 0.084s (avg.: 0.085s). Training for 3.6s.\n",
            "\tLR: 0.0154\n",
            "Epoch 27 finished in 0.082s (avg.: 0.085s). Training for 3.7s.\n",
            "\tLR: 0.01525\n",
            "Epoch 28 finished in 0.085s (avg.: 0.085s). Training for 3.8s.\n",
            "\tLR: 0.01509\n",
            "Epoch 29 finished in 0.087s (avg.: 0.085s). Training for 3.8s.\n",
            "\tLR: 0.01494\n",
            "Epoch 30 finished in 0.077s (avg.: 0.085s). Training for 3.9s.\n",
            "\tLR: 0.01479\n",
            "Epoch 31 finished in 0.094s (avg.: 0.085s). Training for 4.0s.\n",
            "\tLR: 0.01465\n",
            "Epoch 32 finished in 0.083s (avg.: 0.085s). Training for 4.1s.\n",
            "\tLR: 0.0145\n",
            "Epoch 33 finished in 0.096s (avg.: 0.085s). Training for 4.2s.\n",
            "\tLR: 0.01435\n",
            "Epoch 34 finished in 0.087s (avg.: 0.085s). Training for 4.3s.\n",
            "\tLR: 0.01421\n",
            "Epoch 35 finished in 0.087s (avg.: 0.085s). Training for 4.4s.\n",
            "\tLR: 0.01407\n",
            "Epoch 36 finished in 0.088s (avg.: 0.085s). Training for 4.5s.\n",
            "\tLR: 0.01393\n",
            "Epoch 37 finished in 0.074s (avg.: 0.085s). Training for 4.5s.\n",
            "\tLR: 0.01379\n",
            "Epoch 38 finished in 0.092s (avg.: 0.085s). Training for 4.6s.\n",
            "\tLR: 0.01365\n",
            "Epoch 39 finished in 0.087s (avg.: 0.085s). Training for 4.7s.\n",
            "\tLR: 0.01351\n",
            "52085 60000\n",
            "\ttrain loss: 0.424174, accuracy: 0.8681\n",
            "8720 10000\n",
            "\ttest  loss: 0.409006, accuracy: 0.8720\n",
            "Epoch 40 finished in 0.309s (avg.: 0.091s). Training for 7.2s.\n",
            "\tLR: 0.01338\n",
            "Epoch 41 finished in 0.130s (avg.: 0.092s). Training for 7.4s.\n",
            "\tLR: 0.01325\n",
            "Epoch 42 finished in 0.156s (avg.: 0.093s). Training for 7.5s.\n",
            "\tLR: 0.01311\n",
            "Epoch 43 finished in 0.125s (avg.: 0.094s). Training for 7.6s.\n",
            "\tLR: 0.01298\n",
            "Epoch 44 finished in 0.270s (avg.: 0.098s). Training for 7.9s.\n",
            "\tLR: 0.01285\n",
            "Epoch 45 finished in 0.206s (avg.: 0.100s). Training for 8.1s.\n",
            "\tLR: 0.01272\n",
            "Epoch 46 finished in 0.114s (avg.: 0.100s). Training for 8.2s.\n",
            "\tLR: 0.0126\n",
            "Epoch 47 finished in 0.079s (avg.: 0.100s). Training for 8.3s.\n",
            "\tLR: 0.01247\n",
            "Epoch 48 finished in 0.079s (avg.: 0.100s). Training for 8.4s.\n",
            "\tLR: 0.01235\n",
            "Epoch 49 finished in 0.079s (avg.: 0.099s). Training for 8.5s.\n",
            "\tLR: 0.01222\n",
            "Epoch 50 finished in 0.081s (avg.: 0.099s). Training for 8.6s.\n",
            "\tLR: 0.0121\n",
            "Epoch 51 finished in 0.079s (avg.: 0.098s). Training for 8.6s.\n",
            "\tLR: 0.01198\n",
            "Epoch 52 finished in 0.084s (avg.: 0.098s). Training for 8.7s.\n",
            "\tLR: 0.01186\n",
            "Epoch 53 finished in 0.204s (avg.: 0.100s). Training for 8.9s.\n",
            "\tLR: 0.01174\n",
            "Epoch 54 finished in 0.164s (avg.: 0.101s). Training for 9.1s.\n",
            "\tLR: 0.01162\n",
            "Epoch 55 finished in 0.077s (avg.: 0.101s). Training for 9.2s.\n",
            "\tLR: 0.01151\n",
            "Epoch 56 finished in 0.083s (avg.: 0.101s). Training for 9.2s.\n",
            "\tLR: 0.01139\n",
            "Epoch 57 finished in 0.081s (avg.: 0.100s). Training for 9.3s.\n",
            "\tLR: 0.01128\n",
            "Epoch 58 finished in 0.088s (avg.: 0.100s). Training for 9.4s.\n",
            "\tLR: 0.01117\n",
            "Epoch 59 finished in 0.080s (avg.: 0.100s). Training for 9.5s.\n",
            "\tLR: 0.01105\n",
            "54061 60000\n",
            "\ttrain loss: 0.332365, accuracy: 0.9010\n",
            "9044 10000\n",
            "\ttest  loss: 0.317702, accuracy: 0.9044\n",
            "Epoch 60 finished in 0.089s (avg.: 0.099s). Training for 10.9s.\n",
            "\tLR: 0.01094\n",
            "Epoch 61 finished in 0.086s (avg.: 0.099s). Training for 11.0s.\n",
            "\tLR: 0.01083\n",
            "Epoch 62 finished in 0.076s (avg.: 0.099s). Training for 11.0s.\n",
            "\tLR: 0.01073\n",
            "Epoch 63 finished in 0.088s (avg.: 0.099s). Training for 11.1s.\n",
            "\tLR: 0.01062\n",
            "Epoch 64 finished in 0.088s (avg.: 0.099s). Training for 11.2s.\n",
            "\tLR: 0.01051\n",
            "Epoch 65 finished in 0.083s (avg.: 0.098s). Training for 11.3s.\n",
            "\tLR: 0.01041\n",
            "Epoch 66 finished in 0.079s (avg.: 0.098s). Training for 11.4s.\n",
            "\tLR: 0.0103\n",
            "Epoch 67 finished in 0.086s (avg.: 0.098s). Training for 11.5s.\n",
            "\tLR: 0.0102\n",
            "Epoch 68 finished in 0.095s (avg.: 0.098s). Training for 11.6s.\n",
            "\tLR: 0.0101\n",
            "Epoch 69 finished in 0.082s (avg.: 0.098s). Training for 11.6s.\n",
            "\tLR: 0.009997\n",
            "Epoch 70 finished in 0.087s (avg.: 0.097s). Training for 11.7s.\n",
            "\tLR: 0.009897\n",
            "Epoch 71 finished in 0.085s (avg.: 0.097s). Training for 11.8s.\n",
            "\tLR: 0.009798\n",
            "Epoch 72 finished in 0.080s (avg.: 0.097s). Training for 11.9s.\n",
            "\tLR: 0.0097\n",
            "Epoch 73 finished in 0.083s (avg.: 0.097s). Training for 12.0s.\n",
            "\tLR: 0.009603\n",
            "Epoch 74 finished in 0.083s (avg.: 0.097s). Training for 12.1s.\n",
            "\tLR: 0.009507\n",
            "Epoch 75 finished in 0.097s (avg.: 0.097s). Training for 12.2s.\n",
            "\tLR: 0.009412\n",
            "Epoch 76 finished in 0.081s (avg.: 0.096s). Training for 12.2s.\n",
            "\tLR: 0.009318\n",
            "Epoch 77 finished in 0.085s (avg.: 0.096s). Training for 12.3s.\n",
            "\tLR: 0.009224\n",
            "Epoch 78 finished in 0.085s (avg.: 0.096s). Training for 12.4s.\n",
            "\tLR: 0.009132\n",
            "Epoch 79 finished in 0.086s (avg.: 0.096s). Training for 12.5s.\n",
            "\tLR: 0.009041\n",
            "54666 60000\n",
            "\ttrain loss: 0.300335, accuracy: 0.9111\n",
            "9183 10000\n",
            "\ttest  loss: 0.282520, accuracy: 0.9183\n",
            "Epoch 80 finished in 0.091s (avg.: 0.096s). Training for 14.0s.\n",
            "\tLR: 0.00895\n",
            "Epoch 81 finished in 0.085s (avg.: 0.096s). Training for 14.0s.\n",
            "\tLR: 0.008861\n",
            "Epoch 82 finished in 0.091s (avg.: 0.096s). Training for 14.1s.\n",
            "\tLR: 0.008772\n",
            "Epoch 83 finished in 0.092s (avg.: 0.096s). Training for 14.2s.\n",
            "\tLR: 0.008685\n",
            "Epoch 84 finished in 0.081s (avg.: 0.096s). Training for 14.3s.\n",
            "\tLR: 0.008598\n",
            "Epoch 85 finished in 0.082s (avg.: 0.095s). Training for 14.4s.\n",
            "\tLR: 0.008512\n",
            "Epoch 86 finished in 0.097s (avg.: 0.095s). Training for 14.5s.\n",
            "\tLR: 0.008427\n",
            "Epoch 87 finished in 0.089s (avg.: 0.095s). Training for 14.6s.\n",
            "\tLR: 0.008342\n",
            "Epoch 88 finished in 0.079s (avg.: 0.095s). Training for 14.7s.\n",
            "\tLR: 0.008259\n",
            "Epoch 89 finished in 0.086s (avg.: 0.095s). Training for 14.7s.\n",
            "\tLR: 0.008176\n",
            "Epoch 90 finished in 0.083s (avg.: 0.095s). Training for 14.8s.\n",
            "\tLR: 0.008095\n",
            "Epoch 91 finished in 0.092s (avg.: 0.095s). Training for 14.9s.\n",
            "\tLR: 0.008014\n",
            "Epoch 92 finished in 0.080s (avg.: 0.095s). Training for 15.0s.\n",
            "\tLR: 0.007934\n",
            "Epoch 93 finished in 0.075s (avg.: 0.095s). Training for 15.1s.\n",
            "\tLR: 0.007854\n",
            "Epoch 94 finished in 0.101s (avg.: 0.095s). Training for 15.2s.\n",
            "\tLR: 0.007776\n",
            "Epoch 95 finished in 0.092s (avg.: 0.095s). Training for 15.3s.\n",
            "\tLR: 0.007698\n",
            "Epoch 96 finished in 0.086s (avg.: 0.094s). Training for 15.4s.\n",
            "\tLR: 0.007621\n",
            "Epoch 97 finished in 0.077s (avg.: 0.094s). Training for 15.4s.\n",
            "\tLR: 0.007545\n",
            "Epoch 98 finished in 0.085s (avg.: 0.094s). Training for 15.5s.\n",
            "\tLR: 0.007469\n",
            "Epoch 99 finished in 0.084s (avg.: 0.094s). Training for 15.6s.\n",
            "\tLR: 0.007395\n",
            "55200 60000\n",
            "\ttrain loss: 0.272651, accuracy: 0.9200\n",
            "9245 10000\n",
            "\ttest  loss: 0.258817, accuracy: 0.9245\n"
          ]
        }
      ]
    }
  ]
}