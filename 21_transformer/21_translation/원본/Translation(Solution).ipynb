{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1inXPd39KUbHSMp5Y6wVRPQCztTJeRTmt","timestamp":1692106865523},{"file_id":"1FC65ve-l_YqzWs7gNs4zTCAjwUiBrcCn","timestamp":1604048342097}],"collapsed_sections":["9XDVC3lJl1_H","XQZvsEeDmeZt","d09I8_00J2HM","iHQzs6ocPxtQ"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cb00c8026cee4566b2c73ce64536abeb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cb5deff2723347908b76ae2f9eec91a0","IPY_MODEL_8f46cc83a6c04b409374f43ac0027377","IPY_MODEL_8c42c9686c174417a5b952f4a7427702"],"layout":"IPY_MODEL_cdf85eba599846ada05cd2d126176611"}},"cb5deff2723347908b76ae2f9eec91a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_735ddae3fd8147e68bf377ddfb91fd99","placeholder":"​","style":"IPY_MODEL_d54d3c8672d5484ca6b1d45fa1acc345","value":"100%"}},"8f46cc83a6c04b409374f43ac0027377":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c35a249b27c4ca0bbbd6f7958fc7c96","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61a0fb6b876b480ab29d9ce011eb859d","value":20}},"8c42c9686c174417a5b952f4a7427702":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_684287022c5142f3b8a150563f8f1594","placeholder":"​","style":"IPY_MODEL_05c3d65d2e6d4f81babd55e073b1a23e","value":" 20/20 [34:46&lt;00:00, 102.74s/it]"}},"cdf85eba599846ada05cd2d126176611":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"735ddae3fd8147e68bf377ddfb91fd99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d54d3c8672d5484ca6b1d45fa1acc345":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c35a249b27c4ca0bbbd6f7958fc7c96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61a0fb6b876b480ab29d9ce011eb859d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"684287022c5142f3b8a150563f8f1594":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05c3d65d2e6d4f81babd55e073b1a23e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5760abca28ad486db1df3c10aa5e2399":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b63ebe3be9f48b49867c7c95bc2247c","IPY_MODEL_f64d22534b2548d5a65dc3a6b1db9fc3","IPY_MODEL_f343e581fe0f4e148c420f76d36304a1"],"layout":"IPY_MODEL_60a206562cbc42f1801e0792b448fca6"}},"3b63ebe3be9f48b49867c7c95bc2247c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5482a09a130474e8f7a3430c8b0e0e0","placeholder":"​","style":"IPY_MODEL_7aa6606d6a9641ff8d9b9c57cd1c16b2","value":"100%"}},"f64d22534b2548d5a65dc3a6b1db9fc3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5c88fa8af4b42c488cc91e232bea3a2","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba0bfe37489d4c418f647b0b7fb8cf06","value":20}},"f343e581fe0f4e148c420f76d36304a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3f2fcca64974b76b56209029c19b136","placeholder":"​","style":"IPY_MODEL_8501f9dd45bd4b9183c5f807bfa8d74b","value":" 20/20 [17:33&lt;00:00, 52.39s/it]"}},"60a206562cbc42f1801e0792b448fca6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5482a09a130474e8f7a3430c8b0e0e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7aa6606d6a9641ff8d9b9c57cd1c16b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5c88fa8af4b42c488cc91e232bea3a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba0bfe37489d4c418f647b0b7fb8cf06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3f2fcca64974b76b56209029c19b136":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8501f9dd45bd4b9183c5f807bfa8d74b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"AYXIHngPoI2W"},"source":["# Neural Machine Translation with Various Sequence Models\n","\n","## Instructions\n","- In this project, we will perform Neural Machine Translation with recurrent neural networks and attention based models on Multi30k dataset which include language pairs of German and English.\n","- To this end, you need to implement necessary network components (e.g. LSTMCell, Multi-head attention) using nn.Module class and complete whole models with those modules. Then, you will experiment those network architectures and report Bilingual Evaluation Understudy (BLEU) on the test set.\n","- Fill in the section marked **Px.x** with the appropriate code. **You can only modify inside those areas, and not the skeleton code.**\n","- To begin, you should download this ipynb file into your own Google drive clicking `make a copy(사본만들기)`. Find the copy in your drive, change their name to `Translation.ipynb`, if their names were changed to e.g. `Copy of Translation.ipynb` or `Translationipynb의 사본`.\n","- <font color=\"red\">You'll be training large models. We recommend you to create at least **1GB** of space available on your Google drive to run everything properly.</font>"]},{"cell_type":"markdown","metadata":{"id":"x1ZGXqrvlc_O"},"source":["---\n","# Prerequisite: Mount your gdrive."]},{"cell_type":"code","metadata":{"id":"a0HEy2Tok-2u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691933766655,"user_tz":-540,"elapsed":2470,"user":{"displayName":"김동균","userId":"17758291692024531705"}},"outputId":"5dd33d9a-0b40-47d5-ca92-cb988f198677"},"source":["# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n","# login with your google account and type authorization code to mount on your googlbie drive.\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"_Tsq4xR-liMH"},"source":["---\n","# Prerequisite: Setup the `root` directory properly."]},{"cell_type":"code","metadata":{"id":"SHzfVbfmloDz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691933766655,"user_tz":-540,"elapsed":5,"user":{"displayName":"김동균","userId":"17758291692024531705"}},"outputId":"9d8ba6ec-920f-48dc-e534-7097bf89e9a1"},"source":["# Specify the directory path where `Translation.ipynb` exists.\n","# For example, if you saved `Translation.ipynb` in `/gdrive/My Drive/samsung_ai` directory,\n","# then set root = '/gdrive/My Drive/samsung_ai'\n","root = '/gdrive/My Drive/Projects/samsung_2023'\n","\n","root_ = root.replace(' ', '\\ ')\n","!ls $root_"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'Copy of Translation.ipynb'   results  'Translation(Solution).ipynb'\n"]}]},{"cell_type":"markdown","metadata":{"id":"0ekROPNvqvBr"},"source":["---\n","# Prerequisite: Install libraries.\n","You only have to run this cell once per VM at startup."]},{"cell_type":"code","metadata":{"id":"xpWbZsp2quKI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691933807536,"user_tz":-540,"elapsed":40884,"user":{"displayName":"김동균","userId":"17758291692024531705"}},"outputId":"4966bb4c-f07a-4b2d-83e4-0ff5568eae6e"},"source":["!pip install torchtext==0.6.0\n","!pip install spacy\n","!python -m spacy download en\n","!python -m spacy download de"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchtext==0.6.0\n","  Using cached torchtext-0.6.0-py3-none-any.whl (64 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.27.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n","Installing collected packages: torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.15.2\n","    Uninstalling torchtext-0.15.2:\n","      Successfully uninstalled torchtext-0.15.2\n","Successfully installed torchtext-0.6.0\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.11)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n","Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.4.0)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n","2023-08-13 13:36:24.749036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-13 13:36:25.678279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-08-13 13:36:27.264969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-08-13 13:36:27.265576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-08-13 13:36:27.265789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n","full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n","Collecting en-core-web-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.11)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n","Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","2023-08-13 13:36:36.825269: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-13 13:36:37.770046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-08-13 13:36:39.312502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-08-13 13:36:39.312980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-08-13 13:36:39.313189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n","full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n","Collecting de-core-news-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.11)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.66.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.5.0)\n","Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n"]}]},{"cell_type":"markdown","metadata":{"id":"9XDVC3lJl1_H"},"source":["---\n","# Basic settings"]},{"cell_type":"markdown","metadata":{"id":"3jIiyjEGlwJ8"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"eGlC_yM9lvue"},"source":["import os\n","import numpy as np\n","import time\n","from pathlib import Path\n","import torch\n","import torch.nn as nn\n","from torch.nn.parameter import Parameter\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.optim import SGD\n","import torchtext\n","from torchtext.datasets import Multi30k\n","from torchtext.data import Field, BucketIterator\n","from torchtext.data.utils import get_tokenizer\n","from torchtext import data\n","from torchtext.data.metrics import bleu_score\n","import spacy\n","from spacy.symbols import ORTH\n","import math\n","import random\n","import tqdm.notebook as tq\n","import copy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W27cdKyEmJy1"},"source":["## Set Hyperparameters"]},{"cell_type":"code","metadata":{"id":"dczwhJI1l8Kl"},"source":["# Basic settings\n","torch.manual_seed(470)\n","torch.cuda.manual_seed(470)\n","\n","#!pip install easydict\n","from easydict import EasyDict as edict\n","\n","args = edict()\n","args.batch_size = 32\n","args.nlayers = 2\n","args.ninp = 256\n","args.nhid = 256 #512\n","\n","\n","args.clip = 1\n","args.lr_lstm = 0.001\n","args.dropout = 0.2\n","args.nhid_attn = 256\n","args.epochs = 20\n","\n","##### Transformer\n","args.nhid_tran = 256\n","args.nhead = 8\n","args.nlayers_transformer = 6\n","args.attn_pdrop = 0.1\n","args.resid_pdrop = 0.1\n","args.embd_pdrop = 0.1\n","args.nff = 4 * args.nhid_tran\n","\n","\n","args.lr_transformer = 0.0001 #1.0\n","args.betas = (0.9, 0.98)\n","\n","args.gpu = True\n","\n","\n","device = 'cuda:0' if torch.cuda.is_available() and args.gpu else 'cpu'\n","# Create directory name.\n","result_dir = Path(root) / 'results'\n","result_dir.mkdir(parents=True, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQZvsEeDmeZt"},"source":["---\n","# Utility functions\n"]},{"cell_type":"code","metadata":{"id":"LUOvr2A_mf4Q"},"source":["def word_ids_to_sentence(id_tensor, vocab, join=' '):\n","    \"\"\"Converts a sequence of word ids to a sentence\"\"\"\n","    if isinstance(id_tensor, torch.LongTensor):\n","        ids = id_tensor.transpose(0, 1).contiguous().view(-1)\n","    elif isinstance(id_tensor, np.ndarray):\n","        ids = id_tensor.transpose().reshape(-1)\n","    batch = [vocab.itos[ind] for ind in ids] # denumericalize\n","    if join is None:\n","        return batch\n","    else:\n","        return join.join(batch)\n","\n","# Extracts bias and non-bias parameters from a model.\n","def get_parameters(model, bias=False):\n","    for m in model.modules():\n","        if isinstance(m, nn.Linear):\n","            if bias:\n","                yield m.bias\n","            else:\n","                yield m.weight\n","        else:\n","            if not bias:\n","                yield m.parameters()\n","\n","def run_epoch(epoch, model, optimizer, is_train=True, data_iter=None):\n","    total_loss = 0\n","    n_correct = 0\n","    n_total = 0\n","    if data_iter is None:\n","        data_iter = train_iter if is_train else valid_iter\n","    if is_train:\n","        model.train()\n","    else:\n","        model.eval()\n","    for batch in data_iter:\n","        x, y, length = sort_batch(batch.src.to(device), batch.trg.to(device))\n","        target = y[1:]\n","        if isinstance(model, Transformer):\n","            x, y = x.transpose(0, 1), y.transpose(0, 1)\n","            target = target.transpose(0, 1) #y[:, 1:]\n","        pred = model(x, y, length)\n","        loss = criterion(pred.reshape(-1, trg_ntoken), target.reshape(-1))\n","        n_targets = (target != pad_id).long().sum().item()\n","        n_total += n_targets\n","        n_correct += (pred.argmax(-1) == target)[target != pad_id].long().sum().item()\n","        if is_train:\n","            optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","            optimizer.step()\n","\n","\n","        total_loss += loss.item() * n_targets\n","    total_loss /= n_total\n","    print(\"Epoch\", epoch, 'Train' if is_train else 'Valid',\n","          \"Loss\", np.mean(total_loss),\n","          \"Acc\", n_correct / n_total,\n","          \"PPL\", np.exp(total_loss))\n","    return total_loss\n","\n","def word_ids_to_sentence_(ids, vocab):\n","    sentence = []\n","    for ind in ids:\n","        if ind == eos_id:\n","            break\n","        sentence.append(vocab.itos[ind])\n","    return sentence\n","\n","def run_translation(model, data_iter, max_len=100, mode='best'):\n","    with torch.no_grad():\n","        model.eval()\n","        load_model(model, mode)\n","        src_list = []\n","        gt_list = []\n","        pred_list = []\n","        for batch in data_iter:\n","            x, y, length = sort_batch(batch.src.to(device), batch.trg.to(device))\n","            target = y[1:]\n","            if isinstance(model, Transformer):\n","                x, y = x.transpose(0, 1), y.transpose(0, 1)\n","                target = target.transpose(0, 1)\n","            pred = model(x, y, length, max_len=max_len, teacher_forcing=False)\n","            pred_token = pred.argmax(-1)\n","            if not isinstance(model, Transformer):\n","                pred_token = pred_token.transpose(0, 1).cpu().numpy()\n","                y = y.transpose(0, 1).cpu().numpy()\n","                x = x.transpose(0, 1).cpu().numpy()\n","            # pred_token : batch_size x max_len\n","            for x_, y_, pred_ in zip(x, y, pred_token):\n","                src_list.append(word_ids_to_sentence_(x_[1:], SRC.vocab))\n","                gt_list.append([word_ids_to_sentence_(y_[1:], TRG.vocab)])\n","                pred_list.append(word_ids_to_sentence_(pred_, TRG.vocab))\n","\n","        for i in range(5):\n","            print(f\"--------- Translation Example {i+1} ---------\")\n","            print(\"SRC :\", ' '.join(src_list[i]))\n","            print(\"TRG :\", ' '.join(gt_list[i][0]))\n","            print(\"PRED:\", ' '.join(pred_list[i]))\n","        print()\n","        print(\"BLEU:\", bleu_score(pred_list, gt_list))\n","\n","\n","\n","def save_model(model, mode=\"last\"):\n","    torch.save(model.state_dict(),  result_dir / f'{type(model).__name__}_{mode}.ckpt')\n","\n","def load_model(model, mode=\"last\"):\n","    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n","        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))\n","\n","def sort_batch(X, y, lengths=None):\n","    if lengths is None:\n","        lengths = (X != pad_id_src).long().sum(0)\n","    lengths, indx = lengths.sort(dim=0, descending=True)\n","    X = torch.index_select(X, 1, indx)\n","    y = torch.index_select(y, 1, indx)\n","    return X, y, lengths\n","\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# Download Multi30k Dataset"],"metadata":{"id":"d09I8_00J2HM"}},{"cell_type":"code","source":["!wget -P .data/multi30k/ https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\n","!wget -P .data/multi30k/ https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\n","!wget -P .data/multi30k/ https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt_task1_test2016.tar.gz\n","\n","!tar -xzf .data/multi30k/training.tar.gz\n","!tar -xzf .data/multi30k/validation.tar.gz\n","!tar -xzf .data/multi30k/mmt_task1_test2016.tar.gz\n","!mv train.de train.en val.de val.en test2016.de test2016.en .data/multi30k/"],"metadata":{"id":"8IG1bC9HJ1hQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7WMaIEVmoub"},"source":["---\n","# Define `DataLoader` for training & validation set\n"]},{"cell_type":"code","metadata":{"id":"deEP6JnaywMV"},"source":["SRC = Field(tokenize = \"spacy\",\n","            tokenizer_language=\"de_core_news_sm\",\n","            init_token = '<sos>',\n","            eos_token = '<eos>',\n","            lower = True)\n","\n","TRG = Field(tokenize = \"spacy\",\n","            tokenizer_language=\"en_core_web_sm\",\n","            init_token = '<sos>',\n","            eos_token = '<eos>',\n","            lower = True)\n","\n","train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n","                                                    fields = (SRC, TRG))\n","SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)\n","\n","src_ntoken = len(SRC.vocab.stoi)\n","trg_ntoken = len(TRG.vocab.stoi)\n","\n","train_iter, valid_iter, test_iter = BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size = args.batch_size,\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSAnejfCywMZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691934406107,"user_tz":-540,"elapsed":10696,"user":{"displayName":"김동균","userId":"17758291692024531705"}},"outputId":"efe2080b-931e-4874-95a2-3c6d8982b018"},"source":["pad_id_trg = TRG.vocab.stoi[TRG.pad_token]\n","pad_id_src = SRC.vocab.stoi[SRC.pad_token]\n","pad_id = pad_id_src\n","eos_id = TRG.vocab.stoi[TRG.eos_token]\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n","\n","for batch in train_iter:\n","    src, trg, length_src = sort_batch(batch.src, batch.trg)\n","    print(length_src)\n","    print(src, src.shape)\n","    print(trg, trg.shape)\n","    break\n","\n","print(\"##### EXAMPLE #####\")\n","print(\"SRC: \", word_ids_to_sentence(src[:, 1:2].long().cpu(), SRC.vocab))\n","print(\"TRG: \", word_ids_to_sentence(trg[:, 1:2].long().cpu(), TRG.vocab))\n","\n","print(\"SRC vocab size\", len(SRC.vocab.stoi))\n","print(\"TRG vocab size\", len(TRG.vocab.stoi))\n","print(\"Vocab\", list(SRC.vocab.stoi.items())[:10])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([22, 19, 18, 18, 17, 16, 16, 15, 15, 14, 14, 14, 13, 13, 13, 13, 13, 12,\n","        12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 10, 10,  9,  9],\n","       device='cuda:0')\n","tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n","            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n","            2,    2,    2,    2,    2,    2,    2,    2],\n","        [  18,    5,    5,    8,    5,    5,    5,    5,   18,    5,    5,    8,\n","            5, 2475,    5,    5,    5,   18,    8,    5,    5,    5,    5,    5,\n","          105,   18,   73,    8,    8,    5,    5,   43],\n","        [  45,  271,   13,   16,   66,  171,   96,  116,   45,   70,    0,   16,\n","            0,   44,   13, 2912,   70,   25, 1047, 5533,    0, 1049,   49,  435,\n","           54,  241,  279,   36, 2161,  164,  632,   45],\n","        [   7,  676,    7,    7,   25,   32,   13,  218,    9,  551,  228, 1182,\n","           13, 6114,   37,    7,  820,  137, 2475,   11,  116,   69,   11,  228,\n","            7, 4822,   21, 1589,   38, 1343,  217,  137],\n","        [ 520,  684,    6,    6,   10, 3629,  217,  149,   17,   26,   33,   12,\n","         1803,    9,    5,   14,   32,   12,   15, 1054, 2500,   22,   14,   19,\n","         1572,    7,    6, 7051,    7,   12,   21,    7],\n","        [  52,   49,   46,  155,   18,   19,   12,    0,  138,    7,   94,    6,\n","          139,   15,  223,   71,  472,    6,  841, 1849,   52,    6, 5296,   92,\n","         1764,   14, 1466,   19,    6,    6,   14,    6],\n","        [ 131, 1160,   40,   69,  103,   51,   19,   21,  163,    6,   59,  142,\n","         4524,    7,   10,   89,   59, 1462,   21,    0,    9, 2877,  108,  304,\n","           12,    0,   21,   13,  376,    0,  714,  366],\n","        [  12,    9,   11,   11,   80,   32,  282,   14,    9,  175,    6,   47,\n","          306,    5,   93,  270,    6,    7,    6,    7,   64,  221,   11,  273,\n","            6,    0,   14,   20, 2583,  140,    4,    4],\n","        [  24,   35,   14,    6,   57,    9,    9,   11,    8,  414, 2144,   14,\n","            8,  563,  253,  221, 1311,    6,  487, 1997,   75,  485,   63,   56,\n","         1619,  641, 2052,   63,    4,    4,    3,    3],\n","        [ 118,    8,  153,  417, 2216,   15,    5,  386,  411,   38,   12, 3198,\n","         7783, 3775,  198,    0,    8, 1944,  200, 1496, 2476,  154, 6845,    4,\n","            4,    4,    4,    4,    3,    3,    1,    1],\n","        [   9,  315, 1125,    7,   11,   12,  244, 1685,   10,   11,   17,  770,\n","         2774,  108,   21,    0,  494,    4,    4,    4,    4,    4,    4,    3,\n","            3,    3,    3,    3,    1,    1,    1,    1],\n","        [  10,   16,  238,   15, 2980,   24, 1094,  422,    8, 2406, 1042, 1351,\n","            4,    4,    4,    4,    4,    3,    3,    3,    3,    3,    3,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [   8,  158, 2520,   81,   21,  122, 1897,   72,  169,    4,    4,    4,\n","            3,    3,    3,    3,    3,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [  22,    9,    7,    7,    6,  156,  505,    4,    4,    3,    3,    3,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [ 205,  231,    6,   17, 1822,    4,    4,    3,    3,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [  37,   28, 2350,   90,    4,    3,    3,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [   8, 7537,    4,    4,    3,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [ 798,    4,    3,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [  10,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [7592,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [   4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [   3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:0') torch.Size([22, 32])\n","tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n","            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n","            2,    2,    2,    2,    2,    2,    2,    2],\n","        [  16,   21,    4,    4,    4,    4,    4,    4,   16,   53,   46,    4,\n","         4811,  428,    4,    4,    4,   16,    4,    4,    4,    4,    4,    4,\n","          110,   16,   19,    4,    4,    4,    4,   48],\n","        [  14,  106,    9,   14,   24,   61,   24,  154,   50,  122, 5303,   14,\n","            9,   12,    9, 1698,   70,  104,  369,   30,  154,  480,   55,  188,\n","           19,  228,  653,   38,  200,  153, 1012,   50],\n","        [  22,   55,    6,    6,   33,   35,    9,  152,   22,   42, 3417,   10,\n","          410,    4,   10,    6,   61,   78,  428,  102,   45,   10,   13,  105,\n","            6, 5782,   20,   12,   37, 3337,  643,   17],\n","        [ 533,   73,    4,    4,   11,   10,  643,   66,  146,   97, 5632,  854,\n","           27,  123,   45,    4,   11,    8,   12,  302,  181,   92,    4,   10,\n","          703,    6,    4, 2475,    6,    8,    4,   78],\n","        [  17,    6,   29,  118,   16, 2209,    4, 4865,   46,   34,    7,    8,\n","         4052,   68,    4,   26,   25,   21,    7, 1910,  621,  111,   47,  411,\n","          555,    4,  882, 4347,    4,    4,  166,    6],\n","        [  36,   31,   23,   45,   24,    4,  221,  232,   10,    6,   72,    4,\n","          236,  191,  220,   81,   35, 1182,  488,   13,   18,    4,  885,    4,\n","         1490,    0,   71,    4,  178,  223,  108,    4],\n","        [  57,   10,   11,    4,  127,   25,   15,    4,  419,   86,   69,   84,\n","            4, 2761,   22,   10,  798, 1058,    8, 1269,  214,  248,  191, 1515,\n","            8, 1488,   18,    9,  957, 2719,    5,  271],\n","        [   8,  883,   61,   31,   37,   35,   82,  108,   11,   23,    7,   71,\n","         1471,   69,   25,   56,    4,  281,    4, 2997,   20,   94, 2067,   65,\n","            4,    5,    4,    6,  177,  276,    3,    5],\n","        [   7,   58,  188,  297,   13,  114, 1555,   13,   46,   37,  912,   18,\n","          434,    4,  147,   20,  409,    5,  331,   15,  121,   40,   13,    7,\n","         1062,    3,  783,    7,    5,    5,    1,    3],\n","        [  96,   21,   67,   92, 1817,   10,  785,  390,   10,   13,  226,   21,\n","         1396,  767,    5, 3419,   20,    3,  184, 4027,    5,   74,   47, 1057,\n","            5,    1,  299,   47,    3,    3,    1,    1],\n","        [  28,  106,  190,    6,    8,  547,    5,    8,  133,  773,    4, 2721,\n","            5,  538,    3,    0,    4,    1,    5,   72,    3,  378,    5,    5,\n","            3,    1,    5,    5,    1,    1,    1,    1],\n","        [  46,   14, 1704,    7,    4,    8,    3,  141,    5,    5,  638,  287,\n","            3,    3,    1,    5,  575,    1,    3,  674,    1,    5,    3,    3,\n","            1,    1,    3,    3,    1,    1,    1,    1],\n","        [  12,  477,   20,  103,  444,    7,    1,    5,    3,    3,    5,  631,\n","            1,    1,    1,    3,    5,    1,    1,   22,    1,    3,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [ 155,   18,    4,    5,  226,  259,    1,    3,    1,    1,    3,    5,\n","            1,    1,    1,    1,    3,    1,    1,   86,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [  10,  247,  235,    3,    7,    5,    1,    1,    1,    1,    1,    3,\n","            1,    1,    1,    1,    1,    1,    1,    5,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [  45, 2994,    5,    1,  184,    3,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    3,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [   4,   44,    3,    1,    5,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [ 518,    5,    1,    1,    3,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [  11,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [5547,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [   5,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1],\n","        [   3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:0') torch.Size([23, 32])\n","##### EXAMPLE #####\n","SRC:  <sos> ein rot gekleidetes asiatisches kind weint , während eine asiatische frau versucht , es zu trösten . <eos> <pad> <pad> <pad>\n","TRG:  <sos> an asian child dressed in red is crying as an asian woman appears to be comforting her . <eos> <pad> <pad> <pad>\n","SRC vocab size 7863\n","TRG vocab size 5895\n","Vocab [('<unk>', 0), ('<pad>', 1), ('<sos>', 2), ('<eos>', 3), ('.', 4), ('ein', 5), ('einem', 6), ('in', 7), ('eine', 8), (',', 9)]\n"]}]},{"cell_type":"code","source":["!python3 -m spacy validate\n","print(spacy.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnTkWR_EA7RG","executionInfo":{"status":"ok","timestamp":1668247681160,"user_tz":-540,"elapsed":7475,"user":{"displayName":"Jinwoo Kim","userId":"09499597083311632056"}},"outputId":"c5319b19-6c78-46cc-f44a-e99a3be3dc60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r⠙ Loading compatibility table...\r\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n","\u001b[1m\n","================= Installed pipeline packages (spaCy v3.4.2) =================\u001b[0m\n","\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.7/dist-packages/spacy\u001b[0m\n","\n","NAME              SPACY            VERSION                            \n","de_core_news_sm   >=3.4.0,<3.5.0   \u001b[38;5;2m3.4.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n","en_core_web_sm    >=3.4.0,<3.5.0   \u001b[38;5;2m3.4.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n","\n","3.4.2\n"]}]},{"cell_type":"code","source":["src_ntoken"],"metadata":{"id":"w-99HMRvBk68","executionInfo":{"status":"ok","timestamp":1668247696627,"user_tz":-540,"elapsed":517,"user":{"displayName":"Jinwoo Kim","userId":"09499597083311632056"}},"outputId":"c3a471c5-8804-46c3-8c0c-31a832a93dc1","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7853"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"OVy7Dc_rmw2A"},"source":["---\n","# Define networks\n","\n","You should implement the `forward()` method of the given classes. Some classes are provided with the `forward()` method as well; you don't have to change anything in this case. However, **you are not allowed to modify the `__init__` method of all classes.**"]},{"cell_type":"markdown","metadata":{"id":"9yg7ZEwvO9_9"},"source":["## P1. Implement LSTM"]},{"cell_type":"markdown","metadata":{"id":"5YK28D7EO_Fd"},"source":["### (a)  LSTMCell [(illustration)](https://docs.google.com/drawings/d/1ICw_GxDMxkSS5g7D1w6gXDkLm3NAok1otmPcwYzLPEg/edit?usp=sharing)\n","- LSTMCell is a single unit constructing LSTM. It gets current input(`x`) and previous state (which is composed of hidden state `hx` and cell state `cx`) as inputs and returns the state for the next time step (`hy` and `cy`). There are four switch variables to handle information flows through time. Implement forward function with those four switch variables following the illustration."]},{"cell_type":"code","metadata":{"id":"AT0CJePPPS_E"},"source":["class LSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(LSTMCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.linear_input = nn.Linear(input_size, 4 * hidden_size)\n","        self.linear_hidden = nn.Linear(hidden_size, 4 * hidden_size)\n","\n","    def forward(self, x, state):\n","        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n","        hx, cx = state\n","        gates = self.linear_input(x) + self.linear_hidden(hx)\n","        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n","\n","        ingate = torch.sigmoid(ingate)\n","        forgetgate = torch.sigmoid(forgetgate)\n","        cellgate = torch.tanh(cellgate)\n","        outgate = torch.sigmoid(outgate)\n","\n","        cy = (forgetgate * cx) + (ingate * cellgate)\n","        hy = outgate * torch.tanh(cy)\n","\n","        return hy, (hy, cy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IOd-XpyGO_qH"},"source":["### (b)  LSTM[(illustration)](https://docs.google.com/drawings/d/1eiYyY9k6NELizHcRAsOS6jkEi_mISjfFlXntv8gaGZI/edit?usp=sharing)\n","- LSTMLayer is a single layer composed of sequential LSTMCells. While LSTMCell handles a single input, LSTMLayer gets a sequence as an input and processes it in an autoregressive manner. You don't need to implement LSTMLayer. It will be given.\n","- Using LSTMLayer, you should implement one full LSTM module by stacking multiple LSTMLayers. Note that `states` now contain multiple `state`s where each state becomes an initial state for a different level of LSTMLayers. Also, each output of an LSTMLayer is fed into the next layer of LSTMLayer as an input.\n","As a result, LSTM returns `output` tensor of size (L,B,nhid) and `output_states` consists of output states from different levels of LSTMLayers, which is a type of List(Tensor, Tensor, ..., Tensor) and each Tensor has a size of (L,B,nhid). Here L,B,nhid are a maximum length of sentences within a batch (equal to `x.size(0)`), batch size, and dimension size of hidden states, respectively. Implement the forward function following the given illustration."]},{"cell_type":"code","metadata":{"id":"Wjf1QmRAPYdd"},"source":["class LSTMLayer(nn.Module):\n","    def __init__(self,*cell_args):\n","        super(LSTMLayer, self).__init__()\n","        self.cell = LSTMCell(*cell_args)\n","\n","    def forward(self, x, state, length_x=None):\n","        # DO NOT MODIFY\n","        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n","        inputs = x.unbind(0)\n","        assert (length_x is None) or torch.all(length_x == length_x.sort(descending=True)[0])\n","        outputs = []\n","        out_hidden_state = []\n","        out_cell_state = []\n","        for i in range(len(inputs)):\n","            out, state = self.cell(inputs[i] , state)\n","            outputs += [out]\n","            if length_x is not None:\n","                if torch.any(i+1 == length_x):\n","                    out_hidden_state = [state[0][i+1==length_x]] + out_hidden_state\n","                    out_cell_state = [state[1][i+1==length_x]] + out_cell_state\n","        if length_x is not None:\n","            state = (torch.cat(out_hidden_state, dim=0), torch.cat(out_cell_state, dim=0))\n","        return torch.stack(outputs), state\n","\n","\n","class LSTM(nn.Module):\n","    def __init__(self, ninp, nhid, num_layers, dropout):\n","        super(LSTM, self).__init__()\n","        self.layers = []\n","        self.dropout = nn.Dropout(dropout)\n","        for i in range(num_layers):\n","            if i == 0:\n","                self.layers.append(LSTMLayer(ninp, nhid))\n","            else:\n","                self.layers.append(LSTMLayer(nhid, nhid))\n","        self.layers = nn.ModuleList(self.layers)\n","\n","    def forward(self, x, states, length_x=None):\n","        # WRITE YOUR CODE HERE\n","        output_states = []\n","        output = x\n","        i = 0\n","        for rnn_layer, state in zip(self.layers, states):\n","            if i > 0:\n","                output = self.dropout(output)\n","            output, out_state = rnn_layer(output, state, length_x=length_x)\n","            output_states.append(out_state)\n","            i += 1\n","        return output, output_states"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qXJsTGg3O-m4"},"source":["### (c)  Implement LSTMEncoder[(illustration)](https://docs.google.com/drawings/d/1wt5JhHtsx5b28KEem-_RX0FdeDmtUzSm0SJDypoCmtM/edit?usp=sharing)\n","LSTMEncoder encodes a sequence of tokens into the context vector. It first embeds a tokenized sequence using the embedding layer followed by dropout layer, and then LSTM computes `output` and `context_vector`. Implement the forward function following the given illustration.\n"]},{"cell_type":"code","metadata":{"id":"1b6J1a39PY4P"},"source":["class LSTMEncoder(nn.Module):\n","    def __init__(self):\n","        super(LSTMEncoder, self).__init__()\n","        ninp = args.ninp\n","        nhid = args.nhid\n","        nlayers = args.nlayers\n","        dropout = args.dropout\n","        self.embed = nn.Embedding(src_ntoken, ninp, padding_idx=pad_id)\n","        self.dropout = nn.Dropout(dropout)\n","        self.lstm = LSTM(ninp, nhid, nlayers, dropout)\n","\n","    def forward(self, x, states, length_x=None):\n","        # WRITE YOUR CODE HERE\n","        x = self.dropout(self.embed(x))\n","        output, context_vector = self.lstm(x, states, length_x=length_x)\n","        return output, context_vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CrFAmuJRO9eG"},"source":["### (d)  Implement LSTMDecoder[(illustration)](https://docs.google.com/drawings/d/151_NavPXUYtxbEDXBPpeMZnn0HlcZ-UVcIbSZFPU2o4/edit?usp=sharing)\n","LSTMDecoder gets a single token as an input to predict the next token. Similar to LSTMEncoder, it first embeds a given input (usually a predicted token from last time step) using embedding layer followed by dropout layer, and then LSTM computes `output` and `output_states`. Implement the forward function following the given illustration."]},{"cell_type":"code","metadata":{"id":"k2vRxaa3PZom"},"source":["class LSTMDecoder(nn.Module):\n","    def __init__(self):\n","        super(LSTMDecoder, self).__init__()\n","        self.embed = nn.Embedding(trg_ntoken, args.ninp, padding_idx=pad_id)\n","        self.lstm = LSTM(args.ninp, args.nhid, args.nlayers, args.dropout)\n","        self.fc_out = nn.Linear(args.nhid, trg_ntoken)\n","        self.dropout = nn.Dropout(args.dropout)\n","        self.fc_out.weight = self.embed.weight\n","\n","    def forward(self, x, states):\n","        # WRITE YOUR CODE HERE\n","        x = self.dropout(self.embed(x))\n","        output, output_states = self.lstm(x, states)\n","        output = self.fc_out(output)\n","        return output, output_states"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QjbbVJcbO9I3"},"source":["### (e)  Implement LSTMSeq2Seq[(illustration)](https://docs.google.com/drawings/d/1xWxCE44_IaQhtxEXSnvA5fzdBjylj3_Maj9B02glMqs/edit?usp=sharing)\n","LSTMSeq2Seq is a complete model for neural machine translation. It starts with LSTMEncoder encoding a given tokenized sequence into the context vector. LSMTDecoder then decodes the context vector step by step. As mentioned in the description for LSTMDecoder, each input for the decoder is a token predicted by the previous decoder. In the training stage, however, one noisy prediction from the previous decoder can mess up all of the following predictions so teacher forcing is used in the training stage. Teacher forcing allows LSTMdecoder to always ground-truth token as an input instead of predicted one from the previous step. Therefore, implement the forward function to use the ground-truth label for the input for LSTMDecoder if `teacher_focing` is True (it's the case for training stage), and use the predicted token from last time step otherwise (case for inference). Also, note that all of the sentences start with <sos> token so the first input token to LSTMDecoder should be always `<sos>`. Implement the forward function following the given illustration."]},{"cell_type":"code","metadata":{"id":"NLC1mpEFm05u"},"source":["class LSTMSeq2Seq(nn.Module):\n","    def __init__(self):\n","        super(LSTMSeq2Seq, self).__init__()\n","        self.encoder = LSTMEncoder()\n","        self.decoder = LSTMDecoder()\n","\n","    def _get_init_states(self, x):\n","        init_states = [\n","            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n","            torch.zeros((x.size(1), args.nhid)).to(x.device))\n","            for _ in range(args.nlayers)\n","        ]\n","        return init_states\n","\n","    def forward(self, x, y, length, max_len=None, teacher_forcing=True):\n","        # WRITE YOUR CODE HERE\n","        ##### Encoding Procedure\n","        init_states = self._get_init_states(x)\n","        _, output_states = self.encoder(x, init_states, length)\n","\n","        ##### Decoding Procedure\n","        # Decoding Initialize\n","        trg_len = y.size(0) if max_len is None else max_len\n","        dec_outputs = []\n","        for i in range(trg_len-1):\n","            if teacher_forcing or i==0:\n","                dec_input = y[i:i+1]\n","            else:\n","                dec_input= dec_output.argmax(-1)\n","            dec_output, output_states = self.decoder(dec_input, output_states)\n","            dec_outputs.append(dec_output)\n","        return torch.cat(dec_outputs, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TG_fK-sWjLT5"},"source":["\n","## P2. Implement Transformer\n","\n","**This section has no dependency on the two preceding models; you can implement the Transformer first if you want to.**"]},{"cell_type":"markdown","metadata":{"id":"w4tGUyntXKXk"},"source":["### (a) Implement MaskedMultiheadAttention [(Illustration)](https://docs.google.com/drawings/d/1kCsVW-61xHT-riSDxGzF8VBRo2CpqrE0ngG7H21wirs/edit?usp=sharing)\n","In this module, you will implement a single layer of multi-head attention, which will be the key building block of the Transformer model. Each query, key, value input will first pass through a feed-forward network, then scaled dot-product attention is performed. Additionally, there's an optional mask layer inside the scaled dot-product attention applied only in the decoder stage of the Transformer, to prevent the model from being able to see future inputs. Implement the forward function following the given illustration."]},{"cell_type":"code","metadata":{"id":"OFeZAJNCPEll"},"source":["MAX_LEN = 100\n","class MaskedMultiheadAttention(nn.Module):\n","    \"\"\"\n","    A vanilla multi-head masked attention layer with a projection at the end.\n","    \"\"\"\n","    def __init__(self, mask=False):\n","        super(MaskedMultiheadAttention, self).__init__()\n","        assert args.nhid_tran % args.nhead == 0\n","        # mask : whether to use\n","        # key, query, value projections for all heads\n","        self.key = nn.Linear(args.nhid_tran, args.nhid_tran)\n","        self.query = nn.Linear(args.nhid_tran, args.nhid_tran)\n","        self.value = nn.Linear(args.nhid_tran, args.nhid_tran)\n","        # regularization\n","        self.attn_drop = nn.Dropout(args.attn_pdrop)\n","        # output projection\n","        self.proj = nn.Linear(args.nhid_tran, args.nhid_tran)\n","        # causal mask to ensure that attention is only applied to the left in the input sequence\n","        if mask:\n","            self.register_buffer(\"mask\", torch.tril(torch.ones(MAX_LEN, MAX_LEN)))\n","        self.nhead = args.nhead\n","        self.d_k = args.nhid_tran // args.nhead\n","\n","    def forward(self, q, k, v, mask=None):\n","        # WRITE YOUR CODE HERE\n","        B, T_q, C = q.shape\n","        _, T, _ = k.shape\n","        q = self.query(q).view(B, T_q, self.nhead, self.d_k).transpose(1, 2)\n","        k = self.key(k).view(B, T, self.nhead, self.d_k).transpose(1, 2)\n","        v = self.value(v).view(B, T, self.nhead, self.d_k).transpose(1, 2)\n","\n","        # MatMul and Scale\n","        att = (q @ k.transpose(-2, -1)) / (C ** 0.5)\n","\n","        # Mask\n","        if hasattr(self, 'mask'):\n","            att[:, :, self.mask[:T_q, :T]==0] = float('-inf')\n","        if mask is not None:\n","            assert len(mask.shape) == 2 # batch_size x t\n","            att = att.transpose(0, 2)\n","            att[:, :, mask == 0] =  float('-inf')\n","            att = att.transpose(0, 2)\n","\n","        # SoftMax\n","        att = F.softmax(att, dim=-1) #(B, nh, T_q, T)\n","        # Dropout\n","        att = self.attn_drop(att)\n","        # MatMul2\n","        y = att @ v # (B, nh, T_q, T) x (B, nh, T, hs) -> (B, nh, T_q, hs)\n","        y = y.transpose(1, 2).contiguous().view(B, T_q, C) # re-assemble all head outputs side by side\n","\n","        # output projection\n","        y = self.proj(y)\n","        return y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWn3GenhPG1G"},"source":["\n","### (b) Implement TransformerEncLayer [(Illustration)](https://docs.google.com/drawings/d/1DSJmF8z0g79J0EZCY8WyTDR0pxUmsmqNEVU9jxYrrsY/edit?usp=sharing)\n","This module is a single layer of the Transformer encoder, containing a layer of masked multi-head attention and a feed-forward network with dropout and skip connection. Both attention and feed-forward layer have skip connections and are preceded by LayerNorm. You will stack this layer multiple times to create the full version of the encoder. Since attention is performed in a self-attention manner, you will pass the same values to query, key, and value inputs of the MaskedSelfAttention module.\n"]},{"cell_type":"code","metadata":{"id":"1RcLE8QKPPfJ"},"source":["class TransformerEncLayer(nn.Module):\n","    def __init__(self):\n","        super(TransformerEncLayer, self).__init__()\n","        self.ln1 = nn.LayerNorm(args.nhid_tran)\n","        self.ln2 = nn.LayerNorm(args.nhid_tran)\n","        self.attn = MaskedMultiheadAttention()\n","        self.dropout1 = nn.Dropout(args.resid_pdrop)\n","        self.dropout2 = nn.Dropout(args.resid_pdrop)\n","        self.ff = nn.Sequential(\n","            nn.Linear(args.nhid_tran, args.nff),\n","            nn.ReLU(),\n","            nn.Linear(args.nff, args.nhid_tran)\n","        )\n","\n","    def forward(self, x, mask=None):\n","        # WRITE YOUR CODE HERE\n","        x = self.ln1(x)\n","        o = self.dropout1(self.attn(x, x, x, mask))\n","        x = x + o\n","\n","        x = self.ln2(x)\n","        o = self.dropout2(self.ff(x))\n","        x = x + o\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKz8TOrEPVu4"},"source":["### (c) Implement TransformerDecLayer [(Illustration)](https://docs.google.com/drawings/d/1qNP7ibDTWCRhXJdejtO7jRkvj5RrPN90ORwuK-ANT-4/edit?usp=sharing)\n","This module is a single layer of the Transformer decoder. The module contains two masked multi-head attentions and a feed-forward network, all with a skip connection and a preceding LayerNorm. The first attention is identical to the encoder's attention. However, the second attention is a cross-attention: that is, the key and value inputs of this layer would be the encoded words from the **source** sentence, given as `enc_o`.\n","\n"]},{"cell_type":"code","metadata":{"id":"Bnp9dhwsPXjB"},"source":["class TransformerDecLayer(nn.Module):\n","    def __init__(self):\n","        super(TransformerDecLayer, self).__init__()\n","        self.ln1 = nn.LayerNorm(args.nhid_tran)\n","        self.ln2 = nn.LayerNorm(args.nhid_tran)\n","        self.ln3 = nn.LayerNorm(args.nhid_tran)\n","        self.dropout1 = nn.Dropout(args.resid_pdrop)\n","        self.dropout2 = nn.Dropout(args.resid_pdrop)\n","        self.dropout3 = nn.Dropout(args.resid_pdrop)\n","        self.attn1 = MaskedMultiheadAttention(mask=True) # self-attention\n","        self.attn2 = MaskedMultiheadAttention() # tgt to src attention\n","        self.ff = nn.Sequential(\n","            nn.Linear(args.nhid_tran, args.nff),\n","            nn.ReLU(),\n","            nn.Linear(args.nff, args.nhid_tran)\n","        )\n","\n","    def forward(self, x, enc_o, enc_mask=None):\n","        # WRITE YOUR CODE HERE\n","        x = self.ln1(x)\n","        o = self.dropout1(self.attn1(x, x, x))\n","        x = x + o\n","\n","        x = self.ln2(x)\n","        o = self.dropout2(self.attn2(x, enc_o, enc_o, enc_mask))\n","        x = x + o\n","\n","        x = self.ln3(x)\n","        o = self.dropout3(self.ff(x))\n","        x = x + o\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NO7tjDWvPbx3"},"source":["### (d) Implement TransformerEncoder [(Illustration)](https://docs.google.com/drawings/d/1WtbU0xcaAVWsVegSwO0AzZBfk9NNDltTY9P-GiImVqg/edit?usp=sharing)\n","In this module, you will first tokenize the input word, apply positional encoding (Refer to `PositionalEncoding` class that we've implemented for you), then pass through multiple layers of TransformerEncLayer, and conclude with a LayerNorm.\n"]},{"cell_type":"code","metadata":{"id":"EjdoK0opPdEC"},"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, max_len=4096):\n","        super().__init__()\n","        dim = args.nhid_tran\n","        pos = np.arange(0, max_len)[:, None]\n","        i = np.arange(0, dim // 2)\n","        denom = 10000 ** (2 * i / dim)\n","\n","        pe = np.zeros([max_len, dim])\n","        pe[:, 0::2] = np.sin(pos / denom)\n","        pe[:, 1::2] = np.cos(pos / denom)\n","        pe = torch.from_numpy(pe).float()\n","\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # DO NOT MODIFY\n","        return x + self.pe[:x.shape[1]]\n","\n","class TransformerEncoder(nn.Module):\n","\n","    def __init__(self):\n","        super(TransformerEncoder, self).__init__()\n","        # input embedding stem\n","        self.tok_emb = nn.Embedding(src_ntoken, args.nhid_tran)\n","        self.pos_enc = PositionalEncoding()\n","        self.dropout = nn.Dropout(args.embd_pdrop)\n","        # transformer\n","        self.transform = nn.ModuleList([TransformerEncLayer() for _ in range(args.nlayers_transformer)])\n","        # decoder head\n","        self.ln_f = nn.LayerNorm(args.nhid_tran)\n","\n","\n","    def forward(self, x, mask):\n","        # WRITE YOUR CODE HERE\n","        x = self.dropout(self.pos_enc(self.tok_emb(x)))\n","\n","        for m in self.transform:\n","            x = m(x, mask)\n","        outputs = self.ln_f(x)\n","\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XkSdxiGJPgPm"},"source":["### (e) Implement TransformerDecoder [(Illustration)](https://docs.google.com/drawings/d/1cipGddhtugoqM31H6_5fswHSYCXPSiMrJ85GdhGfz4c/edit?usp=sharing)\n","What TransformerDecoder does is pretty much identical to TransformerEncoder. There are two differences: first is that you should use TransformerDecLayer instead of TransformerEncLayer (obviously!), the other difference is that there's an extra linear layer at the very end of the pipeline.\n"]},{"cell_type":"code","metadata":{"id":"Ft64IQHrPlCb"},"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self):\n","        super(TransformerDecoder, self).__init__()\n","        self.tok_emb = nn.Embedding(trg_ntoken, args.nhid_tran)\n","        self.pos_enc = PositionalEncoding()\n","        self.dropout = nn.Dropout(args.embd_pdrop)\n","        self.transform = nn.ModuleList([TransformerDecLayer() for _ in range(args.nlayers_transformer)])\n","        self.ln_f = nn.LayerNorm(args.nhid_tran)\n","        self.lin_out = nn.Linear(args.nhid_tran, trg_ntoken)\n","        self.lin_out.weight = self.tok_emb.weight\n","\n","\n","    def forward(self, x, enc_o, enc_mask):\n","        # WRITE YOUR CODE HERE\n","        x = self.dropout(self.pos_enc(self.tok_emb(x)))\n","\n","        for m in self.transform:\n","            x = m(x, enc_o, enc_mask)\n","        x = self.ln_f(x)\n","        logits = self.lin_out(x)\n","\n","        logits /= args.nhid_tran ** 0.5 # Scaling logits. Do not modify this\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nxKg_RfPpz4"},"source":["### (f) Implement Transformer [(Illustration)](https://docs.google.com/drawings/d/18BeRA4Jl--rR5Txvyfr3nA5SA7ve8nESjZJSiD51ULw/edit?usp=sharing)\n","Finally, we combine everything to construct the full Transformer model. Begin by creating a mask according to `length_x` parameter, and pass the inputs through TransformerEncoder to obtain encoder output. Now if we're on training mode (`self.training == True`) or teacher forcing is enabled, then we run through the decoder exactly once to predict the very next word. Otherwise, we run through the decoder `max_len - 1` times to create a sequence of `max_len` tokens. The first token to feed the decoder is always the first token of `y`.\n"]},{"cell_type":"code","metadata":{"id":"EWsZrYEGywMi"},"source":["class Transformer(nn.Module):\n","    def __init__(self):\n","        super(Transformer, self).__init__()\n","        self.encoder = TransformerEncoder()\n","        self.decoder = TransformerDecoder()\n","\n","    def forward(self, x, y, length_x, max_len=None, teacher_forcing=True):\n","        # WRITE YOUR CODE HERE\n","        max_len_src = x.size(1)\n","        if length_x is not None:\n","            enc_mask = length_x.view(-1, 1) > torch.arange(max_len_src).to(length_x.device)\n","        else:\n","            enc_mask = None\n","        enc_o = self.encoder(x, enc_mask)\n","        if self.training or teacher_forcing:\n","            return self.decoder(y[:, :-1], enc_o, enc_mask)\n","        # Training\n","        dec_input = y[:, :1] # batch_size x 1\n","        if max_len is None:\n","            max_len = y.shape[1]\n","        for t in range(1, max_len):\n","            dec_output = self.decoder(dec_input, enc_o, enc_mask)\n","            dec_input = torch.cat((dec_input, dec_output[:, -1:].argmax(-1)), dim=-1) # batch_size\n","        return dec_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iHQzs6ocPxtQ"},"source":["## Run Experiment\n","\n","You can run the experiment after you've finished at least one out of the three models. However, please **run every single cell above (even the cells you haven't implemented yet)** to run the function properly. We expect training a model for 20 epochs should take less than an hour."]},{"cell_type":"code","metadata":{"id":"YdxUX81cywMk"},"source":["def run_experiment(model):\n","    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=args.lr_lstm if not isinstance(model, Transformer) else args.lr_transformer)\n","\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n","            factor=0.25, patience=1, threshold=0.0001, threshold_mode='rel',\n","            cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n","\n","    best_val_loss = np.inf\n","    for epoch in tq.tqdm(range(args.epochs)):\n","        run_epoch(epoch, model, optimizer, is_train=True)\n","        with torch.no_grad():\n","            val_loss = run_epoch(epoch, model, None, is_train=False)\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            save_model(model, 'best')\n","        save_model(model)\n","        scheduler.step(val_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UytcJfBv8b3Y"},"source":["## P3. Train and Validate models\n","**About evaluation metrics**\n","\n","[PPL(Perplexity)](https://en.wikipedia.org/wiki/Perplexity) can be interpreted as \"how many words are considered as candidate output on every time step\". A lower perplexity means the model is more confident with its output.\n","\n","[BLEU(Bilingual Evaluation Understudy) Score](https://en.wikipedia.org/wiki/BLEU) is a general metric to measure the quality of machine translation output. It takes three elements into calculation:\n","- Precision: How accurate are each n-gram of the predicted sentence?\n","- Clipping: Calibrate the score when a word occurs multiple times in true/predicted sentence.\n","- Brevity penalty: The predicted and true sentence should have identical (or similar) length.\n","\n","A higher  BLEU score is considered a better quality translation.\n","\n","**Expected BLEU Score**\n","- LSTMSeq2Seq: 0.245 ±0.02\n","- Transformer: 0.358 ±0.02"]},{"cell_type":"code","metadata":{"id":"-ByvNFinywMm","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["cb00c8026cee4566b2c73ce64536abeb","cb5deff2723347908b76ae2f9eec91a0","8f46cc83a6c04b409374f43ac0027377","8c42c9686c174417a5b952f4a7427702","cdf85eba599846ada05cd2d126176611","735ddae3fd8147e68bf377ddfb91fd99","d54d3c8672d5484ca6b1d45fa1acc345","8c35a249b27c4ca0bbbd6f7958fc7c96","61a0fb6b876b480ab29d9ce011eb859d","684287022c5142f3b8a150563f8f1594","05c3d65d2e6d4f81babd55e073b1a23e"]},"executionInfo":{"status":"ok","timestamp":1691936544124,"user_tz":-540,"elapsed":2091382,"user":{"displayName":"김동균","userId":"17758291692024531705"}},"outputId":"22bb8338-89e8-4c0f-fb46-08fd96ad391f"},"source":["lstm_model = LSTMSeq2Seq().to(device)\n","lstm_model.apply(init_weights)\n","run_experiment(lstm_model)\n","run_translation(lstm_model, test_iter, max_len=100)\n","print('')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb00c8026cee4566b2c73ce64536abeb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 0 Train Loss 4.323539385868659 Acc 0.3020846061731714 PPL 75.45522137006377\n","Epoch 0 Valid Loss 3.5830149840780243 Acc 0.37049861495844877 PPL 35.98186221308135\n","Epoch 1 Train Loss 3.4227984112167134 Acc 0.40108262665265526 PPL 30.655080622819234\n","Epoch 1 Valid Loss 3.1426588538445923 Acc 0.43843490304709143 PPL 23.165378402809186\n","Epoch 2 Train Loss 3.0496699364683684 Acc 0.44974950511987094 PPL 21.108376167435594\n","Epoch 2 Valid Loss 2.863048366942234 Acc 0.47181440443213296 PPL 17.51483729153387\n","Epoch 3 Train Loss 2.7622231056929163 Acc 0.48531000268823776 PPL 15.835006740997565\n","Epoch 3 Valid Loss 2.6588375709558787 Acc 0.5018005540166205 PPL 14.279680332132656\n","Epoch 4 Train Loss 2.5279890915146 Acc 0.5133141083604194 PPL 12.528287549290305\n","Epoch 4 Valid Loss 2.5133029524309154 Acc 0.5238227146814405 PPL 12.345639853410463\n","Epoch 5 Train Loss 2.330103936713999 Acc 0.5388621422810919 PPL 10.27900984403047\n","Epoch 5 Valid Loss 2.390129504771774 Acc 0.5429362880886427 PPL 10.91490738410603\n","Epoch 6 Train Loss 2.163152295675133 Acc 0.5599134876218872 PPL 8.698514775916728\n","Epoch 6 Valid Loss 2.3135649696859297 Acc 0.5536703601108033 PPL 10.110403767629897\n","Epoch 7 Train Loss 2.016586750926252 Acc 0.5790024194139641 PPL 7.512638614904049\n","Epoch 7 Valid Loss 2.2543762774018368 Acc 0.5659279778393351 PPL 9.529347786600551\n","Epoch 8 Train Loss 1.8889440948285123 Acc 0.5966861360248296 PPL 6.61238294420766\n","Epoch 8 Valid Loss 2.201428275606969 Acc 0.5734764542936288 PPL 9.037912915835957\n","Epoch 9 Train Loss 1.7750950461818475 Acc 0.612458759989247 PPL 5.900841962164606\n","Epoch 9 Valid Loss 2.1761082799811113 Acc 0.5789473684210527 PPL 8.81194581554001\n","Epoch 10 Train Loss 1.6728500745854609 Acc 0.626525574916298 PPL 5.327329467295859\n","Epoch 10 Valid Loss 2.1516414865314797 Acc 0.5822714681440443 PPL 8.598961898798702\n","Epoch 11 Train Loss 1.5774963621956974 Acc 0.6415772623964417 PPL 4.8428159635622405\n","Epoch 11 Valid Loss 2.1460561860961596 Acc 0.5846952908587257 PPL 8.551067988607226\n","Epoch 12 Train Loss 1.4938367753936188 Acc 0.6543170654219311 PPL 4.454152358617122\n","Epoch 12 Valid Loss 2.1295939492221683 Acc 0.5852493074792243 PPL 8.411450641842091\n","Epoch 13 Train Loss 1.4152962369403634 Acc 0.6670495368899533 PPL 4.117706101970686\n","Epoch 13 Valid Loss 2.140911275660232 Acc 0.58601108033241 PPL 8.50718648966086\n","Epoch 14 Train Loss 1.3453339091770495 Acc 0.679149050563308 PPL 3.8394683603028508\n","Epoch 14 Valid Loss 2.1362572239706723 Acc 0.5914819944598338 PPL 8.467685594907975\n","Epoch 15 Train Loss 1.186636322062894 Acc 0.7123854444145752 PPL 3.2760430995756145\n","Epoch 15 Valid Loss 2.11612954409664 Acc 0.593421052631579 PPL 8.298954508992932\n","Epoch 16 Train Loss 1.1391204601784233 Acc 0.7234463207800778 PPL 3.124019456960464\n","Epoch 16 Valid Loss 2.1244263934626804 Acc 0.592590027700831 PPL 8.36809611657247\n","Epoch 17 Train Loss 1.1095542699057073 Acc 0.7293848823285026 PPL 3.033006190963684\n","Epoch 17 Valid Loss 2.1317341628497326 Acc 0.5915512465373961 PPL 8.429472221269279\n","Epoch 18 Train Loss 1.0635945716024904 Acc 0.7400131968034409 PPL 2.896764926143778\n","Epoch 18 Valid Loss 2.1264464838352892 Acc 0.5945983379501385 PPL 8.38501751259487\n","Epoch 19 Train Loss 1.0527958412460168 Acc 0.7426501136391408 PPL 2.86565183608287\n","Epoch 19 Valid Loss 2.130356358144422 Acc 0.5943905817174515 PPL 8.417866052132796\n","--------- Translation Example 1 ---------\n","SRC : ein reh springt über einen zaun .\n","TRG : a deer jumps a fence .\n","PRED: a <unk> jumps over a fence .\n","--------- Translation Example 2 ---------\n","SRC : ein radfahrer springt über eine hindernis .\n","TRG : a biker jumps an obstacle .\n","PRED: a cyclist is jumping over a hurdle .\n","--------- Translation Example 3 ---------\n","SRC : zwei kleine kinder auf dem sand .\n","TRG : two young children are on sand .\n","PRED: two young children are in the sand .\n","--------- Translation Example 4 ---------\n","SRC : ein kleiner schwarzer hund springt über <unk>\n","TRG : a small black dog jumping over gates\n","PRED: a small black dog jumps over the obstacle .\n","--------- Translation Example 5 ---------\n","SRC : ein mann schneidet äste von bäumen .\n","TRG : a man cutting branches of trees .\n","PRED: a man is cleaning some <unk> .\n","\n","BLEU: 0.2377970814704895\n","\n"]}]},{"cell_type":"code","metadata":{"id":"UU65AMeUywMu","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5760abca28ad486db1df3c10aa5e2399","3b63ebe3be9f48b49867c7c95bc2247c","f64d22534b2548d5a65dc3a6b1db9fc3","f343e581fe0f4e148c420f76d36304a1","60a206562cbc42f1801e0792b448fca6","c5482a09a130474e8f7a3430c8b0e0e0","7aa6606d6a9641ff8d9b9c57cd1c16b2","c5c88fa8af4b42c488cc91e232bea3a2","ba0bfe37489d4c418f647b0b7fb8cf06","a3f2fcca64974b76b56209029c19b136","8501f9dd45bd4b9183c5f807bfa8d74b"]},"executionInfo":{"status":"ok","timestamp":1691937643046,"user_tz":-540,"elapsed":1098947,"user":{"displayName":"김동균","userId":"17758291692024531705"}},"outputId":"55ce751e-4041-4db5-e802-f7d5850ef2dc"},"source":["transformer_model = Transformer().to(device)\n","run_experiment(transformer_model)\n","run_translation(transformer_model, test_iter, max_len=100)\n","print('')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5760abca28ad486db1df3c10aa5e2399"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 0 Train Loss 4.811685344572684 Acc 0.38517803465382827 PPL 122.93863698331013\n","Epoch 0 Valid Loss 3.7916825116836463 Acc 0.48822714681440443 PPL 44.330924863187555\n","Epoch 1 Train Loss 3.6758363614471206 Acc 0.4949705515775068 PPL 39.481663993960865\n","Epoch 1 Valid Loss 3.1721370085124496 Acc 0.5426592797783933 PPL 23.858415553976986\n","Epoch 2 Train Loss 3.1995236115079346 Acc 0.5348615557564945 PPL 24.52084596537994\n","Epoch 2 Valid Loss 2.826955975521965 Acc 0.5722299168975069 PPL 16.893956856439342\n","Epoch 3 Train Loss 2.8972878206807233 Acc 0.5624282118331337 PPL 18.12492061158609\n","Epoch 3 Valid Loss 2.6268052824811594 Acc 0.5885041551246537 PPL 13.829517850410818\n","Epoch 4 Train Loss 2.6784492715741273 Acc 0.5822600747818861 PPL 14.562493304866067\n","Epoch 4 Valid Loss 2.475264404008263 Acc 0.6058171745152354 PPL 11.884849099443697\n","Epoch 5 Train Loss 2.507999816642146 Acc 0.5990762237591339 PPL 12.280342542153681\n","Epoch 5 Valid Loss 2.341220359376263 Acc 0.6181440443213296 PPL 10.393913135522364\n","Epoch 6 Train Loss 2.366298166554608 Acc 0.6127178083530878 PPL 10.657865525451982\n","Epoch 6 Valid Loss 2.2357796318560754 Acc 0.62797783933518 PPL 9.353771508445716\n","Epoch 7 Train Loss 2.2428948378544864 Acc 0.6256457880202351 PPL 9.420562854902657\n","Epoch 7 Valid Loss 2.1717941206990847 Acc 0.6341412742382272 PPL 8.774011563745338\n","Epoch 8 Train Loss 2.139550514172714 Acc 0.6362643270852172 PPL 8.495618110606012\n","Epoch 8 Valid Loss 2.1011201072515213 Acc 0.6412049861495844 PPL 8.175322023419854\n","Epoch 9 Train Loss 2.0440429755277907 Acc 0.646337887045138 PPL 7.721765082462717\n","Epoch 9 Valid Loss 2.059860038501404 Acc 0.6473684210526316 PPL 7.844871753470491\n","Epoch 10 Train Loss 1.9551342474321498 Acc 0.6558909064248882 PPL 7.064867400348197\n","Epoch 10 Valid Loss 2.008122512491786 Acc 0.65 PPL 7.4493182079466145\n","Epoch 11 Train Loss 1.8725376021241789 Acc 0.6654219311322369 PPL 6.504782022141137\n","Epoch 11 Valid Loss 1.9681291278874775 Acc 0.6606648199445984 PPL 7.157273611120614\n","Epoch 12 Train Loss 1.7997688087839288 Acc 0.6727632640093844 PPL 6.048249000721336\n","Epoch 12 Valid Loss 1.9363540608384273 Acc 0.660387811634349 PPL 6.933425982693928\n","Epoch 13 Train Loss 1.7294967721577699 Acc 0.6815293628876561 PPL 5.63781608842812\n","Epoch 13 Valid Loss 1.9137427056512675 Acc 0.6641274238227147 PPL 6.778410979316531\n","Epoch 14 Train Loss 1.6622982325647468 Acc 0.6892788191304773 PPL 5.271411864050055\n","Epoch 14 Valid Loss 1.8811564786034607 Acc 0.6661357340720222 PPL 6.561088234500663\n","Epoch 15 Train Loss 1.5992579153478517 Acc 0.6970967032429922 PPL 4.94935821850589\n","Epoch 15 Valid Loss 1.8620244427070722 Acc 0.6720914127423823 PPL 6.436754430795059\n","Epoch 16 Train Loss 1.539356874592531 Acc 0.7051809672768151 PPL 4.6615913189239215\n","Epoch 16 Valid Loss 1.8611068970939129 Acc 0.6702216066481994 PPL 6.430851123695026\n","Epoch 17 Train Loss 1.4842211379019372 Acc 0.7110144431682104 PPL 4.411528102323486\n","Epoch 17 Valid Loss 1.8432265738288451 Acc 0.6725761772853186 PPL 6.3168873178049045\n","Epoch 18 Train Loss 1.4268626131355688 Acc 0.7182653535032626 PPL 4.165609540355541\n","Epoch 18 Valid Loss 1.8467099231696196 Acc 0.6762465373961218 PPL 6.338929611295458\n","Epoch 19 Train Loss 1.376667614962926 Acc 0.7247660011241721 PPL 3.9616777705163515\n","Epoch 19 Valid Loss 1.838520800960031 Acc 0.6758310249307479 PPL 6.287231312787258\n","--------- Translation Example 1 ---------\n","SRC : ein reh springt über einen zaun .\n","TRG : a deer jumps a fence .\n","PRED: a mountain biker is jumping over a fence .\n","--------- Translation Example 2 ---------\n","SRC : ein radfahrer springt über eine hindernis .\n","TRG : a biker jumps an obstacle .\n","PRED: a bicyclist is jumping over an obstacle .\n","--------- Translation Example 3 ---------\n","SRC : zwei kleine kinder auf dem sand .\n","TRG : two young children are on sand .\n","PRED: two young children on the sand .\n","--------- Translation Example 4 ---------\n","SRC : ein kleiner schwarzer hund springt über <unk>\n","TRG : a small black dog jumping over gates\n","PRED: a small black dog jumps over some <unk> .\n","--------- Translation Example 5 ---------\n","SRC : ein mann schneidet äste von bäumen .\n","TRG : a man cutting branches of trees .\n","PRED: a man is cutting up to trees .\n","\n","BLEU: 0.3640564445096655\n","\n"]}]},{"cell_type":"code","metadata":{"id":"iXEhb6xbO850"},"source":[],"execution_count":null,"outputs":[]}]}