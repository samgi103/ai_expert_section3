{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC3TkQ3gJbBP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import copy\n",
        "\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvdDhbPFJbBR"
      },
      "source": [
        "### Step 1. Design BYOL Model & Loss\n",
        "\n",
        "#### Implementation 1-1. Create modules\n",
        "\n",
        "#### Implementation 1-2. Design BYOL loss function\n",
        "\n",
        "#### Implementation 1-3. Design forward function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ftel14QJbBR"
      },
      "source": [
        "import math\n",
        "from torchvision.models.resnet import conv3x3\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, norm_layer, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "        self.bn1 = norm_layer(inplanes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.relu1(residual)\n",
        "        residual = self.conv1(residual)\n",
        "\n",
        "        residual = self.bn2(residual)\n",
        "        residual = self.relu2(residual)\n",
        "        residual = self.conv2(residual)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x + residual\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.avg = nn.AvgPool2d(stride)\n",
        "        assert nOut % nIn == 0\n",
        "        self.expand_ratio = nOut // nIn\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)\n",
        "\n",
        "\n",
        "class ResNetCifar(nn.Module):\n",
        "    def __init__(self, depth=26, width=1, channels=3, norm_layer=nn.BatchNorm2d):\n",
        "        assert (depth - 2) % 6 == 0         # depth is 6N+2\n",
        "        self.N = (depth - 2) // 6\n",
        "        super(ResNetCifar, self).__init__()\n",
        "\n",
        "        # Following the Wide ResNet convention, we fix the very first convolution\n",
        "        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.inplanes = 16\n",
        "        self.layer1 = self._make_layer(norm_layer, 16 * width)\n",
        "        self.layer2 = self._make_layer(norm_layer, 32 * width, stride=2)\n",
        "        self.layer3 = self._make_layer(norm_layer, 64 * width, stride=2)\n",
        "        self.bn = norm_layer(64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "\n",
        "    def _make_layer(self, norm_layer, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = Downsample(self.inplanes, planes, stride)\n",
        "        layers = [BasicBlock(self.inplanes, planes, norm_layer, stride, downsample)]\n",
        "        self.inplanes = planes\n",
        "        for i in range(self.N - 1):\n",
        "            layers.append(BasicBlock(self.inplanes, planes, norm_layer))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, idim, hdim, odim, width=1.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(idim * width, hdim * width)\n",
        "        self.bn1 = nn.BatchNorm1d(hdim * width)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hdim * width, odim * width)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BYOLNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 width=1,\n",
        "                 feat_dim=64,\n",
        "                 hidden_dim=128,\n",
        "                 byol_feat_dim=32,\n",
        "                 num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        # f: convolutional feature encoder, use ResNetCifar class above\n",
        "        # g: projection MLP, use MLP class above\n",
        "        # q: prediction MLP, use MLP class above\n",
        "        # h: linear classifier, use nn.Linear class\n",
        "        self.f = ResNetCifar(width=width)\n",
        "        self.g = MLP(idim=feat_dim, hdim=hidden_dim, odim=byol_feat_dim, width=width)\n",
        "        self.q = MLP(idim=byol_feat_dim, hdim=hidden_dim, odim=byol_feat_dim, width=width)\n",
        "        self.h = nn.Linear(feat_dim, num_classes)\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "\n",
        "        self.online_net = nn.Sequential(self.f, self.g, self.q)\n",
        "\n",
        "        self.f_target = copy.deepcopy(self.f)\n",
        "        self.g_target = copy.deepcopy(self.g)\n",
        "\n",
        "        self.target_net = nn.Sequential(self.f_target, self.g_target)\n",
        "\n",
        "        for p in self.target_net.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def byol_loss(self, pred, proj):\n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        # pred: prediction vectors from online net\n",
        "        # proj: projection vectors from target net (must not have grad)\n",
        "        # return the loss values of shape (batch, 1)\n",
        "        # Note that the last dimension should contain the value of dot product\n",
        "        pred = F.normalize(pred, dim=-1)\n",
        "        proj = F.normalize(proj, dim=-1)\n",
        "        return -2. * (pred * proj).sum(dim=-1)\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "\n",
        "    def byol_forward(self, x1, x2):\n",
        "        ### IMPLEMENTATION 1-3 ###\n",
        "        # x1: tensor containing input image (view #1)\n",
        "        # x2: tensor containing input image (view #2)\n",
        "        # Compute 2 loss values using self.byol_loss method twice\n",
        "        pred1 = self.online_net(x1)\n",
        "        pred2 = self.online_net(x2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            proj1 = self.target_net(x2)\n",
        "            proj2 = self.target_net(x1)\n",
        "\n",
        "        loss1 = self.byol_loss(pred1, proj1)\n",
        "        loss2 = self.byol_loss(pred2, proj2)\n",
        "\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        loss = (loss1 + loss2).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def update_target_net(self, decay):\n",
        "        for p_online, p_target in zip(self.online_net.parameters(), self.target_net.parameters()):\n",
        "            p_target.data = p_target.data * decay + p_online.data * (1 - decay)\n",
        "\n",
        "    def finetune_forward(self, x):\n",
        "        return self.h(self.f(x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ZCWWJ5JbBT"
      },
      "source": [
        "### Step 2. Prepare datasets & data augmentations\n",
        "\n",
        "For contrastaive learning, a set of random augmentation functions is defined.\n",
        "\n",
        "Then, the set is applied twice to each image, which is implemented as in provided DoubleCompose module.\n",
        "\n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "Refer to the torchvision.transforms documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5kHhEH-JbBU"
      },
      "source": [
        "class DoubleCompose(object):\n",
        "    def __init__(self, trf1, trf2):\n",
        "        self.trf1 = trf1\n",
        "        self.trf2 = trf2\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img1 = img.copy()\n",
        "        img2 = img.copy()\n",
        "        for t1 in self.trf1:\n",
        "            img1 = t1(img1)\n",
        "        for t2 in self.trf2:\n",
        "            img2 = t2(img2)\n",
        "        return img1, img2\n",
        "\n",
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "from PIL import Image\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0, p=1.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        if self.kernel_size % 2 == 0:\n",
        "            self.kernel_size += 1\n",
        "\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, sample,):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < self.p:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return Image.fromarray(sample.astype(np.uint8))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUWa_UJnJbBU"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "img_size = (32, 32)\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)\n",
        "\n",
        "transform1 = [\n",
        "    transforms.RandomResizedCrop(size=img_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([color_jitter], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(3, p=1.0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "]\n",
        "\n",
        "transform2 = [\n",
        "    transforms.RandomResizedCrop(size=img_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([color_jitter], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(3, p=0.1),\n",
        "    transforms.RandomSolarize(5, p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "]\n",
        "\n",
        "train_transform = DoubleCompose(transform1, transform2)\n",
        "\n",
        "finetune_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5txTPQjntcL"
      },
      "source": [
        "Note that we should make three dataloaders:\n",
        "\n",
        "1. Pre-training loader with two heterogeneous data augmentations\n",
        "2. Fine-tuning loader with one basic or no data augmentation\n",
        "3. Test loader with no data augmentation\n",
        "\n",
        "And also note that we should always contain **transforms.ToTensor()** to make sure that the input values are normalized into the range [0, 1]. Dataset-specific normalization (whitening) is recommended, but not mandatory (it is good to testify its effectiveness as an ablation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTIHLlI1JbBV",
        "outputId": "88a62c0c-2d0f-4d68-ccb6-492a43a0bcec"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=train_transform\n",
        "                                )\n",
        "\n",
        "finetune_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=finetune_transform\n",
        "                                )\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=False,\n",
        "                                 download=True,\n",
        "                                 transform=test_transform\n",
        "                                )\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )\n",
        "\n",
        "finetune_loader = DataLoader(finetune_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )\n",
        "\n",
        "test_loader = DataLoader(finetune_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=False,\n",
        "                          drop_last=False\n",
        "                         )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 92897712.88it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU5_4d7BJbBW"
      },
      "source": [
        "### Step 3. Run pre-training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCaPUWVLprMr"
      },
      "source": [
        "import math\n",
        "def get_decay_value(base_decay, global_iter, total_iter):\n",
        "    return 1 - (1 - base_decay) * (math.cos(math.pi * global_iter / total_iter) + 1)/2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GATvSb8JbBW"
      },
      "source": [
        "def train(net, loader):\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.online_net.parameters(), 3e-4)\n",
        "\n",
        "    train_start = time.time()\n",
        "\n",
        "    global_iter = 0\n",
        "    total_iter = 100 * len(loader)\n",
        "\n",
        "    for epoch in range(1, 100 + 1):\n",
        "\n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "\n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            xi, xj = data[0].cuda(), data[1].cuda()\n",
        "\n",
        "            loss = net.byol_forward(xi, xj)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            net.update_target_net(get_decay_value(0.996, global_iter, total_iter))\n",
        "            global_iter += 1\n",
        "\n",
        "        train_loss /= (idx + 1)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch,\n",
        "              \"\\tLoss\\t\", train_loss,\n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "\n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeVkjz5MJbBW"
      },
      "source": [
        "GPU_NUM = '0'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_NUM\n",
        "\n",
        "net = BYOLNet()\n",
        "\n",
        "net = net.cuda()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "4geMfDbuJbBW",
        "outputId": "180e29c7-54fd-4972-eefe-b7bd6be10138"
      },
      "source": [
        "train(net, train_loader)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch\t 1 \tLoss\t -2.224425178116713 \tTime\t 102.17479372024536\n",
            "Epoch\t 2 \tLoss\t -2.410510109632443 \tTime\t 103.6117148399353\n",
            "Epoch\t 3 \tLoss\t -2.457722502488356 \tTime\t 104.09776282310486\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-28ad122a9c57>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-8b536a370621>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, loader)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y5bvN78pYXo"
      },
      "source": [
        "### Step 4. Run fine-tuning step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GGJQMMfJbBX"
      },
      "source": [
        "def finetune(net, loader, test_loader):\n",
        "\n",
        "    ### IMPORTANT ###\n",
        "    # When fine-tuning your network, all parameters except the linear classifier must be frozen\n",
        "    # f, g, and q in the BYOLNet instance will not be updated\n",
        "    for p in net.online_net.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # To make sure, pass only the classifier parameters to optimizer\n",
        "    params = list(net.h.parameters())\n",
        "    optimizer = torch.optim.Adam(params, 3e-4)\n",
        "\n",
        "    train_start = time.time()\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    global_iter = 0\n",
        "    total_iter = 100 * len(loader)\n",
        "\n",
        "    for epoch in range(1, 100 + 1):\n",
        "\n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        net.f.eval()\n",
        "        net.g.eval()\n",
        "        net.q.eval()\n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x, target = data.cuda(), target.cuda()\n",
        "\n",
        "            y = net.finetune_forward(x)\n",
        "            # Or use this\n",
        "            # y = net.finetune_forward_no_grad(x)\n",
        "\n",
        "            loss = loss_fn(y, target)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            global_iter += 1\n",
        "\n",
        "        train_loss /= (idx + 1)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch,\n",
        "              \"\\tLoss\\t\", train_loss,\n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "              \"\\tAcc.\", test(net, test_loader),\n",
        "             )\n",
        "\n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ4aJAKptUpI"
      },
      "source": [
        "def test(net, test_loader):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            y = net.finetune_forward(data)\n",
        "\n",
        "            correct += (y.argmax(1) == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    return correct / total"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiFJLykCt6lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023fe9e1-13ed-424e-d250-0a4aeac862d6"
      },
      "source": [
        "finetune(net, finetune_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch\t 1 \tLoss\t 2.246919101323837 \tTime\t 14.052703380584717 \tAcc. 0.19944\n",
            "Epoch\t 2 \tLoss\t 2.175480100436088 \tTime\t 13.943928480148315 \tAcc. 0.21584\n",
            "Epoch\t 3 \tLoss\t 2.142459419446114 \tTime\t 13.978973150253296 \tAcc. 0.23164\n",
            "Epoch\t 4 \tLoss\t 2.122023984713432 \tTime\t 14.180954456329346 \tAcc. 0.24114\n",
            "Epoch\t 5 \tLoss\t 2.1073346443665333 \tTime\t 13.907471656799316 \tAcc. 0.24384\n",
            "Epoch\t 6 \tLoss\t 2.0950179148943 \tTime\t 14.223559856414795 \tAcc. 0.25016\n",
            "Epoch\t 7 \tLoss\t 2.0846202397957825 \tTime\t 13.75892972946167 \tAcc. 0.25332\n",
            "Epoch\t 8 \tLoss\t 2.0755146980285644 \tTime\t 13.971095323562622 \tAcc. 0.25678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDDXtJvuNwcF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}